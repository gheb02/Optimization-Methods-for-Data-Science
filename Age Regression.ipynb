{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e61ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8ff8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.686191</td>\n",
       "      <td>-0.989465</td>\n",
       "      <td>-0.920503</td>\n",
       "      <td>1.607427</td>\n",
       "      <td>-0.896248</td>\n",
       "      <td>1.118974</td>\n",
       "      <td>-0.969456</td>\n",
       "      <td>1.811707</td>\n",
       "      <td>2.560955</td>\n",
       "      <td>3.803463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862891</td>\n",
       "      <td>-0.909545</td>\n",
       "      <td>-0.915361</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>-0.989461</td>\n",
       "      <td>1.911855</td>\n",
       "      <td>1.409705</td>\n",
       "      <td>2.303997</td>\n",
       "      <td>-0.981840</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887917</td>\n",
       "      <td>4.915272</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>-0.964685</td>\n",
       "      <td>-0.478649</td>\n",
       "      <td>4.342395</td>\n",
       "      <td>-0.332870</td>\n",
       "      <td>-0.768041</td>\n",
       "      <td>-0.815375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.965917</td>\n",
       "      <td>-0.969461</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>5.304822</td>\n",
       "      <td>0.934790</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>4.919212</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.923215</td>\n",
       "      <td>2.746968</td>\n",
       "      <td>-0.918085</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.908587</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>2.984481</td>\n",
       "      <td>0.535007</td>\n",
       "      <td>-0.591029</td>\n",
       "      <td>-0.324043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809726</td>\n",
       "      <td>-0.929934</td>\n",
       "      <td>-0.891814</td>\n",
       "      <td>-0.881796</td>\n",
       "      <td>3.415373</td>\n",
       "      <td>1.044108</td>\n",
       "      <td>-0.442615</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>2.628199</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.268866</td>\n",
       "      <td>-0.408416</td>\n",
       "      <td>-0.935145</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>-0.922438</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>-0.046606</td>\n",
       "      <td>1.149634</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.357959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.834968</td>\n",
       "      <td>-0.937475</td>\n",
       "      <td>-0.917737</td>\n",
       "      <td>-0.929519</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>1.608048</td>\n",
       "      <td>0.276169</td>\n",
       "      <td>1.246468</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529231</td>\n",
       "      <td>-0.829957</td>\n",
       "      <td>-0.897425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>-0.865304</td>\n",
       "      <td>0.331018</td>\n",
       "      <td>-0.644940</td>\n",
       "      <td>1.296097</td>\n",
       "      <td>1.166863</td>\n",
       "      <td>2.036034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775411</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-0.864018</td>\n",
       "      <td>-0.908001</td>\n",
       "      <td>-0.784495</td>\n",
       "      <td>1.329586</td>\n",
       "      <td>0.547925</td>\n",
       "      <td>1.195395</td>\n",
       "      <td>-0.810089</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
       "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
       "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
       "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
       "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
       "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
       "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
       "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
       "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
       "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
       "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
       "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
       "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/AGE_PREDICTION.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return 1-(self.activation(x)**2)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return self.activation(x)*(1-self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def mape(y_real, y_pred):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "    def dloss_dypred(y_real, y_pred):\n",
    "        return (y_pred-y_real)/y_real.shape[0]\n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = self.dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            da_dz = self.derivate_activation(zs[layer])\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "\n",
    "    def train(model, X, y, epochs, lr, method=\"Batch\", alpha = 0.5, batch_size=None, X_val = None, y_val = None):\n",
    "\n",
    "        if method != \"Batch\" and method != \"Mini Batch\" and method != \"SGD\":\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\" and \"SGD\".')\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "            print(f\"Mini-batch size: {batch_size}\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                # Use the entire dataset\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                loss = model.forward(X_batch, y_batch)\n",
    "                model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                model.update(lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    loss = model.forward(xi, yi)\n",
    "                    model.backward(xi, yi, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                    print(f\"[SGD] Sample {i+1}/{len(X_shuffled)} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    loss = model.forward(X_batch, y_batch)\n",
    "                    model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                    print(f\"[Mini-batch] Batch {num_batches} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "\n",
    "                y_pred_val = model.forward(X_val, y_val)[0]\n",
    "\n",
    "                val_loss = model.mse_loss(y_val, y_pred_val, alpha=alpha)\n",
    "\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Training completed!\\nFinal training loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb5bde",
   "metadata": {},
   "source": [
    "## Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ed6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    y_real: true target values\n",
    "    y_pred: predicted values\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "def dloss_dypred(y_real, y_pred):\n",
    "    return (y_pred-y_real)/y_real.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62527eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # ✅ Inizializzazione ADAM (dopo aver creato pesi e bias!)\n",
    "        self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_biases  = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_biases  = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0  # time step\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, a):\n",
    "        if self.act == 'tanh':\n",
    "            return 1 - a**2\n",
    "        elif self.act == 'sigmoid':\n",
    "            return a * (1 - a)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            a = activations[layer + 1]\n",
    "            da_dz = self.derivate_activation(a)\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0, keepdims=True))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "    def update_adam(self, grads_W, grads_b, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimizer.\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment time step\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            # ---- Update weights ----\n",
    "            self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * grads_W[i]\n",
    "            self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "\n",
    "            m_hat_w = self.m_weights[i] / (1 - beta1 ** self.t)\n",
    "            v_hat_w = self.v_weights[i] / (1 - beta2 ** self.t)\n",
    "\n",
    "            self.weights[i] -= lr * m_hat_w / (np.sqrt(v_hat_w) + eps)\n",
    "\n",
    "            # ---- Update biases ----\n",
    "            self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * grads_b[i]\n",
    "            self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "\n",
    "            m_hat_b = self.m_biases[i] / (1 - beta1 ** self.t)\n",
    "            v_hat_b = self.v_biases[i] / (1 - beta2 ** self.t)\n",
    "\n",
    "            self.biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs, lr, method=\"Batch\", alpha=0.5, batch_size=None, X_val=None, y_val=None):\n",
    "        if method not in [\"Batch\", \"Mini Batch\", \"SGD\"]:\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\", and \"SGD\".')\n",
    "\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                y_pred, _, _ = self.forward(X_batch)\n",
    "                loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    y_pred, _, _ = self.forward(xi)\n",
    "                    loss = self.mse_loss(yi, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(xi, yi, alpha)\n",
    "                    self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    y_pred, _, _ = self.forward(X_batch)\n",
    "                    loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                    self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            # Validation\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val, _, _ = self.forward(X_val)\n",
    "                val_loss = self.mse_loss(y_val, y_pred_val, alpha)\n",
    "                val_mape = mape(y_val, y_pred_val)\n",
    "                print(f\"Validation Loss: {val_loss:.6f} | Validation MAPE: {val_mape:.2f}%\")\n",
    "\n",
    "        print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, y, alpha=0.0, epsilon=1e-5, verbose=True):\n",
    "    \"\"\"\n",
    "    Confronta gradienti numerici e analitici.\n",
    "    \n",
    "    Args:\n",
    "        nn: istanza della rete (già inizializzata)\n",
    "        X: batch di input\n",
    "        y: target corrispondente\n",
    "        alpha: regolarizzazione (L2)\n",
    "        epsilon: valore piccolo per differenze finite\n",
    "        verbose: se True stampa info\n",
    "\n",
    "    Returns:\n",
    "        max_diff: massima differenza tra gradiente analitico e numerico\n",
    "    \"\"\"\n",
    "    # Backup dei pesi originali\n",
    "    original_params = nn.get_params_vector()\n",
    "\n",
    "    # Calcolo gradienti analitici\n",
    "    grads_W, grads_b = nn.backward(X, y, alpha)\n",
    "    \n",
    "    # Flatten analitici in un solo vettore\n",
    "    analytical_grad = []\n",
    "    for gw, gb in zip(grads_W, grads_b):\n",
    "        analytical_grad.append(gw.flatten())\n",
    "        analytical_grad.append(gb.flatten())\n",
    "    analytical_grad = np.concatenate(analytical_grad)\n",
    "\n",
    "    # Calcolo gradienti numerici\n",
    "    num_grad = np.zeros_like(original_params)\n",
    "\n",
    "    for i in range(len(original_params)):\n",
    "        theta_plus = original_params.copy()\n",
    "        theta_minus = original_params.copy()\n",
    "\n",
    "        theta_plus[i] += epsilon\n",
    "        theta_minus[i] -= epsilon\n",
    "\n",
    "        nn.set_params_vector(theta_plus)\n",
    "        loss_plus = nn.mse_loss(y, nn.forward(X)[0], alpha)\n",
    "\n",
    "        nn.set_params_vector(theta_minus)\n",
    "        loss_minus = nn.mse_loss(y, nn.forward(X)[0], alpha)\n",
    "\n",
    "        num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "    # Ripristina i parametri originali\n",
    "    nn.set_params_vector(original_params)\n",
    "\n",
    "    # Confronto\n",
    "    diff = np.linalg.norm(num_grad - analytical_grad) / (np.linalg.norm(num_grad) + np.linalg.norm(analytical_grad) + 1e-8)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nGradient Check - Relative Difference: {diff:.10f}\")\n",
    "        if diff < 1e-6:\n",
    "            print(\"✅ Gradienti corretti!\")\n",
    "        elif diff < 1e-3:\n",
    "            print(\"⚠️  Gradienti forse accettabili, ma da controllare\")\n",
    "        else:\n",
    "            print(\"❌ Gradienti sbagliati: controlla backpropagation!\")\n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a71659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Check - Relative Difference: 0.0000000017\n",
      "✅ Gradienti corretti!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7232310446966924e-09"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piccolo batch di test (per non pesare troppo)\n",
    "\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "nn = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")\n",
    "\n",
    "# Take a random subset of 1000 rows\n",
    "df_subset = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "# Train/val split (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "X_batch = X_train[:500]\n",
    "y_batch = y_train[:500]\n",
    "\n",
    "gradient_check(nn, X_batch, y_batch, alpha=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34bedf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started using 'Mini Batch' method for 500 epochs.\n",
      "\n",
      "Epoch 1/500\n",
      "[Mini-batch] Average Loss: 352.435028\n",
      "Validation Loss: 160.264842 | Validation MAPE: 33.94%\n",
      "\n",
      "Epoch 2/500\n",
      "[Mini-batch] Average Loss: 121.187104\n",
      "Validation Loss: 87.585244 | Validation MAPE: 25.60%\n",
      "\n",
      "Epoch 3/500\n",
      "[Mini-batch] Average Loss: 72.957071\n",
      "Validation Loss: 63.950983 | Validation MAPE: 24.15%\n",
      "\n",
      "Epoch 4/500\n",
      "[Mini-batch] Average Loss: 60.362564\n",
      "Validation Loss: 63.037583 | Validation MAPE: 25.00%\n",
      "\n",
      "Epoch 5/500\n",
      "[Mini-batch] Average Loss: 56.744313\n",
      "Validation Loss: 54.700395 | Validation MAPE: 24.71%\n",
      "\n",
      "Epoch 6/500\n",
      "[Mini-batch] Average Loss: 54.663570\n",
      "Validation Loss: 53.423379 | Validation MAPE: 23.33%\n",
      "\n",
      "Epoch 7/500\n",
      "[Mini-batch] Average Loss: 53.750876\n",
      "Validation Loss: 54.991406 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 8/500\n",
      "[Mini-batch] Average Loss: 53.049212\n",
      "Validation Loss: 53.445314 | Validation MAPE: 22.31%\n",
      "\n",
      "Epoch 9/500\n",
      "[Mini-batch] Average Loss: 52.358636\n",
      "Validation Loss: 54.420583 | Validation MAPE: 24.05%\n",
      "\n",
      "Epoch 10/500\n",
      "[Mini-batch] Average Loss: 52.096762\n",
      "Validation Loss: 50.203558 | Validation MAPE: 23.53%\n",
      "\n",
      "Epoch 11/500\n",
      "[Mini-batch] Average Loss: 52.098020\n",
      "Validation Loss: 52.427645 | Validation MAPE: 23.28%\n",
      "\n",
      "Epoch 12/500\n",
      "[Mini-batch] Average Loss: 52.028146\n",
      "Validation Loss: 50.404926 | Validation MAPE: 23.21%\n",
      "\n",
      "Epoch 13/500\n",
      "[Mini-batch] Average Loss: 51.299008\n",
      "Validation Loss: 50.144822 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 14/500\n",
      "[Mini-batch] Average Loss: 51.428775\n",
      "Validation Loss: 50.319966 | Validation MAPE: 22.41%\n",
      "\n",
      "Epoch 15/500\n",
      "[Mini-batch] Average Loss: 51.501991\n",
      "Validation Loss: 51.341697 | Validation MAPE: 23.58%\n",
      "\n",
      "Epoch 16/500\n",
      "[Mini-batch] Average Loss: 51.857445\n",
      "Validation Loss: 50.946519 | Validation MAPE: 23.64%\n",
      "\n",
      "Epoch 17/500\n",
      "[Mini-batch] Average Loss: 51.412277\n",
      "Validation Loss: 50.859039 | Validation MAPE: 23.15%\n",
      "\n",
      "Epoch 18/500\n",
      "[Mini-batch] Average Loss: 51.404479\n",
      "Validation Loss: 50.613780 | Validation MAPE: 23.11%\n",
      "\n",
      "Epoch 19/500\n",
      "[Mini-batch] Average Loss: 50.859893\n",
      "Validation Loss: 50.088865 | Validation MAPE: 22.50%\n",
      "\n",
      "Epoch 20/500\n",
      "[Mini-batch] Average Loss: 51.461601\n",
      "Validation Loss: 51.199546 | Validation MAPE: 22.16%\n",
      "\n",
      "Epoch 21/500\n",
      "[Mini-batch] Average Loss: 51.256437\n",
      "Validation Loss: 52.144020 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 22/500\n",
      "[Mini-batch] Average Loss: 52.079790\n",
      "Validation Loss: 50.837412 | Validation MAPE: 25.06%\n",
      "\n",
      "Epoch 23/500\n",
      "[Mini-batch] Average Loss: 51.194086\n",
      "Validation Loss: 49.726480 | Validation MAPE: 22.86%\n",
      "\n",
      "Epoch 24/500\n",
      "[Mini-batch] Average Loss: 51.067697\n",
      "Validation Loss: 50.625697 | Validation MAPE: 23.60%\n",
      "\n",
      "Epoch 25/500\n",
      "[Mini-batch] Average Loss: 51.381478\n",
      "Validation Loss: 51.345630 | Validation MAPE: 23.74%\n",
      "\n",
      "Epoch 26/500\n",
      "[Mini-batch] Average Loss: 51.209967\n",
      "Validation Loss: 50.509223 | Validation MAPE: 22.74%\n",
      "\n",
      "Epoch 27/500\n",
      "[Mini-batch] Average Loss: 51.508784\n",
      "Validation Loss: 50.059830 | Validation MAPE: 23.71%\n",
      "\n",
      "Epoch 28/500\n",
      "[Mini-batch] Average Loss: 51.600837\n",
      "Validation Loss: 51.563520 | Validation MAPE: 22.75%\n",
      "\n",
      "Epoch 29/500\n",
      "[Mini-batch] Average Loss: 51.657564\n",
      "Validation Loss: 51.873924 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 30/500\n",
      "[Mini-batch] Average Loss: 51.999923\n",
      "Validation Loss: 50.536391 | Validation MAPE: 22.82%\n",
      "\n",
      "Epoch 31/500\n",
      "[Mini-batch] Average Loss: 51.078818\n",
      "Validation Loss: 51.355744 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 32/500\n",
      "[Mini-batch] Average Loss: 51.003462\n",
      "Validation Loss: 49.655610 | Validation MAPE: 23.43%\n",
      "\n",
      "Epoch 33/500\n",
      "[Mini-batch] Average Loss: 51.488056\n",
      "Validation Loss: 50.131848 | Validation MAPE: 24.36%\n",
      "\n",
      "Epoch 34/500\n",
      "[Mini-batch] Average Loss: 51.575879\n",
      "Validation Loss: 49.366296 | Validation MAPE: 23.36%\n",
      "\n",
      "Epoch 35/500\n",
      "[Mini-batch] Average Loss: 50.874888\n",
      "Validation Loss: 51.123846 | Validation MAPE: 22.43%\n",
      "\n",
      "Epoch 36/500\n",
      "[Mini-batch] Average Loss: 50.988376\n",
      "Validation Loss: 49.828373 | Validation MAPE: 24.07%\n",
      "\n",
      "Epoch 37/500\n",
      "[Mini-batch] Average Loss: 50.934238\n",
      "Validation Loss: 49.889827 | Validation MAPE: 23.92%\n",
      "\n",
      "Epoch 38/500\n",
      "[Mini-batch] Average Loss: 51.374412\n",
      "Validation Loss: 49.986975 | Validation MAPE: 22.83%\n",
      "\n",
      "Epoch 39/500\n",
      "[Mini-batch] Average Loss: 51.409194\n",
      "Validation Loss: 50.641294 | Validation MAPE: 23.30%\n",
      "\n",
      "Epoch 40/500\n",
      "[Mini-batch] Average Loss: 50.628371\n",
      "Validation Loss: 49.307042 | Validation MAPE: 22.65%\n",
      "\n",
      "Epoch 41/500\n",
      "[Mini-batch] Average Loss: 51.099571\n",
      "Validation Loss: 52.899610 | Validation MAPE: 23.80%\n",
      "\n",
      "Epoch 42/500\n",
      "[Mini-batch] Average Loss: 51.603333\n",
      "Validation Loss: 51.213531 | Validation MAPE: 23.05%\n",
      "\n",
      "Epoch 43/500\n",
      "[Mini-batch] Average Loss: 51.096186\n",
      "Validation Loss: 50.156749 | Validation MAPE: 22.64%\n",
      "\n",
      "Epoch 44/500\n",
      "[Mini-batch] Average Loss: 50.555210\n",
      "Validation Loss: 50.869972 | Validation MAPE: 21.98%\n",
      "\n",
      "Epoch 45/500\n",
      "[Mini-batch] Average Loss: 50.906803\n",
      "Validation Loss: 49.490536 | Validation MAPE: 23.38%\n",
      "\n",
      "Epoch 46/500\n",
      "[Mini-batch] Average Loss: 50.912743\n",
      "Validation Loss: 50.415282 | Validation MAPE: 22.51%\n",
      "\n",
      "Epoch 47/500\n",
      "[Mini-batch] Average Loss: 50.958263\n",
      "Validation Loss: 50.664622 | Validation MAPE: 22.19%\n",
      "\n",
      "Epoch 48/500\n",
      "[Mini-batch] Average Loss: 50.967721\n",
      "Validation Loss: 50.039037 | Validation MAPE: 23.12%\n",
      "\n",
      "Epoch 49/500\n",
      "[Mini-batch] Average Loss: 51.156583\n",
      "Validation Loss: 51.407176 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 50/500\n",
      "[Mini-batch] Average Loss: 51.069715\n",
      "Validation Loss: 50.096486 | Validation MAPE: 24.00%\n",
      "\n",
      "Epoch 51/500\n",
      "[Mini-batch] Average Loss: 50.849694\n",
      "Validation Loss: 50.360876 | Validation MAPE: 23.07%\n",
      "\n",
      "Epoch 52/500\n",
      "[Mini-batch] Average Loss: 51.134179\n",
      "Validation Loss: 51.608519 | Validation MAPE: 22.43%\n",
      "\n",
      "Epoch 53/500\n",
      "[Mini-batch] Average Loss: 51.045319\n",
      "Validation Loss: 50.065488 | Validation MAPE: 22.31%\n",
      "\n",
      "Epoch 54/500\n",
      "[Mini-batch] Average Loss: 51.075452\n",
      "Validation Loss: 50.196001 | Validation MAPE: 23.31%\n",
      "\n",
      "Epoch 55/500\n",
      "[Mini-batch] Average Loss: 51.083868\n",
      "Validation Loss: 50.366994 | Validation MAPE: 23.85%\n",
      "\n",
      "Epoch 56/500\n",
      "[Mini-batch] Average Loss: 50.547179\n",
      "Validation Loss: 49.779519 | Validation MAPE: 22.78%\n",
      "\n",
      "Epoch 57/500\n",
      "[Mini-batch] Average Loss: 50.960750\n",
      "Validation Loss: 50.745261 | Validation MAPE: 22.97%\n",
      "\n",
      "Epoch 58/500\n",
      "[Mini-batch] Average Loss: 51.559126\n",
      "Validation Loss: 49.991673 | Validation MAPE: 23.39%\n",
      "\n",
      "Epoch 59/500\n",
      "[Mini-batch] Average Loss: 51.432521\n",
      "Validation Loss: 49.876909 | Validation MAPE: 22.64%\n",
      "\n",
      "Epoch 60/500\n",
      "[Mini-batch] Average Loss: 51.284544\n",
      "Validation Loss: 50.421287 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 61/500\n",
      "[Mini-batch] Average Loss: 51.024674\n",
      "Validation Loss: 50.398826 | Validation MAPE: 22.99%\n",
      "\n",
      "Epoch 62/500\n",
      "[Mini-batch] Average Loss: 51.117465\n",
      "Validation Loss: 50.321940 | Validation MAPE: 22.54%\n",
      "\n",
      "Epoch 63/500\n",
      "[Mini-batch] Average Loss: 50.987207\n",
      "Validation Loss: 51.316548 | Validation MAPE: 23.81%\n",
      "\n",
      "Epoch 64/500\n",
      "[Mini-batch] Average Loss: 50.893257\n",
      "Validation Loss: 50.240669 | Validation MAPE: 23.54%\n",
      "\n",
      "Epoch 65/500\n",
      "[Mini-batch] Average Loss: 51.161995\n",
      "Validation Loss: 50.196974 | Validation MAPE: 23.25%\n",
      "\n",
      "Epoch 66/500\n",
      "[Mini-batch] Average Loss: 50.753853\n",
      "Validation Loss: 52.735991 | Validation MAPE: 22.33%\n",
      "\n",
      "Epoch 67/500\n",
      "[Mini-batch] Average Loss: 51.834865\n",
      "Validation Loss: 50.494134 | Validation MAPE: 23.74%\n",
      "\n",
      "Epoch 68/500\n",
      "[Mini-batch] Average Loss: 51.397772\n",
      "Validation Loss: 51.811551 | Validation MAPE: 25.25%\n",
      "\n",
      "Epoch 69/500\n",
      "[Mini-batch] Average Loss: 51.576520\n",
      "Validation Loss: 50.013592 | Validation MAPE: 23.09%\n",
      "\n",
      "Epoch 70/500\n",
      "[Mini-batch] Average Loss: 50.892901\n",
      "Validation Loss: 49.901639 | Validation MAPE: 22.39%\n",
      "\n",
      "Epoch 71/500\n",
      "[Mini-batch] Average Loss: 51.166266\n",
      "Validation Loss: 50.599979 | Validation MAPE: 22.17%\n",
      "\n",
      "Epoch 72/500\n",
      "[Mini-batch] Average Loss: 51.269892\n",
      "Validation Loss: 49.484687 | Validation MAPE: 23.90%\n",
      "\n",
      "Epoch 73/500\n",
      "[Mini-batch] Average Loss: 51.212915\n",
      "Validation Loss: 49.343716 | Validation MAPE: 23.64%\n",
      "\n",
      "Epoch 74/500\n",
      "[Mini-batch] Average Loss: 51.240564\n",
      "Validation Loss: 48.938626 | Validation MAPE: 23.05%\n",
      "\n",
      "Epoch 75/500\n",
      "[Mini-batch] Average Loss: 51.191883\n",
      "Validation Loss: 49.483105 | Validation MAPE: 22.55%\n",
      "\n",
      "Epoch 76/500\n",
      "[Mini-batch] Average Loss: 51.072740\n",
      "Validation Loss: 49.656187 | Validation MAPE: 23.04%\n",
      "\n",
      "Epoch 77/500\n",
      "[Mini-batch] Average Loss: 50.859527\n",
      "Validation Loss: 49.279444 | Validation MAPE: 22.58%\n",
      "\n",
      "Epoch 78/500\n",
      "[Mini-batch] Average Loss: 51.724409\n",
      "Validation Loss: 50.202115 | Validation MAPE: 23.26%\n",
      "\n",
      "Epoch 79/500\n",
      "[Mini-batch] Average Loss: 51.242878\n",
      "Validation Loss: 50.409340 | Validation MAPE: 22.26%\n",
      "\n",
      "Epoch 80/500\n",
      "[Mini-batch] Average Loss: 51.343476\n",
      "Validation Loss: 50.599553 | Validation MAPE: 23.75%\n",
      "\n",
      "Epoch 81/500\n",
      "[Mini-batch] Average Loss: 51.429507\n",
      "Validation Loss: 51.060839 | Validation MAPE: 22.89%\n",
      "\n",
      "Epoch 82/500\n",
      "[Mini-batch] Average Loss: 51.414224\n",
      "Validation Loss: 50.925101 | Validation MAPE: 22.75%\n",
      "\n",
      "Epoch 83/500\n",
      "[Mini-batch] Average Loss: 50.889060\n",
      "Validation Loss: 49.849035 | Validation MAPE: 24.17%\n",
      "\n",
      "Epoch 84/500\n",
      "[Mini-batch] Average Loss: 50.833177\n",
      "Validation Loss: 49.335340 | Validation MAPE: 23.26%\n",
      "\n",
      "Epoch 85/500\n",
      "[Mini-batch] Average Loss: 50.909799\n",
      "Validation Loss: 49.339817 | Validation MAPE: 23.53%\n",
      "\n",
      "Epoch 86/500\n",
      "[Mini-batch] Average Loss: 50.500420\n",
      "Validation Loss: 49.767217 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 87/500\n",
      "[Mini-batch] Average Loss: 50.755390\n",
      "Validation Loss: 53.491255 | Validation MAPE: 25.36%\n",
      "\n",
      "Epoch 88/500\n",
      "[Mini-batch] Average Loss: 51.105910\n",
      "Validation Loss: 50.445250 | Validation MAPE: 23.87%\n",
      "\n",
      "Epoch 89/500\n",
      "[Mini-batch] Average Loss: 50.758851\n",
      "Validation Loss: 49.586018 | Validation MAPE: 22.67%\n",
      "\n",
      "Epoch 90/500\n",
      "[Mini-batch] Average Loss: 50.905374\n",
      "Validation Loss: 49.070616 | Validation MAPE: 22.90%\n",
      "\n",
      "Epoch 91/500\n",
      "[Mini-batch] Average Loss: 50.708460\n",
      "Validation Loss: 50.451802 | Validation MAPE: 22.82%\n",
      "\n",
      "Epoch 92/500\n",
      "[Mini-batch] Average Loss: 50.934341\n",
      "Validation Loss: 49.096148 | Validation MAPE: 23.45%\n",
      "\n",
      "Epoch 93/500\n",
      "[Mini-batch] Average Loss: 50.801741\n",
      "Validation Loss: 49.375830 | Validation MAPE: 23.25%\n",
      "\n",
      "Epoch 94/500\n",
      "[Mini-batch] Average Loss: 50.623772\n",
      "Validation Loss: 49.722163 | Validation MAPE: 22.97%\n",
      "\n",
      "Epoch 95/500\n",
      "[Mini-batch] Average Loss: 50.641869\n",
      "Validation Loss: 49.882112 | Validation MAPE: 22.78%\n",
      "\n",
      "Epoch 96/500\n",
      "[Mini-batch] Average Loss: 50.440353\n",
      "Validation Loss: 50.384573 | Validation MAPE: 21.93%\n",
      "\n",
      "Epoch 97/500\n",
      "[Mini-batch] Average Loss: 51.300889\n",
      "Validation Loss: 49.487096 | Validation MAPE: 23.75%\n",
      "\n",
      "Epoch 98/500\n",
      "[Mini-batch] Average Loss: 51.017871\n",
      "Validation Loss: 50.837363 | Validation MAPE: 22.97%\n",
      "\n",
      "Epoch 99/500\n",
      "[Mini-batch] Average Loss: 51.261804\n",
      "Validation Loss: 50.406655 | Validation MAPE: 23.07%\n",
      "\n",
      "Epoch 100/500\n",
      "[Mini-batch] Average Loss: 51.048985\n",
      "Validation Loss: 50.871701 | Validation MAPE: 23.54%\n",
      "\n",
      "Epoch 101/500\n",
      "[Mini-batch] Average Loss: 51.190454\n",
      "Validation Loss: 50.055613 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 102/500\n",
      "[Mini-batch] Average Loss: 50.682105\n",
      "Validation Loss: 49.663589 | Validation MAPE: 23.49%\n",
      "\n",
      "Epoch 103/500\n",
      "[Mini-batch] Average Loss: 50.678243\n",
      "Validation Loss: 51.101786 | Validation MAPE: 23.41%\n",
      "\n",
      "Epoch 104/500\n",
      "[Mini-batch] Average Loss: 51.215829\n",
      "Validation Loss: 49.572127 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 105/500\n",
      "[Mini-batch] Average Loss: 51.134333\n",
      "Validation Loss: 50.296348 | Validation MAPE: 22.69%\n",
      "\n",
      "Epoch 106/500\n",
      "[Mini-batch] Average Loss: 50.997547\n",
      "Validation Loss: 50.002932 | Validation MAPE: 22.62%\n",
      "\n",
      "Epoch 107/500\n",
      "[Mini-batch] Average Loss: 50.830837\n",
      "Validation Loss: 49.816190 | Validation MAPE: 22.70%\n",
      "\n",
      "Epoch 108/500\n",
      "[Mini-batch] Average Loss: 50.446454\n",
      "Validation Loss: 50.947901 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 109/500\n",
      "[Mini-batch] Average Loss: 50.913532\n",
      "Validation Loss: 50.654383 | Validation MAPE: 23.88%\n",
      "\n",
      "Epoch 110/500\n",
      "[Mini-batch] Average Loss: 50.638631\n",
      "Validation Loss: 50.204833 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 111/500\n",
      "[Mini-batch] Average Loss: 50.686453\n",
      "Validation Loss: 51.449481 | Validation MAPE: 25.35%\n",
      "\n",
      "Epoch 112/500\n",
      "[Mini-batch] Average Loss: 50.973448\n",
      "Validation Loss: 50.717376 | Validation MAPE: 22.32%\n",
      "\n",
      "Epoch 113/500\n",
      "[Mini-batch] Average Loss: 50.689307\n",
      "Validation Loss: 50.770272 | Validation MAPE: 23.84%\n",
      "\n",
      "Epoch 114/500\n",
      "[Mini-batch] Average Loss: 51.036852\n",
      "Validation Loss: 50.606530 | Validation MAPE: 22.61%\n",
      "\n",
      "Epoch 115/500\n",
      "[Mini-batch] Average Loss: 51.123370\n",
      "Validation Loss: 49.704644 | Validation MAPE: 22.98%\n",
      "\n",
      "Epoch 116/500\n",
      "[Mini-batch] Average Loss: 51.383648\n",
      "Validation Loss: 50.776571 | Validation MAPE: 22.03%\n",
      "\n",
      "Epoch 117/500\n",
      "[Mini-batch] Average Loss: 50.828608\n",
      "Validation Loss: 50.635817 | Validation MAPE: 22.57%\n",
      "\n",
      "Epoch 118/500\n",
      "[Mini-batch] Average Loss: 50.919752\n",
      "Validation Loss: 49.496280 | Validation MAPE: 22.82%\n",
      "\n",
      "Epoch 119/500\n",
      "[Mini-batch] Average Loss: 50.656546\n",
      "Validation Loss: 49.413733 | Validation MAPE: 22.47%\n",
      "\n",
      "Epoch 120/500\n",
      "[Mini-batch] Average Loss: 50.829750\n",
      "Validation Loss: 49.124877 | Validation MAPE: 22.71%\n",
      "\n",
      "Epoch 121/500\n",
      "[Mini-batch] Average Loss: 50.951644\n",
      "Validation Loss: 51.532818 | Validation MAPE: 22.00%\n",
      "\n",
      "Epoch 122/500\n",
      "[Mini-batch] Average Loss: 50.668147\n",
      "Validation Loss: 50.629854 | Validation MAPE: 23.71%\n",
      "\n",
      "Epoch 123/500\n",
      "[Mini-batch] Average Loss: 50.832579\n",
      "Validation Loss: 50.690499 | Validation MAPE: 22.59%\n",
      "\n",
      "Epoch 124/500\n",
      "[Mini-batch] Average Loss: 50.910348\n",
      "Validation Loss: 49.897665 | Validation MAPE: 23.23%\n",
      "\n",
      "Epoch 125/500\n",
      "[Mini-batch] Average Loss: 51.238684\n",
      "Validation Loss: 49.790437 | Validation MAPE: 22.94%\n",
      "\n",
      "Epoch 126/500\n",
      "[Mini-batch] Average Loss: 50.561529\n",
      "Validation Loss: 48.621657 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 127/500\n",
      "[Mini-batch] Average Loss: 50.466367\n",
      "Validation Loss: 50.459169 | Validation MAPE: 22.66%\n",
      "\n",
      "Epoch 128/500\n",
      "[Mini-batch] Average Loss: 50.826600\n",
      "Validation Loss: 50.920081 | Validation MAPE: 22.08%\n",
      "\n",
      "Epoch 129/500\n",
      "[Mini-batch] Average Loss: 50.706256\n",
      "Validation Loss: 50.086331 | Validation MAPE: 24.10%\n",
      "\n",
      "Epoch 130/500\n",
      "[Mini-batch] Average Loss: 50.528252\n",
      "Validation Loss: 50.269476 | Validation MAPE: 22.84%\n",
      "\n",
      "Epoch 131/500\n",
      "[Mini-batch] Average Loss: 50.480509\n",
      "Validation Loss: 49.511593 | Validation MAPE: 23.10%\n",
      "\n",
      "Epoch 132/500\n",
      "[Mini-batch] Average Loss: 50.575414\n",
      "Validation Loss: 50.109552 | Validation MAPE: 23.12%\n",
      "\n",
      "Epoch 133/500\n",
      "[Mini-batch] Average Loss: 50.924979\n",
      "Validation Loss: 48.920390 | Validation MAPE: 23.41%\n",
      "\n",
      "Epoch 134/500\n",
      "[Mini-batch] Average Loss: 50.737452\n",
      "Validation Loss: 49.710987 | Validation MAPE: 23.85%\n",
      "\n",
      "Epoch 135/500\n",
      "[Mini-batch] Average Loss: 50.527503\n",
      "Validation Loss: 49.549569 | Validation MAPE: 22.62%\n",
      "\n",
      "Epoch 136/500\n",
      "[Mini-batch] Average Loss: 50.829646\n",
      "Validation Loss: 48.896328 | Validation MAPE: 23.44%\n",
      "\n",
      "Epoch 137/500\n",
      "[Mini-batch] Average Loss: 50.802414\n",
      "Validation Loss: 49.770891 | Validation MAPE: 23.69%\n",
      "\n",
      "Epoch 138/500\n",
      "[Mini-batch] Average Loss: 51.013866\n",
      "Validation Loss: 50.714443 | Validation MAPE: 22.82%\n",
      "\n",
      "Epoch 139/500\n",
      "[Mini-batch] Average Loss: 51.162432\n",
      "Validation Loss: 50.301984 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 140/500\n",
      "[Mini-batch] Average Loss: 50.466916\n",
      "Validation Loss: 50.366431 | Validation MAPE: 22.23%\n",
      "\n",
      "Epoch 141/500\n",
      "[Mini-batch] Average Loss: 50.740575\n",
      "Validation Loss: 50.840924 | Validation MAPE: 22.70%\n",
      "\n",
      "Epoch 142/500\n",
      "[Mini-batch] Average Loss: 50.919924\n",
      "Validation Loss: 49.964884 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 143/500\n",
      "[Mini-batch] Average Loss: 50.671663\n",
      "Validation Loss: 50.375573 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 144/500\n",
      "[Mini-batch] Average Loss: 50.452009\n",
      "Validation Loss: 49.638839 | Validation MAPE: 22.83%\n",
      "\n",
      "Epoch 145/500\n",
      "[Mini-batch] Average Loss: 50.876134\n",
      "Validation Loss: 48.742346 | Validation MAPE: 23.26%\n",
      "\n",
      "Epoch 146/500\n",
      "[Mini-batch] Average Loss: 50.692969\n",
      "Validation Loss: 49.836543 | Validation MAPE: 23.69%\n",
      "\n",
      "Epoch 147/500\n",
      "[Mini-batch] Average Loss: 50.654570\n",
      "Validation Loss: 49.435802 | Validation MAPE: 22.34%\n",
      "\n",
      "Epoch 148/500\n",
      "[Mini-batch] Average Loss: 50.286192\n",
      "Validation Loss: 49.591492 | Validation MAPE: 24.19%\n",
      "\n",
      "Epoch 149/500\n",
      "[Mini-batch] Average Loss: 50.191411\n",
      "Validation Loss: 50.204681 | Validation MAPE: 22.45%\n",
      "\n",
      "Epoch 150/500\n",
      "[Mini-batch] Average Loss: 50.567722\n",
      "Validation Loss: 49.105854 | Validation MAPE: 23.27%\n",
      "\n",
      "Epoch 151/500\n",
      "[Mini-batch] Average Loss: 51.149361\n",
      "Validation Loss: 49.554349 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 152/500\n",
      "[Mini-batch] Average Loss: 50.599866\n",
      "Validation Loss: 50.623133 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 153/500\n",
      "[Mini-batch] Average Loss: 50.378909\n",
      "Validation Loss: 49.328426 | Validation MAPE: 22.62%\n",
      "\n",
      "Epoch 154/500\n",
      "[Mini-batch] Average Loss: 50.755633\n",
      "Validation Loss: 49.272259 | Validation MAPE: 22.57%\n",
      "\n",
      "Epoch 155/500\n",
      "[Mini-batch] Average Loss: 50.483009\n",
      "Validation Loss: 49.749705 | Validation MAPE: 22.31%\n",
      "\n",
      "Epoch 156/500\n",
      "[Mini-batch] Average Loss: 50.580915\n",
      "Validation Loss: 49.993035 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 157/500\n",
      "[Mini-batch] Average Loss: 50.912137\n",
      "Validation Loss: 49.910028 | Validation MAPE: 22.60%\n",
      "\n",
      "Epoch 158/500\n",
      "[Mini-batch] Average Loss: 51.001802\n",
      "Validation Loss: 50.475438 | Validation MAPE: 22.75%\n",
      "\n",
      "Epoch 159/500\n",
      "[Mini-batch] Average Loss: 50.885354\n",
      "Validation Loss: 49.408140 | Validation MAPE: 22.49%\n",
      "\n",
      "Epoch 160/500\n",
      "[Mini-batch] Average Loss: 50.920232\n",
      "Validation Loss: 49.935401 | Validation MAPE: 23.72%\n",
      "\n",
      "Epoch 161/500\n",
      "[Mini-batch] Average Loss: 50.998767\n",
      "Validation Loss: 50.007829 | Validation MAPE: 23.42%\n",
      "\n",
      "Epoch 162/500\n",
      "[Mini-batch] Average Loss: 51.151211\n",
      "Validation Loss: 51.835064 | Validation MAPE: 23.91%\n",
      "\n",
      "Epoch 163/500\n",
      "[Mini-batch] Average Loss: 51.102558\n",
      "Validation Loss: 49.959155 | Validation MAPE: 22.28%\n",
      "\n",
      "Epoch 164/500\n",
      "[Mini-batch] Average Loss: 50.915130\n",
      "Validation Loss: 49.764500 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 165/500\n",
      "[Mini-batch] Average Loss: 51.113184\n",
      "Validation Loss: 50.020567 | Validation MAPE: 23.83%\n",
      "\n",
      "Epoch 166/500\n",
      "[Mini-batch] Average Loss: 51.088462\n",
      "Validation Loss: 49.646071 | Validation MAPE: 22.70%\n",
      "\n",
      "Epoch 167/500\n",
      "[Mini-batch] Average Loss: 51.331169\n",
      "Validation Loss: 50.917234 | Validation MAPE: 24.09%\n",
      "\n",
      "Epoch 168/500\n",
      "[Mini-batch] Average Loss: 51.161392\n",
      "Validation Loss: 49.323405 | Validation MAPE: 23.57%\n",
      "\n",
      "Epoch 169/500\n",
      "[Mini-batch] Average Loss: 50.455545\n",
      "Validation Loss: 49.331375 | Validation MAPE: 23.09%\n",
      "\n",
      "Epoch 170/500\n",
      "[Mini-batch] Average Loss: 50.952369\n",
      "Validation Loss: 49.732194 | Validation MAPE: 22.84%\n",
      "\n",
      "Epoch 171/500\n",
      "[Mini-batch] Average Loss: 50.835803\n",
      "Validation Loss: 50.005543 | Validation MAPE: 22.85%\n",
      "\n",
      "Epoch 172/500\n",
      "[Mini-batch] Average Loss: 50.737710\n",
      "Validation Loss: 49.014178 | Validation MAPE: 23.23%\n",
      "\n",
      "Epoch 173/500\n",
      "[Mini-batch] Average Loss: 50.868825\n",
      "Validation Loss: 49.140525 | Validation MAPE: 23.39%\n",
      "\n",
      "Epoch 174/500\n",
      "[Mini-batch] Average Loss: 51.092305\n",
      "Validation Loss: 49.753482 | Validation MAPE: 23.31%\n",
      "\n",
      "Epoch 175/500\n",
      "[Mini-batch] Average Loss: 50.850433\n",
      "Validation Loss: 50.609995 | Validation MAPE: 22.88%\n",
      "\n",
      "Epoch 176/500\n",
      "[Mini-batch] Average Loss: 50.944987\n",
      "Validation Loss: 49.405604 | Validation MAPE: 22.62%\n",
      "\n",
      "Epoch 177/500\n",
      "[Mini-batch] Average Loss: 50.808119\n",
      "Validation Loss: 49.184713 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 178/500\n",
      "[Mini-batch] Average Loss: 50.801961\n",
      "Validation Loss: 51.279736 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 179/500\n",
      "[Mini-batch] Average Loss: 51.014736\n",
      "Validation Loss: 51.909013 | Validation MAPE: 25.19%\n",
      "\n",
      "Epoch 180/500\n",
      "[Mini-batch] Average Loss: 51.116424\n",
      "Validation Loss: 49.648807 | Validation MAPE: 23.42%\n",
      "\n",
      "Epoch 181/500\n",
      "[Mini-batch] Average Loss: 50.723830\n",
      "Validation Loss: 49.837697 | Validation MAPE: 22.71%\n",
      "\n",
      "Epoch 182/500\n",
      "[Mini-batch] Average Loss: 50.756703\n",
      "Validation Loss: 49.336030 | Validation MAPE: 23.14%\n",
      "\n",
      "Epoch 183/500\n",
      "[Mini-batch] Average Loss: 51.185347\n",
      "Validation Loss: 51.584364 | Validation MAPE: 25.53%\n",
      "\n",
      "Epoch 184/500\n",
      "[Mini-batch] Average Loss: 50.906861\n",
      "Validation Loss: 49.843676 | Validation MAPE: 22.03%\n",
      "\n",
      "Epoch 185/500\n",
      "[Mini-batch] Average Loss: 51.058548\n",
      "Validation Loss: 49.602215 | Validation MAPE: 23.76%\n",
      "\n",
      "Epoch 186/500\n",
      "[Mini-batch] Average Loss: 50.714468\n",
      "Validation Loss: 50.152614 | Validation MAPE: 22.47%\n",
      "\n",
      "Epoch 187/500\n",
      "[Mini-batch] Average Loss: 50.836094\n",
      "Validation Loss: 51.706549 | Validation MAPE: 23.03%\n",
      "\n",
      "Epoch 188/500\n",
      "[Mini-batch] Average Loss: 51.551988\n",
      "Validation Loss: 51.247287 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 189/500\n",
      "[Mini-batch] Average Loss: 50.788391\n",
      "Validation Loss: 50.214956 | Validation MAPE: 23.45%\n",
      "\n",
      "Epoch 190/500\n",
      "[Mini-batch] Average Loss: 50.890994\n",
      "Validation Loss: 49.455975 | Validation MAPE: 22.73%\n",
      "\n",
      "Epoch 191/500\n",
      "[Mini-batch] Average Loss: 50.922075\n",
      "Validation Loss: 49.090481 | Validation MAPE: 23.50%\n",
      "\n",
      "Epoch 192/500\n",
      "[Mini-batch] Average Loss: 50.891642\n",
      "Validation Loss: 53.674833 | Validation MAPE: 26.67%\n",
      "\n",
      "Epoch 193/500\n",
      "[Mini-batch] Average Loss: 51.056594\n",
      "Validation Loss: 51.100167 | Validation MAPE: 23.17%\n",
      "\n",
      "Epoch 194/500\n",
      "[Mini-batch] Average Loss: 51.055505\n",
      "Validation Loss: 49.530318 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 195/500\n",
      "[Mini-batch] Average Loss: 50.973956\n",
      "Validation Loss: 50.110415 | Validation MAPE: 23.62%\n",
      "\n",
      "Epoch 196/500\n",
      "[Mini-batch] Average Loss: 50.877315\n",
      "Validation Loss: 50.162625 | Validation MAPE: 24.19%\n",
      "\n",
      "Epoch 197/500\n",
      "[Mini-batch] Average Loss: 50.819444\n",
      "Validation Loss: 50.959719 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 198/500\n",
      "[Mini-batch] Average Loss: 50.609975\n",
      "Validation Loss: 50.438888 | Validation MAPE: 22.92%\n",
      "\n",
      "Epoch 199/500\n",
      "[Mini-batch] Average Loss: 50.756163\n",
      "Validation Loss: 50.220766 | Validation MAPE: 22.91%\n",
      "\n",
      "Epoch 200/500\n",
      "[Mini-batch] Average Loss: 51.216697\n",
      "Validation Loss: 50.226706 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 201/500\n",
      "[Mini-batch] Average Loss: 51.415733\n",
      "Validation Loss: 51.160933 | Validation MAPE: 23.12%\n",
      "\n",
      "Epoch 202/500\n",
      "[Mini-batch] Average Loss: 51.405972\n",
      "Validation Loss: 50.475961 | Validation MAPE: 23.41%\n",
      "\n",
      "Epoch 203/500\n",
      "[Mini-batch] Average Loss: 51.112619\n",
      "Validation Loss: 49.965811 | Validation MAPE: 23.50%\n",
      "\n",
      "Epoch 204/500\n",
      "[Mini-batch] Average Loss: 51.170923\n",
      "Validation Loss: 51.312227 | Validation MAPE: 23.50%\n",
      "\n",
      "Epoch 205/500\n",
      "[Mini-batch] Average Loss: 51.182716\n",
      "Validation Loss: 50.084407 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 206/500\n",
      "[Mini-batch] Average Loss: 51.547391\n",
      "Validation Loss: 50.634951 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 207/500\n",
      "[Mini-batch] Average Loss: 50.776272\n",
      "Validation Loss: 49.211053 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 208/500\n",
      "[Mini-batch] Average Loss: 50.767794\n",
      "Validation Loss: 50.497944 | Validation MAPE: 22.07%\n",
      "\n",
      "Epoch 209/500\n",
      "[Mini-batch] Average Loss: 51.024045\n",
      "Validation Loss: 50.075349 | Validation MAPE: 23.42%\n",
      "\n",
      "Epoch 210/500\n",
      "[Mini-batch] Average Loss: 50.993625\n",
      "Validation Loss: 49.984379 | Validation MAPE: 22.69%\n",
      "\n",
      "Epoch 211/500\n",
      "[Mini-batch] Average Loss: 50.760758\n",
      "Validation Loss: 49.547706 | Validation MAPE: 22.89%\n",
      "\n",
      "Epoch 212/500\n",
      "[Mini-batch] Average Loss: 51.264136\n",
      "Validation Loss: 50.922509 | Validation MAPE: 23.57%\n",
      "\n",
      "Epoch 213/500\n",
      "[Mini-batch] Average Loss: 50.590277\n",
      "Validation Loss: 49.032843 | Validation MAPE: 23.39%\n",
      "\n",
      "Epoch 214/500\n",
      "[Mini-batch] Average Loss: 50.605427\n",
      "Validation Loss: 48.970721 | Validation MAPE: 22.63%\n",
      "\n",
      "Epoch 215/500\n",
      "[Mini-batch] Average Loss: 50.906150\n",
      "Validation Loss: 51.041736 | Validation MAPE: 21.94%\n",
      "\n",
      "Epoch 216/500\n",
      "[Mini-batch] Average Loss: 50.647092\n",
      "Validation Loss: 49.920000 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 217/500\n",
      "[Mini-batch] Average Loss: 51.182747\n",
      "Validation Loss: 49.724478 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 218/500\n",
      "[Mini-batch] Average Loss: 50.840484\n",
      "Validation Loss: 50.556267 | Validation MAPE: 23.65%\n",
      "\n",
      "Epoch 219/500\n",
      "[Mini-batch] Average Loss: 51.414862\n",
      "Validation Loss: 49.587628 | Validation MAPE: 23.38%\n",
      "\n",
      "Epoch 220/500\n",
      "[Mini-batch] Average Loss: 50.767204\n",
      "Validation Loss: 49.368087 | Validation MAPE: 22.87%\n",
      "\n",
      "Epoch 221/500\n",
      "[Mini-batch] Average Loss: 50.815838\n",
      "Validation Loss: 50.172008 | Validation MAPE: 23.10%\n",
      "\n",
      "Epoch 222/500\n",
      "[Mini-batch] Average Loss: 50.790231\n",
      "Validation Loss: 51.491422 | Validation MAPE: 23.10%\n",
      "\n",
      "Epoch 223/500\n",
      "[Mini-batch] Average Loss: 50.541291\n",
      "Validation Loss: 49.156270 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 224/500\n",
      "[Mini-batch] Average Loss: 50.579232\n",
      "Validation Loss: 50.049860 | Validation MAPE: 24.05%\n",
      "\n",
      "Epoch 225/500\n",
      "[Mini-batch] Average Loss: 50.881008\n",
      "Validation Loss: 48.647380 | Validation MAPE: 23.18%\n",
      "\n",
      "Epoch 226/500\n",
      "[Mini-batch] Average Loss: 50.766727\n",
      "Validation Loss: 49.734884 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 227/500\n",
      "[Mini-batch] Average Loss: 50.322931\n",
      "Validation Loss: 50.433317 | Validation MAPE: 22.16%\n",
      "\n",
      "Epoch 228/500\n",
      "[Mini-batch] Average Loss: 50.865447\n",
      "Validation Loss: 49.048786 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 229/500\n",
      "[Mini-batch] Average Loss: 51.085665\n",
      "Validation Loss: 50.159572 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 230/500\n",
      "[Mini-batch] Average Loss: 50.748151\n",
      "Validation Loss: 52.276645 | Validation MAPE: 22.45%\n",
      "\n",
      "Epoch 231/500\n",
      "[Mini-batch] Average Loss: 50.996808\n",
      "Validation Loss: 51.196806 | Validation MAPE: 22.54%\n",
      "\n",
      "Epoch 232/500\n",
      "[Mini-batch] Average Loss: 51.208847\n",
      "Validation Loss: 50.279087 | Validation MAPE: 22.19%\n",
      "\n",
      "Epoch 233/500\n",
      "[Mini-batch] Average Loss: 50.865092\n",
      "Validation Loss: 49.454748 | Validation MAPE: 22.50%\n",
      "\n",
      "Epoch 234/500\n",
      "[Mini-batch] Average Loss: 50.501465\n",
      "Validation Loss: 49.232135 | Validation MAPE: 23.01%\n",
      "\n",
      "Epoch 235/500\n",
      "[Mini-batch] Average Loss: 50.474390\n",
      "Validation Loss: 49.964247 | Validation MAPE: 22.72%\n",
      "\n",
      "Epoch 236/500\n",
      "[Mini-batch] Average Loss: 50.785039\n",
      "Validation Loss: 50.711070 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 237/500\n",
      "[Mini-batch] Average Loss: 51.044200\n",
      "Validation Loss: 50.010454 | Validation MAPE: 23.84%\n",
      "\n",
      "Epoch 238/500\n",
      "[Mini-batch] Average Loss: 50.454752\n",
      "Validation Loss: 51.188039 | Validation MAPE: 24.76%\n",
      "\n",
      "Epoch 239/500\n",
      "[Mini-batch] Average Loss: 50.721799\n",
      "Validation Loss: 50.372658 | Validation MAPE: 24.49%\n",
      "\n",
      "Epoch 240/500\n",
      "[Mini-batch] Average Loss: 50.627230\n",
      "Validation Loss: 49.408757 | Validation MAPE: 24.03%\n",
      "\n",
      "Epoch 241/500\n",
      "[Mini-batch] Average Loss: 50.915247\n",
      "Validation Loss: 49.571589 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 242/500\n",
      "[Mini-batch] Average Loss: 50.822716\n",
      "Validation Loss: 49.789541 | Validation MAPE: 23.71%\n",
      "\n",
      "Epoch 243/500\n",
      "[Mini-batch] Average Loss: 50.917150\n",
      "Validation Loss: 49.370588 | Validation MAPE: 22.82%\n",
      "\n",
      "Epoch 244/500\n",
      "[Mini-batch] Average Loss: 50.479550\n",
      "Validation Loss: 50.876528 | Validation MAPE: 22.40%\n",
      "\n",
      "Epoch 245/500\n",
      "[Mini-batch] Average Loss: 50.804146\n",
      "Validation Loss: 52.344461 | Validation MAPE: 22.15%\n",
      "\n",
      "Epoch 246/500\n",
      "[Mini-batch] Average Loss: 50.589741\n",
      "Validation Loss: 49.947814 | Validation MAPE: 22.58%\n",
      "\n",
      "Epoch 247/500\n",
      "[Mini-batch] Average Loss: 50.644714\n",
      "Validation Loss: 50.508790 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 248/500\n",
      "[Mini-batch] Average Loss: 50.681384\n",
      "Validation Loss: 50.252570 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 249/500\n",
      "[Mini-batch] Average Loss: 50.693183\n",
      "Validation Loss: 50.370238 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 250/500\n",
      "[Mini-batch] Average Loss: 50.869643\n",
      "Validation Loss: 50.130813 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 251/500\n",
      "[Mini-batch] Average Loss: 50.793255\n",
      "Validation Loss: 49.192842 | Validation MAPE: 24.53%\n",
      "\n",
      "Epoch 252/500\n",
      "[Mini-batch] Average Loss: 50.313211\n",
      "Validation Loss: 49.774067 | Validation MAPE: 23.48%\n",
      "\n",
      "Epoch 253/500\n",
      "[Mini-batch] Average Loss: 50.787057\n",
      "Validation Loss: 49.594179 | Validation MAPE: 23.45%\n",
      "\n",
      "Epoch 254/500\n",
      "[Mini-batch] Average Loss: 50.744893\n",
      "Validation Loss: 51.303021 | Validation MAPE: 22.72%\n",
      "\n",
      "Epoch 255/500\n",
      "[Mini-batch] Average Loss: 51.003128\n",
      "Validation Loss: 50.051047 | Validation MAPE: 23.22%\n",
      "\n",
      "Epoch 256/500\n",
      "[Mini-batch] Average Loss: 51.105630\n",
      "Validation Loss: 50.173780 | Validation MAPE: 23.75%\n",
      "\n",
      "Epoch 257/500\n",
      "[Mini-batch] Average Loss: 50.755796\n",
      "Validation Loss: 50.376610 | Validation MAPE: 24.29%\n",
      "\n",
      "Epoch 258/500\n",
      "[Mini-batch] Average Loss: 50.635672\n",
      "Validation Loss: 49.775596 | Validation MAPE: 22.19%\n",
      "\n",
      "Epoch 259/500\n",
      "[Mini-batch] Average Loss: 50.704438\n",
      "Validation Loss: 49.793413 | Validation MAPE: 22.60%\n",
      "\n",
      "Epoch 260/500\n",
      "[Mini-batch] Average Loss: 50.537302\n",
      "Validation Loss: 51.479358 | Validation MAPE: 22.20%\n",
      "\n",
      "Epoch 261/500\n",
      "[Mini-batch] Average Loss: 50.416044\n",
      "Validation Loss: 51.000696 | Validation MAPE: 21.89%\n",
      "\n",
      "Epoch 262/500\n",
      "[Mini-batch] Average Loss: 50.538653\n",
      "Validation Loss: 49.648289 | Validation MAPE: 22.30%\n",
      "\n",
      "Epoch 263/500\n",
      "[Mini-batch] Average Loss: 50.447385\n",
      "Validation Loss: 49.401635 | Validation MAPE: 23.86%\n",
      "\n",
      "Epoch 264/500\n",
      "[Mini-batch] Average Loss: 50.934069\n",
      "Validation Loss: 49.206061 | Validation MAPE: 22.49%\n",
      "\n",
      "Epoch 265/500\n",
      "[Mini-batch] Average Loss: 50.575474\n",
      "Validation Loss: 48.433262 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 266/500\n",
      "[Mini-batch] Average Loss: 50.828866\n",
      "Validation Loss: 48.852939 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 267/500\n",
      "[Mini-batch] Average Loss: 50.522428\n",
      "Validation Loss: 49.438875 | Validation MAPE: 22.73%\n",
      "\n",
      "Epoch 268/500\n",
      "[Mini-batch] Average Loss: 50.526357\n",
      "Validation Loss: 49.027615 | Validation MAPE: 23.55%\n",
      "\n",
      "Epoch 269/500\n",
      "[Mini-batch] Average Loss: 50.452478\n",
      "Validation Loss: 50.171703 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 270/500\n",
      "[Mini-batch] Average Loss: 50.302864\n",
      "Validation Loss: 49.223281 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 271/500\n",
      "[Mini-batch] Average Loss: 50.583044\n",
      "Validation Loss: 49.264969 | Validation MAPE: 22.42%\n",
      "\n",
      "Epoch 272/500\n",
      "[Mini-batch] Average Loss: 50.475929\n",
      "Validation Loss: 49.639779 | Validation MAPE: 23.04%\n",
      "\n",
      "Epoch 273/500\n",
      "[Mini-batch] Average Loss: 51.027613\n",
      "Validation Loss: 51.740329 | Validation MAPE: 25.31%\n",
      "\n",
      "Epoch 274/500\n",
      "[Mini-batch] Average Loss: 50.920348\n",
      "Validation Loss: 49.317504 | Validation MAPE: 22.37%\n",
      "\n",
      "Epoch 275/500\n",
      "[Mini-batch] Average Loss: 50.757759\n",
      "Validation Loss: 49.964956 | Validation MAPE: 23.91%\n",
      "\n",
      "Epoch 276/500\n",
      "[Mini-batch] Average Loss: 50.909612\n",
      "Validation Loss: 49.428592 | Validation MAPE: 22.64%\n",
      "\n",
      "Epoch 277/500\n",
      "[Mini-batch] Average Loss: 51.134771\n",
      "Validation Loss: 51.342836 | Validation MAPE: 22.30%\n",
      "\n",
      "Epoch 278/500\n",
      "[Mini-batch] Average Loss: 50.791362\n",
      "Validation Loss: 49.814346 | Validation MAPE: 22.94%\n",
      "\n",
      "Epoch 279/500\n",
      "[Mini-batch] Average Loss: 50.784834\n",
      "Validation Loss: 49.933149 | Validation MAPE: 24.14%\n",
      "\n",
      "Epoch 280/500\n",
      "[Mini-batch] Average Loss: 50.640162\n",
      "Validation Loss: 49.370848 | Validation MAPE: 23.03%\n",
      "\n",
      "Epoch 281/500\n",
      "[Mini-batch] Average Loss: 50.648518\n",
      "Validation Loss: 49.177108 | Validation MAPE: 23.24%\n",
      "\n",
      "Epoch 282/500\n",
      "[Mini-batch] Average Loss: 50.654605\n",
      "Validation Loss: 49.442851 | Validation MAPE: 23.50%\n",
      "\n",
      "Epoch 283/500\n",
      "[Mini-batch] Average Loss: 50.185222\n",
      "Validation Loss: 49.352515 | Validation MAPE: 23.26%\n",
      "\n",
      "Epoch 284/500\n",
      "[Mini-batch] Average Loss: 51.086358\n",
      "Validation Loss: 49.762791 | Validation MAPE: 23.24%\n",
      "\n",
      "Epoch 285/500\n",
      "[Mini-batch] Average Loss: 50.466317\n",
      "Validation Loss: 48.842884 | Validation MAPE: 23.40%\n",
      "\n",
      "Epoch 286/500\n",
      "[Mini-batch] Average Loss: 50.948454\n",
      "Validation Loss: 49.396994 | Validation MAPE: 22.54%\n",
      "\n",
      "Epoch 287/500\n",
      "[Mini-batch] Average Loss: 50.972843\n",
      "Validation Loss: 49.288220 | Validation MAPE: 22.26%\n",
      "\n",
      "Epoch 288/500\n",
      "[Mini-batch] Average Loss: 50.412863\n",
      "Validation Loss: 49.304294 | Validation MAPE: 23.32%\n",
      "\n",
      "Epoch 289/500\n",
      "[Mini-batch] Average Loss: 50.849939\n",
      "Validation Loss: 50.360674 | Validation MAPE: 22.59%\n",
      "\n",
      "Epoch 290/500\n",
      "[Mini-batch] Average Loss: 50.543358\n",
      "Validation Loss: 50.400046 | Validation MAPE: 22.93%\n",
      "\n",
      "Epoch 291/500\n",
      "[Mini-batch] Average Loss: 50.561384\n",
      "Validation Loss: 49.414457 | Validation MAPE: 22.47%\n",
      "\n",
      "Epoch 292/500\n",
      "[Mini-batch] Average Loss: 50.378132\n",
      "Validation Loss: 49.503367 | Validation MAPE: 22.76%\n",
      "\n",
      "Epoch 293/500\n",
      "[Mini-batch] Average Loss: 50.678400\n",
      "Validation Loss: 49.830649 | Validation MAPE: 22.46%\n",
      "\n",
      "Epoch 294/500\n",
      "[Mini-batch] Average Loss: 50.267151\n",
      "Validation Loss: 49.201385 | Validation MAPE: 23.32%\n",
      "\n",
      "Epoch 295/500\n",
      "[Mini-batch] Average Loss: 50.602509\n",
      "Validation Loss: 49.582237 | Validation MAPE: 23.17%\n",
      "\n",
      "Epoch 296/500\n",
      "[Mini-batch] Average Loss: 50.604631\n",
      "Validation Loss: 50.125826 | Validation MAPE: 22.90%\n",
      "\n",
      "Epoch 297/500\n",
      "[Mini-batch] Average Loss: 50.358194\n",
      "Validation Loss: 50.608983 | Validation MAPE: 22.07%\n",
      "\n",
      "Epoch 298/500\n",
      "[Mini-batch] Average Loss: 50.343535\n",
      "Validation Loss: 49.299989 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 299/500\n",
      "[Mini-batch] Average Loss: 50.282595\n",
      "Validation Loss: 48.805649 | Validation MAPE: 22.33%\n",
      "\n",
      "Epoch 300/500\n",
      "[Mini-batch] Average Loss: 50.657221\n",
      "Validation Loss: 49.906671 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 301/500\n",
      "[Mini-batch] Average Loss: 50.404125\n",
      "Validation Loss: 50.355500 | Validation MAPE: 22.08%\n",
      "\n",
      "Epoch 302/500\n",
      "[Mini-batch] Average Loss: 50.313770\n",
      "Validation Loss: 49.793517 | Validation MAPE: 22.43%\n",
      "\n",
      "Epoch 303/500\n",
      "[Mini-batch] Average Loss: 50.273471\n",
      "Validation Loss: 49.627887 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 304/500\n",
      "[Mini-batch] Average Loss: 50.257967\n",
      "Validation Loss: 49.503133 | Validation MAPE: 23.06%\n",
      "\n",
      "Epoch 305/500\n",
      "[Mini-batch] Average Loss: 50.717108\n",
      "Validation Loss: 50.441174 | Validation MAPE: 22.68%\n",
      "\n",
      "Epoch 306/500\n",
      "[Mini-batch] Average Loss: 50.113570\n",
      "Validation Loss: 49.667519 | Validation MAPE: 22.44%\n",
      "\n",
      "Epoch 307/500\n",
      "[Mini-batch] Average Loss: 50.405221\n",
      "Validation Loss: 49.623599 | Validation MAPE: 23.36%\n",
      "\n",
      "Epoch 308/500\n",
      "[Mini-batch] Average Loss: 50.409882\n",
      "Validation Loss: 49.788943 | Validation MAPE: 23.77%\n",
      "\n",
      "Epoch 309/500\n",
      "[Mini-batch] Average Loss: 50.666682\n",
      "Validation Loss: 49.971232 | Validation MAPE: 22.71%\n",
      "\n",
      "Epoch 310/500\n",
      "[Mini-batch] Average Loss: 50.480166\n",
      "Validation Loss: 49.163566 | Validation MAPE: 22.88%\n",
      "\n",
      "Epoch 311/500\n",
      "[Mini-batch] Average Loss: 50.816110\n",
      "Validation Loss: 49.909743 | Validation MAPE: 24.07%\n",
      "\n",
      "Epoch 312/500\n",
      "[Mini-batch] Average Loss: 50.660890\n",
      "Validation Loss: 50.698376 | Validation MAPE: 23.88%\n",
      "\n",
      "Epoch 313/500\n",
      "[Mini-batch] Average Loss: 50.536022\n",
      "Validation Loss: 50.028639 | Validation MAPE: 23.48%\n",
      "\n",
      "Epoch 314/500\n",
      "[Mini-batch] Average Loss: 50.714932\n",
      "Validation Loss: 51.376311 | Validation MAPE: 22.78%\n",
      "\n",
      "Epoch 315/500\n",
      "[Mini-batch] Average Loss: 51.213643\n",
      "Validation Loss: 49.794321 | Validation MAPE: 23.44%\n",
      "\n",
      "Epoch 316/500\n",
      "[Mini-batch] Average Loss: 50.669379\n",
      "Validation Loss: 50.262970 | Validation MAPE: 22.62%\n",
      "\n",
      "Epoch 317/500\n",
      "[Mini-batch] Average Loss: 50.715835\n",
      "Validation Loss: 49.306354 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 318/500\n",
      "[Mini-batch] Average Loss: 50.459558\n",
      "Validation Loss: 50.192162 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 319/500\n",
      "[Mini-batch] Average Loss: 50.446786\n",
      "Validation Loss: 49.765946 | Validation MAPE: 22.23%\n",
      "\n",
      "Epoch 320/500\n",
      "[Mini-batch] Average Loss: 50.584499\n",
      "Validation Loss: 49.283914 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 321/500\n",
      "[Mini-batch] Average Loss: 50.894220\n",
      "Validation Loss: 49.374716 | Validation MAPE: 22.69%\n",
      "\n",
      "Epoch 322/500\n",
      "[Mini-batch] Average Loss: 50.355431\n",
      "Validation Loss: 50.077226 | Validation MAPE: 23.79%\n",
      "\n",
      "Epoch 323/500\n",
      "[Mini-batch] Average Loss: 50.492333\n",
      "Validation Loss: 50.035460 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 324/500\n",
      "[Mini-batch] Average Loss: 50.533915\n",
      "Validation Loss: 51.398774 | Validation MAPE: 22.80%\n",
      "\n",
      "Epoch 325/500\n",
      "[Mini-batch] Average Loss: 50.420074\n",
      "Validation Loss: 48.938931 | Validation MAPE: 22.73%\n",
      "\n",
      "Epoch 326/500\n",
      "[Mini-batch] Average Loss: 50.507103\n",
      "Validation Loss: 50.052420 | Validation MAPE: 24.26%\n",
      "\n",
      "Epoch 327/500\n",
      "[Mini-batch] Average Loss: 50.586688\n",
      "Validation Loss: 49.060039 | Validation MAPE: 23.23%\n",
      "\n",
      "Epoch 328/500\n",
      "[Mini-batch] Average Loss: 50.760921\n",
      "Validation Loss: 49.837988 | Validation MAPE: 22.08%\n",
      "\n",
      "Epoch 329/500\n",
      "[Mini-batch] Average Loss: 51.156553\n",
      "Validation Loss: 49.449513 | Validation MAPE: 22.87%\n",
      "\n",
      "Epoch 330/500\n",
      "[Mini-batch] Average Loss: 50.976079\n",
      "Validation Loss: 49.650979 | Validation MAPE: 23.63%\n",
      "\n",
      "Epoch 331/500\n",
      "[Mini-batch] Average Loss: 50.280312\n",
      "Validation Loss: 49.253505 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 332/500\n",
      "[Mini-batch] Average Loss: 50.349716\n",
      "Validation Loss: 49.376961 | Validation MAPE: 23.81%\n",
      "\n",
      "Epoch 333/500\n",
      "[Mini-batch] Average Loss: 50.598471\n",
      "Validation Loss: 49.827458 | Validation MAPE: 24.09%\n",
      "\n",
      "Epoch 334/500\n",
      "[Mini-batch] Average Loss: 50.746884\n",
      "Validation Loss: 49.521339 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 335/500\n",
      "[Mini-batch] Average Loss: 50.885143\n",
      "Validation Loss: 51.047673 | Validation MAPE: 23.43%\n",
      "\n",
      "Epoch 336/500\n",
      "[Mini-batch] Average Loss: 51.289243\n",
      "Validation Loss: 49.466582 | Validation MAPE: 22.76%\n",
      "\n",
      "Epoch 337/500\n",
      "[Mini-batch] Average Loss: 50.544354\n",
      "Validation Loss: 50.000008 | Validation MAPE: 23.68%\n",
      "\n",
      "Epoch 338/500\n",
      "[Mini-batch] Average Loss: 50.571805\n",
      "Validation Loss: 50.233192 | Validation MAPE: 22.65%\n",
      "\n",
      "Epoch 339/500\n",
      "[Mini-batch] Average Loss: 50.777870\n",
      "Validation Loss: 49.351609 | Validation MAPE: 23.85%\n",
      "\n",
      "Epoch 340/500\n",
      "[Mini-batch] Average Loss: 51.018001\n",
      "Validation Loss: 48.957315 | Validation MAPE: 22.46%\n",
      "\n",
      "Epoch 341/500\n",
      "[Mini-batch] Average Loss: 50.998755\n",
      "Validation Loss: 50.550955 | Validation MAPE: 24.17%\n",
      "\n",
      "Epoch 342/500\n",
      "[Mini-batch] Average Loss: 50.458823\n",
      "Validation Loss: 50.490300 | Validation MAPE: 22.76%\n",
      "\n",
      "Epoch 343/500\n",
      "[Mini-batch] Average Loss: 50.779951\n",
      "Validation Loss: 50.202802 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 344/500\n",
      "[Mini-batch] Average Loss: 51.037928\n",
      "Validation Loss: 49.262179 | Validation MAPE: 22.72%\n",
      "\n",
      "Epoch 345/500\n",
      "[Mini-batch] Average Loss: 50.365857\n",
      "Validation Loss: 50.838525 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 346/500\n",
      "[Mini-batch] Average Loss: 50.863518\n",
      "Validation Loss: 50.204129 | Validation MAPE: 23.07%\n",
      "\n",
      "Epoch 347/500\n",
      "[Mini-batch] Average Loss: 50.514017\n",
      "Validation Loss: 50.813552 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 348/500\n",
      "[Mini-batch] Average Loss: 50.552820\n",
      "Validation Loss: 49.860381 | Validation MAPE: 22.88%\n",
      "\n",
      "Epoch 349/500\n",
      "[Mini-batch] Average Loss: 50.495054\n",
      "Validation Loss: 50.108894 | Validation MAPE: 22.91%\n",
      "\n",
      "Epoch 350/500\n",
      "[Mini-batch] Average Loss: 50.506462\n",
      "Validation Loss: 51.614259 | Validation MAPE: 22.30%\n",
      "\n",
      "Epoch 351/500\n",
      "[Mini-batch] Average Loss: 50.596573\n",
      "Validation Loss: 50.184325 | Validation MAPE: 23.63%\n",
      "\n",
      "Epoch 352/500\n",
      "[Mini-batch] Average Loss: 50.459387\n",
      "Validation Loss: 48.883790 | Validation MAPE: 22.56%\n",
      "\n",
      "Epoch 353/500\n",
      "[Mini-batch] Average Loss: 50.478413\n",
      "Validation Loss: 50.932267 | Validation MAPE: 22.39%\n",
      "\n",
      "Epoch 354/500\n",
      "[Mini-batch] Average Loss: 50.582336\n",
      "Validation Loss: 50.094839 | Validation MAPE: 22.97%\n",
      "\n",
      "Epoch 355/500\n",
      "[Mini-batch] Average Loss: 50.058820\n",
      "Validation Loss: 50.279658 | Validation MAPE: 23.73%\n",
      "\n",
      "Epoch 356/500\n",
      "[Mini-batch] Average Loss: 50.504752\n",
      "Validation Loss: 49.048040 | Validation MAPE: 22.90%\n",
      "\n",
      "Epoch 357/500\n",
      "[Mini-batch] Average Loss: 50.711852\n",
      "Validation Loss: 49.617335 | Validation MAPE: 23.56%\n",
      "\n",
      "Epoch 358/500\n",
      "[Mini-batch] Average Loss: 50.536130\n",
      "Validation Loss: 49.073138 | Validation MAPE: 22.54%\n",
      "\n",
      "Epoch 359/500\n",
      "[Mini-batch] Average Loss: 50.578759\n",
      "Validation Loss: 49.811084 | Validation MAPE: 23.30%\n",
      "\n",
      "Epoch 360/500\n",
      "[Mini-batch] Average Loss: 50.446138\n",
      "Validation Loss: 49.597399 | Validation MAPE: 23.27%\n",
      "\n",
      "Epoch 361/500\n",
      "[Mini-batch] Average Loss: 50.824708\n",
      "Validation Loss: 50.520413 | Validation MAPE: 22.16%\n",
      "\n",
      "Epoch 362/500\n",
      "[Mini-batch] Average Loss: 50.973860\n",
      "Validation Loss: 49.903205 | Validation MAPE: 23.03%\n",
      "\n",
      "Epoch 363/500\n",
      "[Mini-batch] Average Loss: 50.936570\n",
      "Validation Loss: 49.389388 | Validation MAPE: 22.33%\n",
      "\n",
      "Epoch 364/500\n",
      "[Mini-batch] Average Loss: 50.552657\n",
      "Validation Loss: 49.550782 | Validation MAPE: 22.63%\n",
      "\n",
      "Epoch 365/500\n",
      "[Mini-batch] Average Loss: 50.994843\n",
      "Validation Loss: 49.536631 | Validation MAPE: 23.62%\n",
      "\n",
      "Epoch 366/500\n",
      "[Mini-batch] Average Loss: 50.500871\n",
      "Validation Loss: 50.216766 | Validation MAPE: 22.26%\n",
      "\n",
      "Epoch 367/500\n",
      "[Mini-batch] Average Loss: 50.360764\n",
      "Validation Loss: 48.606229 | Validation MAPE: 22.56%\n",
      "\n",
      "Epoch 368/500\n",
      "[Mini-batch] Average Loss: 50.588273\n",
      "Validation Loss: 48.769688 | Validation MAPE: 22.59%\n",
      "\n",
      "Epoch 369/500\n",
      "[Mini-batch] Average Loss: 50.557517\n",
      "Validation Loss: 49.888001 | Validation MAPE: 23.43%\n",
      "\n",
      "Epoch 370/500\n",
      "[Mini-batch] Average Loss: 50.521043\n",
      "Validation Loss: 50.076562 | Validation MAPE: 22.83%\n",
      "\n",
      "Epoch 371/500\n",
      "[Mini-batch] Average Loss: 50.434971\n",
      "Validation Loss: 49.845900 | Validation MAPE: 23.36%\n",
      "\n",
      "Epoch 372/500\n",
      "[Mini-batch] Average Loss: 50.604331\n",
      "Validation Loss: 50.402752 | Validation MAPE: 23.12%\n",
      "\n",
      "Epoch 373/500\n",
      "[Mini-batch] Average Loss: 50.783564\n",
      "Validation Loss: 50.131882 | Validation MAPE: 22.59%\n",
      "\n",
      "Epoch 374/500\n",
      "[Mini-batch] Average Loss: 50.618209\n",
      "Validation Loss: 49.514809 | Validation MAPE: 22.24%\n",
      "\n",
      "Epoch 375/500\n",
      "[Mini-batch] Average Loss: 50.343605\n",
      "Validation Loss: 49.590811 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 376/500\n",
      "[Mini-batch] Average Loss: 50.583309\n",
      "Validation Loss: 48.909724 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 377/500\n",
      "[Mini-batch] Average Loss: 51.119320\n",
      "Validation Loss: 51.582574 | Validation MAPE: 23.16%\n",
      "\n",
      "Epoch 378/500\n",
      "[Mini-batch] Average Loss: 50.300633\n",
      "Validation Loss: 50.025704 | Validation MAPE: 23.34%\n",
      "\n",
      "Epoch 379/500\n",
      "[Mini-batch] Average Loss: 50.866552\n",
      "Validation Loss: 50.425273 | Validation MAPE: 24.93%\n",
      "\n",
      "Epoch 380/500\n",
      "[Mini-batch] Average Loss: 50.860271\n",
      "Validation Loss: 49.724254 | Validation MAPE: 22.98%\n",
      "\n",
      "Epoch 381/500\n",
      "[Mini-batch] Average Loss: 50.526481\n",
      "Validation Loss: 49.579673 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 382/500\n",
      "[Mini-batch] Average Loss: 51.195909\n",
      "Validation Loss: 49.022452 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 383/500\n",
      "[Mini-batch] Average Loss: 50.628451\n",
      "Validation Loss: 49.662591 | Validation MAPE: 23.46%\n",
      "\n",
      "Epoch 384/500\n",
      "[Mini-batch] Average Loss: 50.663818\n",
      "Validation Loss: 49.234363 | Validation MAPE: 22.77%\n",
      "\n",
      "Epoch 385/500\n",
      "[Mini-batch] Average Loss: 50.422170\n",
      "Validation Loss: 51.269227 | Validation MAPE: 23.98%\n",
      "\n",
      "Epoch 386/500\n",
      "[Mini-batch] Average Loss: 50.590255\n",
      "Validation Loss: 49.385800 | Validation MAPE: 23.57%\n",
      "\n",
      "Epoch 387/500\n",
      "[Mini-batch] Average Loss: 50.577016\n",
      "Validation Loss: 50.010655 | Validation MAPE: 23.73%\n",
      "\n",
      "Epoch 388/500\n",
      "[Mini-batch] Average Loss: 50.464020\n",
      "Validation Loss: 49.605197 | Validation MAPE: 23.03%\n",
      "\n",
      "Epoch 389/500\n",
      "[Mini-batch] Average Loss: 50.526487\n",
      "Validation Loss: 49.182535 | Validation MAPE: 23.04%\n",
      "\n",
      "Epoch 390/500\n",
      "[Mini-batch] Average Loss: 50.583511\n",
      "Validation Loss: 52.209512 | Validation MAPE: 21.80%\n",
      "\n",
      "Epoch 391/500\n",
      "[Mini-batch] Average Loss: 50.252284\n",
      "Validation Loss: 49.528684 | Validation MAPE: 23.29%\n",
      "\n",
      "Epoch 392/500\n",
      "[Mini-batch] Average Loss: 50.427919\n",
      "Validation Loss: 49.297051 | Validation MAPE: 23.02%\n",
      "\n",
      "Epoch 393/500\n",
      "[Mini-batch] Average Loss: 50.304338\n",
      "Validation Loss: 49.687510 | Validation MAPE: 22.41%\n",
      "\n",
      "Epoch 394/500\n",
      "[Mini-batch] Average Loss: 50.630118\n",
      "Validation Loss: 50.408456 | Validation MAPE: 24.72%\n",
      "\n",
      "Epoch 395/500\n",
      "[Mini-batch] Average Loss: 50.559495\n",
      "Validation Loss: 49.937831 | Validation MAPE: 23.38%\n",
      "\n",
      "Epoch 396/500\n",
      "[Mini-batch] Average Loss: 50.505664\n",
      "Validation Loss: 49.670004 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 397/500\n",
      "[Mini-batch] Average Loss: 50.538645\n",
      "Validation Loss: 50.254512 | Validation MAPE: 22.67%\n",
      "\n",
      "Epoch 398/500\n",
      "[Mini-batch] Average Loss: 50.699464\n",
      "Validation Loss: 49.529094 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 399/500\n",
      "[Mini-batch] Average Loss: 50.642714\n",
      "Validation Loss: 49.413423 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 400/500\n",
      "[Mini-batch] Average Loss: 50.491670\n",
      "Validation Loss: 51.790029 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 401/500\n",
      "[Mini-batch] Average Loss: 50.507355\n",
      "Validation Loss: 50.064603 | Validation MAPE: 23.49%\n",
      "\n",
      "Epoch 402/500\n",
      "[Mini-batch] Average Loss: 50.802222\n",
      "Validation Loss: 49.087882 | Validation MAPE: 23.35%\n",
      "\n",
      "Epoch 403/500\n",
      "[Mini-batch] Average Loss: 50.757042\n",
      "Validation Loss: 49.537121 | Validation MAPE: 23.38%\n",
      "\n",
      "Epoch 404/500\n",
      "[Mini-batch] Average Loss: 50.844887\n",
      "Validation Loss: 48.970124 | Validation MAPE: 22.78%\n",
      "\n",
      "Epoch 405/500\n",
      "[Mini-batch] Average Loss: 50.559312\n",
      "Validation Loss: 49.952459 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 406/500\n",
      "[Mini-batch] Average Loss: 50.651487\n",
      "Validation Loss: 49.482856 | Validation MAPE: 23.59%\n",
      "\n",
      "Epoch 407/500\n",
      "[Mini-batch] Average Loss: 50.702384\n",
      "Validation Loss: 50.375868 | Validation MAPE: 22.04%\n",
      "\n",
      "Epoch 408/500\n",
      "[Mini-batch] Average Loss: 50.781301\n",
      "Validation Loss: 48.860116 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 409/500\n",
      "[Mini-batch] Average Loss: 50.470973\n",
      "Validation Loss: 49.043776 | Validation MAPE: 22.91%\n",
      "\n",
      "Epoch 410/500\n",
      "[Mini-batch] Average Loss: 50.403598\n",
      "Validation Loss: 49.902471 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 411/500\n",
      "[Mini-batch] Average Loss: 50.538758\n",
      "Validation Loss: 50.046038 | Validation MAPE: 22.21%\n",
      "\n",
      "Epoch 412/500\n",
      "[Mini-batch] Average Loss: 50.677211\n",
      "Validation Loss: 49.939722 | Validation MAPE: 23.27%\n",
      "\n",
      "Epoch 413/500\n",
      "[Mini-batch] Average Loss: 50.927971\n",
      "Validation Loss: 49.813946 | Validation MAPE: 22.75%\n",
      "\n",
      "Epoch 414/500\n",
      "[Mini-batch] Average Loss: 50.644536\n",
      "Validation Loss: 49.806396 | Validation MAPE: 23.87%\n",
      "\n",
      "Epoch 415/500\n",
      "[Mini-batch] Average Loss: 50.539585\n",
      "Validation Loss: 49.487879 | Validation MAPE: 23.11%\n",
      "\n",
      "Epoch 416/500\n",
      "[Mini-batch] Average Loss: 50.563820\n",
      "Validation Loss: 49.993944 | Validation MAPE: 23.68%\n",
      "\n",
      "Epoch 417/500\n",
      "[Mini-batch] Average Loss: 50.079050\n",
      "Validation Loss: 49.196527 | Validation MAPE: 23.09%\n",
      "\n",
      "Epoch 418/500\n",
      "[Mini-batch] Average Loss: 50.406737\n",
      "Validation Loss: 49.025202 | Validation MAPE: 23.01%\n",
      "\n",
      "Epoch 419/500\n",
      "[Mini-batch] Average Loss: 50.303661\n",
      "Validation Loss: 50.812174 | Validation MAPE: 22.53%\n",
      "\n",
      "Epoch 420/500\n",
      "[Mini-batch] Average Loss: 50.732641\n",
      "Validation Loss: 50.634675 | Validation MAPE: 22.61%\n",
      "\n",
      "Epoch 421/500\n",
      "[Mini-batch] Average Loss: 50.512199\n",
      "Validation Loss: 49.757137 | Validation MAPE: 22.15%\n",
      "\n",
      "Epoch 422/500\n",
      "[Mini-batch] Average Loss: 50.571662\n",
      "Validation Loss: 49.367950 | Validation MAPE: 23.15%\n",
      "\n",
      "Epoch 423/500\n",
      "[Mini-batch] Average Loss: 50.470981\n",
      "Validation Loss: 50.254736 | Validation MAPE: 22.23%\n",
      "\n",
      "Epoch 424/500\n",
      "[Mini-batch] Average Loss: 50.502289\n",
      "Validation Loss: 49.324567 | Validation MAPE: 23.20%\n",
      "\n",
      "Epoch 425/500\n",
      "[Mini-batch] Average Loss: 50.585298\n",
      "Validation Loss: 50.478457 | Validation MAPE: 23.52%\n",
      "\n",
      "Epoch 426/500\n",
      "[Mini-batch] Average Loss: 50.321638\n",
      "Validation Loss: 49.387417 | Validation MAPE: 22.76%\n",
      "\n",
      "Epoch 427/500\n",
      "[Mini-batch] Average Loss: 50.688785\n",
      "Validation Loss: 49.475265 | Validation MAPE: 23.02%\n",
      "\n",
      "Epoch 428/500\n",
      "[Mini-batch] Average Loss: 50.843094\n",
      "Validation Loss: 49.320822 | Validation MAPE: 23.32%\n",
      "\n",
      "Epoch 429/500\n",
      "[Mini-batch] Average Loss: 50.693910\n",
      "Validation Loss: 50.263966 | Validation MAPE: 22.20%\n",
      "\n",
      "Epoch 430/500\n",
      "[Mini-batch] Average Loss: 50.679742\n",
      "Validation Loss: 49.632504 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 431/500\n",
      "[Mini-batch] Average Loss: 50.323126\n",
      "Validation Loss: 49.389322 | Validation MAPE: 23.40%\n",
      "\n",
      "Epoch 432/500\n",
      "[Mini-batch] Average Loss: 50.846096\n",
      "Validation Loss: 49.874019 | Validation MAPE: 23.80%\n",
      "\n",
      "Epoch 433/500\n",
      "[Mini-batch] Average Loss: 50.624922\n",
      "Validation Loss: 50.198217 | Validation MAPE: 23.33%\n",
      "\n",
      "Epoch 434/500\n",
      "[Mini-batch] Average Loss: 50.607996\n",
      "Validation Loss: 49.761485 | Validation MAPE: 22.47%\n",
      "\n",
      "Epoch 435/500\n",
      "[Mini-batch] Average Loss: 50.811994\n",
      "Validation Loss: 49.986585 | Validation MAPE: 24.42%\n",
      "\n",
      "Epoch 436/500\n",
      "[Mini-batch] Average Loss: 50.943134\n",
      "Validation Loss: 50.069508 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 437/500\n",
      "[Mini-batch] Average Loss: 50.593732\n",
      "Validation Loss: 49.285160 | Validation MAPE: 22.17%\n",
      "\n",
      "Epoch 438/500\n",
      "[Mini-batch] Average Loss: 50.826957\n",
      "Validation Loss: 50.908093 | Validation MAPE: 21.85%\n",
      "\n",
      "Epoch 439/500\n",
      "[Mini-batch] Average Loss: 50.668642\n",
      "Validation Loss: 48.954018 | Validation MAPE: 22.56%\n",
      "\n",
      "Epoch 440/500\n",
      "[Mini-batch] Average Loss: 50.568343\n",
      "Validation Loss: 50.538984 | Validation MAPE: 22.67%\n",
      "\n",
      "Epoch 441/500\n",
      "[Mini-batch] Average Loss: 50.620038\n",
      "Validation Loss: 48.961277 | Validation MAPE: 23.30%\n",
      "\n",
      "Epoch 442/500\n",
      "[Mini-batch] Average Loss: 50.642672\n",
      "Validation Loss: 50.209836 | Validation MAPE: 23.93%\n",
      "\n",
      "Epoch 443/500\n",
      "[Mini-batch] Average Loss: 50.356276\n",
      "Validation Loss: 48.687454 | Validation MAPE: 22.64%\n",
      "\n",
      "Epoch 444/500\n",
      "[Mini-batch] Average Loss: 50.323007\n",
      "Validation Loss: 49.157709 | Validation MAPE: 23.51%\n",
      "\n",
      "Epoch 445/500\n",
      "[Mini-batch] Average Loss: 50.187342\n",
      "Validation Loss: 50.400309 | Validation MAPE: 23.92%\n",
      "\n",
      "Epoch 446/500\n",
      "[Mini-batch] Average Loss: 50.326779\n",
      "Validation Loss: 49.808423 | Validation MAPE: 22.32%\n",
      "\n",
      "Epoch 447/500\n",
      "[Mini-batch] Average Loss: 50.478565\n",
      "Validation Loss: 49.004492 | Validation MAPE: 22.81%\n",
      "\n",
      "Epoch 448/500\n",
      "[Mini-batch] Average Loss: 50.410099\n",
      "Validation Loss: 49.087842 | Validation MAPE: 22.89%\n",
      "\n",
      "Epoch 449/500\n",
      "[Mini-batch] Average Loss: 50.464999\n",
      "Validation Loss: 49.338599 | Validation MAPE: 22.91%\n",
      "\n",
      "Epoch 450/500\n",
      "[Mini-batch] Average Loss: 50.407955\n",
      "Validation Loss: 49.367268 | Validation MAPE: 22.50%\n",
      "\n",
      "Epoch 451/500\n",
      "[Mini-batch] Average Loss: 50.429286\n",
      "Validation Loss: 49.459483 | Validation MAPE: 24.09%\n",
      "\n",
      "Epoch 452/500\n",
      "[Mini-batch] Average Loss: 50.366973\n",
      "Validation Loss: 48.657397 | Validation MAPE: 23.00%\n",
      "\n",
      "Epoch 453/500\n",
      "[Mini-batch] Average Loss: 50.868817\n",
      "Validation Loss: 51.302883 | Validation MAPE: 22.39%\n",
      "\n",
      "Epoch 454/500\n",
      "[Mini-batch] Average Loss: 50.438747\n",
      "Validation Loss: 49.602451 | Validation MAPE: 23.92%\n",
      "\n",
      "Epoch 455/500\n",
      "[Mini-batch] Average Loss: 50.505719\n",
      "Validation Loss: 50.305884 | Validation MAPE: 25.05%\n",
      "\n",
      "Epoch 456/500\n",
      "[Mini-batch] Average Loss: 50.346919\n",
      "Validation Loss: 49.843864 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 457/500\n",
      "[Mini-batch] Average Loss: 50.242037\n",
      "Validation Loss: 50.903590 | Validation MAPE: 23.47%\n",
      "\n",
      "Epoch 458/500\n",
      "[Mini-batch] Average Loss: 50.313815\n",
      "Validation Loss: 49.169598 | Validation MAPE: 22.29%\n",
      "\n",
      "Epoch 459/500\n",
      "[Mini-batch] Average Loss: 50.433752\n",
      "Validation Loss: 48.978245 | Validation MAPE: 22.97%\n",
      "\n",
      "Epoch 460/500\n",
      "[Mini-batch] Average Loss: 49.959228\n",
      "Validation Loss: 50.493460 | Validation MAPE: 24.94%\n",
      "\n",
      "Epoch 461/500\n",
      "[Mini-batch] Average Loss: 50.216151\n",
      "Validation Loss: 49.311059 | Validation MAPE: 22.15%\n",
      "\n",
      "Epoch 462/500\n",
      "[Mini-batch] Average Loss: 50.678150\n",
      "Validation Loss: 49.910242 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 463/500\n",
      "[Mini-batch] Average Loss: 50.542500\n",
      "Validation Loss: 49.520911 | Validation MAPE: 23.26%\n",
      "\n",
      "Epoch 464/500\n",
      "[Mini-batch] Average Loss: 50.493083\n",
      "Validation Loss: 49.340463 | Validation MAPE: 23.42%\n",
      "\n",
      "Epoch 465/500\n",
      "[Mini-batch] Average Loss: 50.261482\n",
      "Validation Loss: 49.325151 | Validation MAPE: 22.90%\n",
      "\n",
      "Epoch 466/500\n",
      "[Mini-batch] Average Loss: 50.722280\n",
      "Validation Loss: 49.410222 | Validation MAPE: 22.48%\n",
      "\n",
      "Epoch 467/500\n",
      "[Mini-batch] Average Loss: 50.836274\n",
      "Validation Loss: 49.002449 | Validation MAPE: 23.42%\n",
      "\n",
      "Epoch 468/500\n",
      "[Mini-batch] Average Loss: 50.444004\n",
      "Validation Loss: 49.369327 | Validation MAPE: 22.59%\n",
      "\n",
      "Epoch 469/500\n",
      "[Mini-batch] Average Loss: 50.006332\n",
      "Validation Loss: 50.483798 | Validation MAPE: 24.07%\n",
      "\n",
      "Epoch 470/500\n",
      "[Mini-batch] Average Loss: 50.536415\n",
      "Validation Loss: 48.878209 | Validation MAPE: 22.75%\n",
      "\n",
      "Epoch 471/500\n",
      "[Mini-batch] Average Loss: 50.528493\n",
      "Validation Loss: 49.254604 | Validation MAPE: 22.60%\n",
      "\n",
      "Epoch 472/500\n",
      "[Mini-batch] Average Loss: 50.422448\n",
      "Validation Loss: 49.120219 | Validation MAPE: 23.88%\n",
      "\n",
      "Epoch 473/500\n",
      "[Mini-batch] Average Loss: 50.351808\n",
      "Validation Loss: 49.977425 | Validation MAPE: 23.36%\n",
      "\n",
      "Epoch 474/500\n",
      "[Mini-batch] Average Loss: 50.103283\n",
      "Validation Loss: 49.359743 | Validation MAPE: 22.73%\n",
      "\n",
      "Epoch 475/500\n",
      "[Mini-batch] Average Loss: 50.400289\n",
      "Validation Loss: 50.155935 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 476/500\n",
      "[Mini-batch] Average Loss: 50.561415\n",
      "Validation Loss: 50.464215 | Validation MAPE: 23.31%\n",
      "\n",
      "Epoch 477/500\n",
      "[Mini-batch] Average Loss: 50.747283\n",
      "Validation Loss: 49.315725 | Validation MAPE: 23.08%\n",
      "\n",
      "Epoch 478/500\n",
      "[Mini-batch] Average Loss: 50.507858\n",
      "Validation Loss: 50.506523 | Validation MAPE: 22.73%\n",
      "\n",
      "Epoch 479/500\n",
      "[Mini-batch] Average Loss: 50.828646\n",
      "Validation Loss: 50.764790 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 480/500\n",
      "[Mini-batch] Average Loss: 50.522379\n",
      "Validation Loss: 50.711510 | Validation MAPE: 22.87%\n",
      "\n",
      "Epoch 481/500\n",
      "[Mini-batch] Average Loss: 50.521886\n",
      "Validation Loss: 50.335773 | Validation MAPE: 22.23%\n",
      "\n",
      "Epoch 482/500\n",
      "[Mini-batch] Average Loss: 50.300290\n",
      "Validation Loss: 48.970766 | Validation MAPE: 22.38%\n",
      "\n",
      "Epoch 483/500\n",
      "[Mini-batch] Average Loss: 50.440289\n",
      "Validation Loss: 48.900712 | Validation MAPE: 22.95%\n",
      "\n",
      "Epoch 484/500\n",
      "[Mini-batch] Average Loss: 50.560687\n",
      "Validation Loss: 50.546592 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 485/500\n",
      "[Mini-batch] Average Loss: 50.745828\n",
      "Validation Loss: 48.950895 | Validation MAPE: 22.60%\n",
      "\n",
      "Epoch 486/500\n",
      "[Mini-batch] Average Loss: 50.377851\n",
      "Validation Loss: 48.838443 | Validation MAPE: 22.55%\n",
      "\n",
      "Epoch 487/500\n",
      "[Mini-batch] Average Loss: 50.267602\n",
      "Validation Loss: 49.497378 | Validation MAPE: 22.20%\n",
      "\n",
      "Epoch 488/500\n",
      "[Mini-batch] Average Loss: 50.343926\n",
      "Validation Loss: 49.780190 | Validation MAPE: 22.40%\n",
      "\n",
      "Epoch 489/500\n",
      "[Mini-batch] Average Loss: 51.361250\n",
      "Validation Loss: 49.633714 | Validation MAPE: 23.13%\n",
      "\n",
      "Epoch 490/500\n",
      "[Mini-batch] Average Loss: 50.481911\n",
      "Validation Loss: 49.377242 | Validation MAPE: 22.70%\n",
      "\n",
      "Epoch 491/500\n",
      "[Mini-batch] Average Loss: 50.365970\n",
      "Validation Loss: 49.177784 | Validation MAPE: 22.98%\n",
      "\n",
      "Epoch 492/500\n",
      "[Mini-batch] Average Loss: 50.632026\n",
      "Validation Loss: 50.150375 | Validation MAPE: 22.13%\n",
      "\n",
      "Epoch 493/500\n",
      "[Mini-batch] Average Loss: 50.340371\n",
      "Validation Loss: 49.648872 | Validation MAPE: 23.96%\n",
      "\n",
      "Epoch 494/500\n",
      "[Mini-batch] Average Loss: 50.164667\n",
      "Validation Loss: 49.924368 | Validation MAPE: 23.63%\n",
      "\n",
      "Epoch 495/500\n",
      "[Mini-batch] Average Loss: 50.538964\n",
      "Validation Loss: 49.248105 | Validation MAPE: 22.34%\n",
      "\n",
      "Epoch 496/500\n",
      "[Mini-batch] Average Loss: 50.499350\n",
      "Validation Loss: 49.673893 | Validation MAPE: 24.27%\n",
      "\n",
      "Epoch 497/500\n",
      "[Mini-batch] Average Loss: 50.592032\n",
      "Validation Loss: 48.587938 | Validation MAPE: 22.96%\n",
      "\n",
      "Epoch 498/500\n",
      "[Mini-batch] Average Loss: 50.830567\n",
      "Validation Loss: 50.257831 | Validation MAPE: 23.54%\n",
      "\n",
      "Epoch 499/500\n",
      "[Mini-batch] Average Loss: 50.763063\n",
      "Validation Loss: 51.536983 | Validation MAPE: 25.08%\n",
      "\n",
      "Epoch 500/500\n",
      "[Mini-batch] Average Loss: 50.696483\n",
      "Validation Loss: 50.709644 | Validation MAPE: 22.21%\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "# Train/val split (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "# Define and train model\n",
    "nn = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")\n",
    "\n",
    "nn.train(X_train, y_train, \n",
    "      lr=0.01, \n",
    "      epochs= 500,\n",
    "      method=\"Mini Batch\", \n",
    "      batch_size=32, \n",
    "      alpha=0.001, \n",
    "      X_val=X_val, y_val=y_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
