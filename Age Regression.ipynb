{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e61ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8ff8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.686191</td>\n",
       "      <td>-0.989465</td>\n",
       "      <td>-0.920503</td>\n",
       "      <td>1.607427</td>\n",
       "      <td>-0.896248</td>\n",
       "      <td>1.118974</td>\n",
       "      <td>-0.969456</td>\n",
       "      <td>1.811707</td>\n",
       "      <td>2.560955</td>\n",
       "      <td>3.803463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862891</td>\n",
       "      <td>-0.909545</td>\n",
       "      <td>-0.915361</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>-0.989461</td>\n",
       "      <td>1.911855</td>\n",
       "      <td>1.409705</td>\n",
       "      <td>2.303997</td>\n",
       "      <td>-0.981840</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887917</td>\n",
       "      <td>4.915272</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>-0.964685</td>\n",
       "      <td>-0.478649</td>\n",
       "      <td>4.342395</td>\n",
       "      <td>-0.332870</td>\n",
       "      <td>-0.768041</td>\n",
       "      <td>-0.815375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.965917</td>\n",
       "      <td>-0.969461</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>5.304822</td>\n",
       "      <td>0.934790</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>4.919212</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.923215</td>\n",
       "      <td>2.746968</td>\n",
       "      <td>-0.918085</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.908587</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>2.984481</td>\n",
       "      <td>0.535007</td>\n",
       "      <td>-0.591029</td>\n",
       "      <td>-0.324043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809726</td>\n",
       "      <td>-0.929934</td>\n",
       "      <td>-0.891814</td>\n",
       "      <td>-0.881796</td>\n",
       "      <td>3.415373</td>\n",
       "      <td>1.044108</td>\n",
       "      <td>-0.442615</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>2.628199</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.268866</td>\n",
       "      <td>-0.408416</td>\n",
       "      <td>-0.935145</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>-0.922438</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>-0.046606</td>\n",
       "      <td>1.149634</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.357959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.834968</td>\n",
       "      <td>-0.937475</td>\n",
       "      <td>-0.917737</td>\n",
       "      <td>-0.929519</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>1.608048</td>\n",
       "      <td>0.276169</td>\n",
       "      <td>1.246468</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529231</td>\n",
       "      <td>-0.829957</td>\n",
       "      <td>-0.897425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>-0.865304</td>\n",
       "      <td>0.331018</td>\n",
       "      <td>-0.644940</td>\n",
       "      <td>1.296097</td>\n",
       "      <td>1.166863</td>\n",
       "      <td>2.036034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775411</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-0.864018</td>\n",
       "      <td>-0.908001</td>\n",
       "      <td>-0.784495</td>\n",
       "      <td>1.329586</td>\n",
       "      <td>0.547925</td>\n",
       "      <td>1.195395</td>\n",
       "      <td>-0.810089</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
       "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
       "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
       "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
       "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
       "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
       "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
       "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
       "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
       "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
       "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
       "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
       "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/AGE_PREDICTION.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c559dd1",
   "metadata": {},
   "source": [
    "## alallala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return 1-(self.activation(x)**2)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return self.activation(x)*(1-self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def mape(y_real, y_pred):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "    def dloss_dypred(y_real, y_pred):\n",
    "        return (y_pred-y_real)/y_real.shape[0]\n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = self.dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            da_dz = self.derivate_activation(zs[layer])\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "\n",
    "    def train(model, X, y, epochs, lr, method=\"Batch\", alpha = 0.5, batch_size=None, X_val = None, y_val = None):\n",
    "\n",
    "        if method != \"Batch\" and method != \"Mini Batch\" and method != \"SGD\":\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\" and \"SGD\".')\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "            print(f\"Mini-batch size: {batch_size}\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                # Use the entire dataset\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                loss = model.forward(X_batch, y_batch)\n",
    "                model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                model.update(lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    loss = model.forward(xi, yi)\n",
    "                    model.backward(xi, yi, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                    print(f\"[SGD] Sample {i+1}/{len(X_shuffled)} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    loss = model.forward(X_batch, y_batch)\n",
    "                    model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                    print(f\"[Mini-batch] Batch {num_batches} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "\n",
    "                y_pred_val = model.forward(X_val, y_val)[0]\n",
    "\n",
    "                val_loss = model.mse_loss(y_val, y_pred_val, alpha=alpha)\n",
    "\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Training completed!\\nFinal training loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb5bde",
   "metadata": {},
   "source": [
    "## Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ed6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    y_real: true target values\n",
    "    y_pred: predicted values\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "def dloss_dypred(y_real, y_pred):\n",
    "    return (y_pred-y_real)/y_real.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62527eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # ✅ Inizializzazione ADAM (dopo aver creato pesi e bias!)\n",
    "        self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_biases  = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_biases  = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0  # time step\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, a):\n",
    "        if self.act == 'tanh':\n",
    "            return 1 - a**2\n",
    "        elif self.act == 'sigmoid':\n",
    "            return a * (1 - a)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            a = activations[layer + 1]\n",
    "            da_dz = self.derivate_activation(a)\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0, keepdims=True))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "    def update_adam(self, grads_W, grads_b, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimizer.\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment time step\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            # ---- Update weights ----\n",
    "            self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * grads_W[i]\n",
    "            self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "\n",
    "            m_hat_w = self.m_weights[i] / (1 - beta1 ** self.t)\n",
    "            v_hat_w = self.v_weights[i] / (1 - beta2 ** self.t)\n",
    "\n",
    "            self.weights[i] -= lr * m_hat_w / (np.sqrt(v_hat_w) + eps)\n",
    "\n",
    "            # ---- Update biases ----\n",
    "            self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * grads_b[i]\n",
    "            self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "\n",
    "            m_hat_b = self.m_biases[i] / (1 - beta1 ** self.t)\n",
    "            v_hat_b = self.v_biases[i] / (1 - beta2 ** self.t)\n",
    "\n",
    "            self.biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs, lr, method=\"Batch\", alpha=0.5, batch_size=None, X_val=None, y_val=None):\n",
    "        if method not in [\"Batch\", \"Mini Batch\", \"SGD\"]:\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\", and \"SGD\".')\n",
    "\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                y_pred, _, _ = self.forward(X_batch)\n",
    "                loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    y_pred, _, _ = self.forward(xi)\n",
    "                    loss = self.mse_loss(yi, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(xi, yi, alpha)\n",
    "                    self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    y_pred, _, _ = self.forward(X_batch)\n",
    "                    loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                    self.update_adam(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            # Validation\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val, _, _ = self.forward(X_val)\n",
    "                val_loss = self.mse_loss(y_val, y_pred_val, alpha)\n",
    "                val_mape = mape(y_val, y_pred_val)\n",
    "                print(f\"Validation Loss: {val_loss:.6f} | Validation MAPE: {val_mape:.2f}%\")\n",
    "\n",
    "        print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b01dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(nn, X_test, y_test, alpha=0.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a trained NeuralNetwork instance on test data.\n",
    "\n",
    "    Args:\n",
    "        nn: trained NeuralNetwork instance\n",
    "        X_test, y_test: test data\n",
    "        alpha: regularization weight (set to 0 for pure performance)\n",
    "        verbose: whether to print the result\n",
    "\n",
    "    Returns:\n",
    "        test_loss: MSE (with optional reg)\n",
    "        test_mape: Mean Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    y_pred, _, _ = nn.forward(X_test)\n",
    "    \n",
    "    test_loss = nn.mse_loss(y_test, y_pred, alpha)\n",
    "    test_mape = mape(y_test, y_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n--- TEST EVALUATION ---\")\n",
    "        print(f\"Test Loss: {test_loss:.6f} | Test MAPE: {test_mape:.2f}%\")\n",
    "    \n",
    "    return test_loss, test_mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, y, alpha=0.0, epsilon=1e-5, verbose=True):\n",
    "    \"\"\"\n",
    "    Confronta gradienti numerici e analitici.\n",
    "    \n",
    "    Args:\n",
    "        nn: istanza della rete (già inizializzata)\n",
    "        X: batch di input\n",
    "        y: target corrispondente\n",
    "        alpha: regolarizzazione (L2)\n",
    "        epsilon: valore piccolo per differenze finite\n",
    "        verbose: se True stampa info\n",
    "\n",
    "    Returns:\n",
    "        max_diff: massima differenza tra gradiente analitico e numerico\n",
    "    \"\"\"\n",
    "    # Backup dei pesi originali\n",
    "    original_params = nn.get_params_vector()\n",
    "\n",
    "    # Calcolo gradienti analitici\n",
    "    grads_W, grads_b = nn.backward(X, y, alpha)\n",
    "    \n",
    "    # Flatten analitici in un solo vettore\n",
    "    analytical_grad = []\n",
    "    for gw, gb in zip(grads_W, grads_b):\n",
    "        analytical_grad.append(gw.flatten())\n",
    "        analytical_grad.append(gb.flatten())\n",
    "    analytical_grad = np.concatenate(analytical_grad)\n",
    "\n",
    "    # Calcolo gradienti numerici\n",
    "    num_grad = np.zeros_like(original_params)\n",
    "\n",
    "    for i in range(len(original_params)):\n",
    "        theta_plus = original_params.copy()\n",
    "        theta_minus = original_params.copy()\n",
    "\n",
    "        theta_plus[i] += epsilon\n",
    "        theta_minus[i] -= epsilon\n",
    "\n",
    "        nn.set_params_vector(theta_plus)\n",
    "        loss_plus = nn.mse_loss(y, nn.forward(X)[0], alpha)\n",
    "\n",
    "        nn.set_params_vector(theta_minus)\n",
    "        loss_minus = nn.mse_loss(y, nn.forward(X)[0], alpha)\n",
    "\n",
    "        num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "    # Ripristina i parametri originali\n",
    "    nn.set_params_vector(original_params)\n",
    "\n",
    "    # Confronto\n",
    "    diff = np.linalg.norm(num_grad - analytical_grad) / (np.linalg.norm(num_grad) + np.linalg.norm(analytical_grad) + 1e-8)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nGradient Check - Relative Difference: {diff:.10f}\")\n",
    "        if diff < 1e-6:\n",
    "            print(\"✅ Gradienti corretti!\")\n",
    "        elif diff < 1e-3:\n",
    "            print(\"⚠️  Gradienti forse accettabili, ma da controllare\")\n",
    "        else:\n",
    "            print(\"❌ Gradienti sbagliati: controlla backpropagation!\")\n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a71659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Check - Relative Difference: 0.0000000017\n",
      "✅ Gradienti corretti!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7232310446966924e-09"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piccolo batch di test (per non pesare troppo)\n",
    "\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "nn = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")\n",
    "\n",
    "# Take a random subset of 1000 rows\n",
    "df_subset = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "# Train/val split (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "X_batch = X_train[:500]\n",
    "y_batch = y_train[:500]\n",
    "\n",
    "gradient_check(nn, X_batch, y_batch, alpha=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb340d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"gt\", axis=1).values\n",
    "y = df[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "# Indices for 60/20/20 split\n",
    "n = len(X)\n",
    "train_end = int(0.6 * n)\n",
    "val_end   = int(0.8 * n)\n",
    "\n",
    "# Train, val, test sets\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val,   y_val   = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test,  y_test  = X[val_end:], y[val_end:]\n",
    "\n",
    "\n",
    "# Define and train model\n",
    "nn_adam = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34bedf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started using 'Mini Batch' method for 500 epochs.\n",
      "\n",
      "Epoch 1/500\n",
      "[Mini-batch] Average Loss: 51.375607\n",
      "Validation Loss: 58.719767 | Validation MAPE: 23.94%\n",
      "\n",
      "Epoch 2/500\n",
      "[Mini-batch] Average Loss: 50.299319\n",
      "Validation Loss: 49.896882 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 3/500\n",
      "[Mini-batch] Average Loss: 50.275198\n",
      "Validation Loss: 50.727294 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 4/500\n",
      "[Mini-batch] Average Loss: 50.085020\n",
      "Validation Loss: 51.094258 | Validation MAPE: 25.20%\n",
      "\n",
      "Epoch 5/500\n",
      "[Mini-batch] Average Loss: 50.793166\n",
      "Validation Loss: 54.885968 | Validation MAPE: 24.74%\n",
      "\n",
      "Epoch 6/500\n",
      "[Mini-batch] Average Loss: 50.988826\n",
      "Validation Loss: 49.820880 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 7/500\n",
      "[Mini-batch] Average Loss: 50.557418\n",
      "Validation Loss: 52.688245 | Validation MAPE: 26.50%\n",
      "\n",
      "Epoch 8/500\n",
      "[Mini-batch] Average Loss: 50.368532\n",
      "Validation Loss: 50.566702 | Validation MAPE: 23.88%\n",
      "\n",
      "Epoch 9/500\n",
      "[Mini-batch] Average Loss: 50.156657\n",
      "Validation Loss: 50.218471 | Validation MAPE: 24.59%\n",
      "\n",
      "Epoch 10/500\n",
      "[Mini-batch] Average Loss: 50.627003\n",
      "Validation Loss: 52.891401 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 11/500\n",
      "[Mini-batch] Average Loss: 50.182441\n",
      "Validation Loss: 51.885466 | Validation MAPE: 26.13%\n",
      "\n",
      "Epoch 12/500\n",
      "[Mini-batch] Average Loss: 50.257777\n",
      "Validation Loss: 50.031922 | Validation MAPE: 23.57%\n",
      "\n",
      "Epoch 13/500\n",
      "[Mini-batch] Average Loss: 50.407430\n",
      "Validation Loss: 51.464732 | Validation MAPE: 23.93%\n",
      "\n",
      "Epoch 14/500\n",
      "[Mini-batch] Average Loss: 50.565085\n",
      "Validation Loss: 51.911304 | Validation MAPE: 25.10%\n",
      "\n",
      "Epoch 15/500\n",
      "[Mini-batch] Average Loss: 50.956271\n",
      "Validation Loss: 52.098303 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 16/500\n",
      "[Mini-batch] Average Loss: 50.382932\n",
      "Validation Loss: 50.464834 | Validation MAPE: 24.30%\n",
      "\n",
      "Epoch 17/500\n",
      "[Mini-batch] Average Loss: 50.034449\n",
      "Validation Loss: 52.048421 | Validation MAPE: 24.68%\n",
      "\n",
      "Epoch 18/500\n",
      "[Mini-batch] Average Loss: 50.269960\n",
      "Validation Loss: 51.060072 | Validation MAPE: 24.85%\n",
      "\n",
      "Epoch 19/500\n",
      "[Mini-batch] Average Loss: 50.058452\n",
      "Validation Loss: 50.510122 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 20/500\n",
      "[Mini-batch] Average Loss: 50.309799\n",
      "Validation Loss: 49.965728 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 21/500\n",
      "[Mini-batch] Average Loss: 50.289118\n",
      "Validation Loss: 51.182475 | Validation MAPE: 23.77%\n",
      "\n",
      "Epoch 22/500\n",
      "[Mini-batch] Average Loss: 50.279623\n",
      "Validation Loss: 52.287799 | Validation MAPE: 25.40%\n",
      "\n",
      "Epoch 23/500\n",
      "[Mini-batch] Average Loss: 50.551585\n",
      "Validation Loss: 51.410890 | Validation MAPE: 24.52%\n",
      "\n",
      "Epoch 24/500\n",
      "[Mini-batch] Average Loss: 50.708795\n",
      "Validation Loss: 52.486929 | Validation MAPE: 25.00%\n",
      "\n",
      "Epoch 25/500\n",
      "[Mini-batch] Average Loss: 50.357980\n",
      "Validation Loss: 51.973590 | Validation MAPE: 24.94%\n",
      "\n",
      "Epoch 26/500\n",
      "[Mini-batch] Average Loss: 50.105770\n",
      "Validation Loss: 50.895297 | Validation MAPE: 24.03%\n",
      "\n",
      "Epoch 27/500\n",
      "[Mini-batch] Average Loss: 50.556445\n",
      "Validation Loss: 51.142416 | Validation MAPE: 24.67%\n",
      "\n",
      "Epoch 28/500\n",
      "[Mini-batch] Average Loss: 51.036169\n",
      "Validation Loss: 51.317544 | Validation MAPE: 24.51%\n",
      "\n",
      "Epoch 29/500\n",
      "[Mini-batch] Average Loss: 50.778149\n",
      "Validation Loss: 51.753146 | Validation MAPE: 24.95%\n",
      "\n",
      "Epoch 30/500\n",
      "[Mini-batch] Average Loss: 51.337163\n",
      "Validation Loss: 50.925067 | Validation MAPE: 24.82%\n",
      "\n",
      "Epoch 31/500\n",
      "[Mini-batch] Average Loss: 50.397667\n",
      "Validation Loss: 50.798366 | Validation MAPE: 24.13%\n",
      "\n",
      "Epoch 32/500\n",
      "[Mini-batch] Average Loss: 50.538654\n",
      "Validation Loss: 50.743261 | Validation MAPE: 24.07%\n",
      "\n",
      "Epoch 33/500\n",
      "[Mini-batch] Average Loss: 50.725062\n",
      "Validation Loss: 51.450860 | Validation MAPE: 23.65%\n",
      "\n",
      "Epoch 34/500\n",
      "[Mini-batch] Average Loss: 50.795820\n",
      "Validation Loss: 50.792776 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 35/500\n",
      "[Mini-batch] Average Loss: 50.464595\n",
      "Validation Loss: 50.597194 | Validation MAPE: 25.31%\n",
      "\n",
      "Epoch 36/500\n",
      "[Mini-batch] Average Loss: 50.719678\n",
      "Validation Loss: 51.184220 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 37/500\n",
      "[Mini-batch] Average Loss: 50.929016\n",
      "Validation Loss: 50.998103 | Validation MAPE: 24.06%\n",
      "\n",
      "Epoch 38/500\n",
      "[Mini-batch] Average Loss: 50.541701\n",
      "Validation Loss: 51.148184 | Validation MAPE: 24.23%\n",
      "\n",
      "Epoch 39/500\n",
      "[Mini-batch] Average Loss: 50.977655\n",
      "Validation Loss: 50.504927 | Validation MAPE: 23.86%\n",
      "\n",
      "Epoch 40/500\n",
      "[Mini-batch] Average Loss: 50.853326\n",
      "Validation Loss: 51.598361 | Validation MAPE: 23.84%\n",
      "\n",
      "Epoch 41/500\n",
      "[Mini-batch] Average Loss: 50.515411\n",
      "Validation Loss: 51.482033 | Validation MAPE: 24.58%\n",
      "\n",
      "Epoch 42/500\n",
      "[Mini-batch] Average Loss: 50.833357\n",
      "Validation Loss: 51.490866 | Validation MAPE: 24.42%\n",
      "\n",
      "Epoch 43/500\n",
      "[Mini-batch] Average Loss: 51.396303\n",
      "Validation Loss: 51.289992 | Validation MAPE: 23.94%\n",
      "\n",
      "Epoch 44/500\n",
      "[Mini-batch] Average Loss: 51.114115\n",
      "Validation Loss: 50.768698 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 45/500\n",
      "[Mini-batch] Average Loss: 50.744417\n",
      "Validation Loss: 51.933469 | Validation MAPE: 24.13%\n",
      "\n",
      "Epoch 46/500\n",
      "[Mini-batch] Average Loss: 51.259708\n",
      "Validation Loss: 51.932059 | Validation MAPE: 24.15%\n",
      "\n",
      "Epoch 47/500\n",
      "[Mini-batch] Average Loss: 51.181355\n",
      "Validation Loss: 51.400143 | Validation MAPE: 23.89%\n",
      "\n",
      "Epoch 48/500\n",
      "[Mini-batch] Average Loss: 51.007503\n",
      "Validation Loss: 51.863695 | Validation MAPE: 24.27%\n",
      "\n",
      "Epoch 49/500\n",
      "[Mini-batch] Average Loss: 50.628523\n",
      "Validation Loss: 50.850881 | Validation MAPE: 24.53%\n",
      "\n",
      "Epoch 50/500\n",
      "[Mini-batch] Average Loss: 51.128501\n",
      "Validation Loss: 51.866440 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 51/500\n",
      "[Mini-batch] Average Loss: 50.872385\n",
      "Validation Loss: 52.127069 | Validation MAPE: 24.06%\n",
      "\n",
      "Epoch 52/500\n",
      "[Mini-batch] Average Loss: 50.715343\n",
      "Validation Loss: 51.478536 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 53/500\n",
      "[Mini-batch] Average Loss: 51.200116\n",
      "Validation Loss: 50.401760 | Validation MAPE: 23.72%\n",
      "\n",
      "Epoch 54/500\n",
      "[Mini-batch] Average Loss: 51.104359\n",
      "Validation Loss: 51.709487 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 55/500\n",
      "[Mini-batch] Average Loss: 51.026545\n",
      "Validation Loss: 51.184076 | Validation MAPE: 23.88%\n",
      "\n",
      "Epoch 56/500\n",
      "[Mini-batch] Average Loss: 51.086913\n",
      "Validation Loss: 51.153882 | Validation MAPE: 24.03%\n",
      "\n",
      "Epoch 57/500\n",
      "[Mini-batch] Average Loss: 51.431918\n",
      "Validation Loss: 50.844451 | Validation MAPE: 24.84%\n",
      "\n",
      "Epoch 58/500\n",
      "[Mini-batch] Average Loss: 50.998112\n",
      "Validation Loss: 52.599169 | Validation MAPE: 23.90%\n",
      "\n",
      "Epoch 59/500\n",
      "[Mini-batch] Average Loss: 50.606370\n",
      "Validation Loss: 50.782267 | Validation MAPE: 24.15%\n",
      "\n",
      "Epoch 60/500\n",
      "[Mini-batch] Average Loss: 51.057579\n",
      "Validation Loss: 52.438487 | Validation MAPE: 25.62%\n",
      "\n",
      "Epoch 61/500\n",
      "[Mini-batch] Average Loss: 50.707391\n",
      "Validation Loss: 51.034871 | Validation MAPE: 24.39%\n",
      "\n",
      "Epoch 62/500\n",
      "[Mini-batch] Average Loss: 50.402074\n",
      "Validation Loss: 51.399164 | Validation MAPE: 24.91%\n",
      "\n",
      "Epoch 63/500\n",
      "[Mini-batch] Average Loss: 50.904402\n",
      "Validation Loss: 50.884623 | Validation MAPE: 23.69%\n",
      "\n",
      "Epoch 64/500\n",
      "[Mini-batch] Average Loss: 51.026936\n",
      "Validation Loss: 51.976032 | Validation MAPE: 25.27%\n",
      "\n",
      "Epoch 65/500\n",
      "[Mini-batch] Average Loss: 51.247491\n",
      "Validation Loss: 52.528890 | Validation MAPE: 23.60%\n",
      "\n",
      "Epoch 66/500\n",
      "[Mini-batch] Average Loss: 51.549559\n",
      "Validation Loss: 51.653629 | Validation MAPE: 24.85%\n",
      "\n",
      "Epoch 67/500\n",
      "[Mini-batch] Average Loss: 51.323971\n",
      "Validation Loss: 51.584432 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 68/500\n",
      "[Mini-batch] Average Loss: 50.939807\n",
      "Validation Loss: 51.759111 | Validation MAPE: 24.59%\n",
      "\n",
      "Epoch 69/500\n",
      "[Mini-batch] Average Loss: 51.208091\n",
      "Validation Loss: 51.814960 | Validation MAPE: 25.13%\n",
      "\n",
      "Epoch 70/500\n",
      "[Mini-batch] Average Loss: 50.681414\n",
      "Validation Loss: 51.006462 | Validation MAPE: 24.23%\n",
      "\n",
      "Epoch 71/500\n",
      "[Mini-batch] Average Loss: 50.822910\n",
      "Validation Loss: 51.397603 | Validation MAPE: 24.18%\n",
      "\n",
      "Epoch 72/500\n",
      "[Mini-batch] Average Loss: 50.659494\n",
      "Validation Loss: 51.388095 | Validation MAPE: 24.78%\n",
      "\n",
      "Epoch 73/500\n",
      "[Mini-batch] Average Loss: 50.945136\n",
      "Validation Loss: 51.793851 | Validation MAPE: 24.80%\n",
      "\n",
      "Epoch 74/500\n",
      "[Mini-batch] Average Loss: 50.836353\n",
      "Validation Loss: 52.120035 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 75/500\n",
      "[Mini-batch] Average Loss: 51.165868\n",
      "Validation Loss: 52.134746 | Validation MAPE: 23.74%\n",
      "\n",
      "Epoch 76/500\n",
      "[Mini-batch] Average Loss: 51.654359\n",
      "Validation Loss: 51.943028 | Validation MAPE: 23.51%\n",
      "\n",
      "Epoch 77/500\n",
      "[Mini-batch] Average Loss: 50.783515\n",
      "Validation Loss: 50.942215 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 78/500\n",
      "[Mini-batch] Average Loss: 50.611297\n",
      "Validation Loss: 52.233511 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 79/500\n",
      "[Mini-batch] Average Loss: 50.689204\n",
      "Validation Loss: 51.199872 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 80/500\n",
      "[Mini-batch] Average Loss: 50.992109\n",
      "Validation Loss: 51.768270 | Validation MAPE: 24.76%\n",
      "\n",
      "Epoch 81/500\n",
      "[Mini-batch] Average Loss: 50.644564\n",
      "Validation Loss: 50.575913 | Validation MAPE: 23.98%\n",
      "\n",
      "Epoch 82/500\n",
      "[Mini-batch] Average Loss: 50.814857\n",
      "Validation Loss: 52.228763 | Validation MAPE: 25.07%\n",
      "\n",
      "Epoch 83/500\n",
      "[Mini-batch] Average Loss: 51.390112\n",
      "Validation Loss: 51.510158 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 84/500\n",
      "[Mini-batch] Average Loss: 50.871361\n",
      "Validation Loss: 50.416169 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 85/500\n",
      "[Mini-batch] Average Loss: 51.438949\n",
      "Validation Loss: 50.918756 | Validation MAPE: 24.00%\n",
      "\n",
      "Epoch 86/500\n",
      "[Mini-batch] Average Loss: 51.540565\n",
      "Validation Loss: 57.055198 | Validation MAPE: 25.85%\n",
      "\n",
      "Epoch 87/500\n",
      "[Mini-batch] Average Loss: 51.909740\n",
      "Validation Loss: 51.354043 | Validation MAPE: 24.26%\n",
      "\n",
      "Epoch 88/500\n",
      "[Mini-batch] Average Loss: 50.516158\n",
      "Validation Loss: 51.378266 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 89/500\n",
      "[Mini-batch] Average Loss: 51.319147\n",
      "Validation Loss: 52.639869 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 90/500\n",
      "[Mini-batch] Average Loss: 51.154644\n",
      "Validation Loss: 51.365539 | Validation MAPE: 24.45%\n",
      "\n",
      "Epoch 91/500\n",
      "[Mini-batch] Average Loss: 50.503069\n",
      "Validation Loss: 51.020117 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 92/500\n",
      "[Mini-batch] Average Loss: 50.810441\n",
      "Validation Loss: 51.037525 | Validation MAPE: 24.33%\n",
      "\n",
      "Epoch 93/500\n",
      "[Mini-batch] Average Loss: 51.298595\n",
      "Validation Loss: 51.733326 | Validation MAPE: 24.76%\n",
      "\n",
      "Epoch 94/500\n",
      "[Mini-batch] Average Loss: 51.251875\n",
      "Validation Loss: 51.431875 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 95/500\n",
      "[Mini-batch] Average Loss: 51.683760\n",
      "Validation Loss: 50.880475 | Validation MAPE: 24.61%\n",
      "\n",
      "Epoch 96/500\n",
      "[Mini-batch] Average Loss: 50.862134\n",
      "Validation Loss: 51.117338 | Validation MAPE: 24.29%\n",
      "\n",
      "Epoch 97/500\n",
      "[Mini-batch] Average Loss: 50.468587\n",
      "Validation Loss: 50.495438 | Validation MAPE: 24.41%\n",
      "\n",
      "Epoch 98/500\n",
      "[Mini-batch] Average Loss: 50.192646\n",
      "Validation Loss: 51.478410 | Validation MAPE: 24.23%\n",
      "\n",
      "Epoch 99/500\n",
      "[Mini-batch] Average Loss: 50.894948\n",
      "Validation Loss: 51.011956 | Validation MAPE: 23.71%\n",
      "\n",
      "Epoch 100/500\n",
      "[Mini-batch] Average Loss: 50.973601\n",
      "Validation Loss: 51.814646 | Validation MAPE: 23.99%\n",
      "\n",
      "Epoch 101/500\n",
      "[Mini-batch] Average Loss: 51.022462\n",
      "Validation Loss: 51.538868 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 102/500\n",
      "[Mini-batch] Average Loss: 51.593118\n",
      "Validation Loss: 52.885899 | Validation MAPE: 26.19%\n",
      "\n",
      "Epoch 103/500\n",
      "[Mini-batch] Average Loss: 50.919928\n",
      "Validation Loss: 51.429971 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 104/500\n",
      "[Mini-batch] Average Loss: 50.912576\n",
      "Validation Loss: 50.866569 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 105/500\n",
      "[Mini-batch] Average Loss: 50.926420\n",
      "Validation Loss: 51.944746 | Validation MAPE: 24.06%\n",
      "\n",
      "Epoch 106/500\n",
      "[Mini-batch] Average Loss: 50.898969\n",
      "Validation Loss: 51.475320 | Validation MAPE: 24.28%\n",
      "\n",
      "Epoch 107/500\n",
      "[Mini-batch] Average Loss: 50.391503\n",
      "Validation Loss: 51.614699 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 108/500\n",
      "[Mini-batch] Average Loss: 50.663417\n",
      "Validation Loss: 51.998810 | Validation MAPE: 24.98%\n",
      "\n",
      "Epoch 109/500\n",
      "[Mini-batch] Average Loss: 50.819875\n",
      "Validation Loss: 51.422877 | Validation MAPE: 23.99%\n",
      "\n",
      "Epoch 110/500\n",
      "[Mini-batch] Average Loss: 50.606697\n",
      "Validation Loss: 50.833449 | Validation MAPE: 24.10%\n",
      "\n",
      "Epoch 111/500\n",
      "[Mini-batch] Average Loss: 50.541482\n",
      "Validation Loss: 51.510102 | Validation MAPE: 23.60%\n",
      "\n",
      "Epoch 112/500\n",
      "[Mini-batch] Average Loss: 50.476624\n",
      "Validation Loss: 51.470621 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 113/500\n",
      "[Mini-batch] Average Loss: 50.708946\n",
      "Validation Loss: 50.764007 | Validation MAPE: 24.69%\n",
      "\n",
      "Epoch 114/500\n",
      "[Mini-batch] Average Loss: 50.538748\n",
      "Validation Loss: 50.501361 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 115/500\n",
      "[Mini-batch] Average Loss: 50.914441\n",
      "Validation Loss: 51.800983 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 116/500\n",
      "[Mini-batch] Average Loss: 51.028503\n",
      "Validation Loss: 50.680107 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 117/500\n",
      "[Mini-batch] Average Loss: 50.570295\n",
      "Validation Loss: 51.602447 | Validation MAPE: 24.63%\n",
      "\n",
      "Epoch 118/500\n",
      "[Mini-batch] Average Loss: 50.769511\n",
      "Validation Loss: 50.736399 | Validation MAPE: 24.45%\n",
      "\n",
      "Epoch 119/500\n",
      "[Mini-batch] Average Loss: 50.542236\n",
      "Validation Loss: 51.757928 | Validation MAPE: 24.08%\n",
      "\n",
      "Epoch 120/500\n",
      "[Mini-batch] Average Loss: 51.096886\n",
      "Validation Loss: 51.759633 | Validation MAPE: 23.63%\n",
      "\n",
      "Epoch 121/500\n",
      "[Mini-batch] Average Loss: 51.355250\n",
      "Validation Loss: 52.882817 | Validation MAPE: 25.76%\n",
      "\n",
      "Epoch 122/500\n",
      "[Mini-batch] Average Loss: 51.205982\n",
      "Validation Loss: 51.267191 | Validation MAPE: 24.50%\n",
      "\n",
      "Epoch 123/500\n",
      "[Mini-batch] Average Loss: 50.615237\n",
      "Validation Loss: 51.011391 | Validation MAPE: 24.39%\n",
      "\n",
      "Epoch 124/500\n",
      "[Mini-batch] Average Loss: 50.878879\n",
      "Validation Loss: 52.804165 | Validation MAPE: 25.71%\n",
      "\n",
      "Epoch 125/500\n",
      "[Mini-batch] Average Loss: 50.971366\n",
      "Validation Loss: 51.295053 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 126/500\n",
      "[Mini-batch] Average Loss: 50.520616\n",
      "Validation Loss: 53.137923 | Validation MAPE: 24.08%\n",
      "\n",
      "Epoch 127/500\n",
      "[Mini-batch] Average Loss: 51.116871\n",
      "Validation Loss: 52.174026 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 128/500\n",
      "[Mini-batch] Average Loss: 50.705687\n",
      "Validation Loss: 51.800776 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 129/500\n",
      "[Mini-batch] Average Loss: 50.706738\n",
      "Validation Loss: 50.785299 | Validation MAPE: 24.53%\n",
      "\n",
      "Epoch 130/500\n",
      "[Mini-batch] Average Loss: 50.637156\n",
      "Validation Loss: 51.897605 | Validation MAPE: 24.61%\n",
      "\n",
      "Epoch 131/500\n",
      "[Mini-batch] Average Loss: 50.456467\n",
      "Validation Loss: 52.247582 | Validation MAPE: 24.14%\n",
      "\n",
      "Epoch 132/500\n",
      "[Mini-batch] Average Loss: 51.006547\n",
      "Validation Loss: 51.767730 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 133/500\n",
      "[Mini-batch] Average Loss: 50.459905\n",
      "Validation Loss: 51.297033 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 134/500\n",
      "[Mini-batch] Average Loss: 50.979430\n",
      "Validation Loss: 51.479070 | Validation MAPE: 24.05%\n",
      "\n",
      "Epoch 135/500\n",
      "[Mini-batch] Average Loss: 51.279342\n",
      "Validation Loss: 51.405933 | Validation MAPE: 24.12%\n",
      "\n",
      "Epoch 136/500\n",
      "[Mini-batch] Average Loss: 51.408409\n",
      "Validation Loss: 51.824437 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 137/500\n",
      "[Mini-batch] Average Loss: 50.771414\n",
      "Validation Loss: 52.027415 | Validation MAPE: 24.51%\n",
      "\n",
      "Epoch 138/500\n",
      "[Mini-batch] Average Loss: 50.912087\n",
      "Validation Loss: 51.715992 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 139/500\n",
      "[Mini-batch] Average Loss: 51.062028\n",
      "Validation Loss: 50.897607 | Validation MAPE: 23.79%\n",
      "\n",
      "Epoch 140/500\n",
      "[Mini-batch] Average Loss: 50.472657\n",
      "Validation Loss: 50.314115 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 141/500\n",
      "[Mini-batch] Average Loss: 50.307861\n",
      "Validation Loss: 50.493334 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 142/500\n",
      "[Mini-batch] Average Loss: 51.010082\n",
      "Validation Loss: 51.058981 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 143/500\n",
      "[Mini-batch] Average Loss: 50.826150\n",
      "Validation Loss: 52.952710 | Validation MAPE: 25.00%\n",
      "\n",
      "Epoch 144/500\n",
      "[Mini-batch] Average Loss: 50.473243\n",
      "Validation Loss: 52.332742 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 145/500\n",
      "[Mini-batch] Average Loss: 50.815783\n",
      "Validation Loss: 52.059807 | Validation MAPE: 25.27%\n",
      "\n",
      "Epoch 146/500\n",
      "[Mini-batch] Average Loss: 50.875702\n",
      "Validation Loss: 50.915693 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 147/500\n",
      "[Mini-batch] Average Loss: 50.952342\n",
      "Validation Loss: 52.194962 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 148/500\n",
      "[Mini-batch] Average Loss: 51.024826\n",
      "Validation Loss: 50.809063 | Validation MAPE: 24.71%\n",
      "\n",
      "Epoch 149/500\n",
      "[Mini-batch] Average Loss: 51.122388\n",
      "Validation Loss: 51.318739 | Validation MAPE: 24.60%\n",
      "\n",
      "Epoch 150/500\n",
      "[Mini-batch] Average Loss: 50.648044\n",
      "Validation Loss: 52.016595 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 151/500\n",
      "[Mini-batch] Average Loss: 51.341480\n",
      "Validation Loss: 52.992177 | Validation MAPE: 24.38%\n",
      "\n",
      "Epoch 152/500\n",
      "[Mini-batch] Average Loss: 50.646463\n",
      "Validation Loss: 52.624421 | Validation MAPE: 25.08%\n",
      "\n",
      "Epoch 153/500\n",
      "[Mini-batch] Average Loss: 51.382374\n",
      "Validation Loss: 50.932341 | Validation MAPE: 23.70%\n",
      "\n",
      "Epoch 154/500\n",
      "[Mini-batch] Average Loss: 51.091449\n",
      "Validation Loss: 52.904343 | Validation MAPE: 24.26%\n",
      "\n",
      "Epoch 155/500\n",
      "[Mini-batch] Average Loss: 50.607950\n",
      "Validation Loss: 51.868369 | Validation MAPE: 25.91%\n",
      "\n",
      "Epoch 156/500\n",
      "[Mini-batch] Average Loss: 50.905924\n",
      "Validation Loss: 51.156746 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 157/500\n",
      "[Mini-batch] Average Loss: 51.672872\n",
      "Validation Loss: 52.145120 | Validation MAPE: 24.66%\n",
      "\n",
      "Epoch 158/500\n",
      "[Mini-batch] Average Loss: 50.886685\n",
      "Validation Loss: 52.076753 | Validation MAPE: 24.14%\n",
      "\n",
      "Epoch 159/500\n",
      "[Mini-batch] Average Loss: 50.905997\n",
      "Validation Loss: 52.272790 | Validation MAPE: 25.17%\n",
      "\n",
      "Epoch 160/500\n",
      "[Mini-batch] Average Loss: 50.886653\n",
      "Validation Loss: 50.619753 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 161/500\n",
      "[Mini-batch] Average Loss: 50.938641\n",
      "Validation Loss: 52.434298 | Validation MAPE: 23.92%\n",
      "\n",
      "Epoch 162/500\n",
      "[Mini-batch] Average Loss: 50.848631\n",
      "Validation Loss: 51.511326 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 163/500\n",
      "[Mini-batch] Average Loss: 51.174660\n",
      "Validation Loss: 52.341077 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 164/500\n",
      "[Mini-batch] Average Loss: 51.286193\n",
      "Validation Loss: 51.595411 | Validation MAPE: 24.05%\n",
      "\n",
      "Epoch 165/500\n",
      "[Mini-batch] Average Loss: 50.542714\n",
      "Validation Loss: 53.142307 | Validation MAPE: 24.82%\n",
      "\n",
      "Epoch 166/500\n",
      "[Mini-batch] Average Loss: 50.807062\n",
      "Validation Loss: 51.251647 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 167/500\n",
      "[Mini-batch] Average Loss: 50.584053\n",
      "Validation Loss: 52.037480 | Validation MAPE: 25.01%\n",
      "\n",
      "Epoch 168/500\n",
      "[Mini-batch] Average Loss: 51.298674\n",
      "Validation Loss: 51.831645 | Validation MAPE: 23.96%\n",
      "\n",
      "Epoch 169/500\n",
      "[Mini-batch] Average Loss: 50.818642\n",
      "Validation Loss: 52.413907 | Validation MAPE: 24.75%\n",
      "\n",
      "Epoch 170/500\n",
      "[Mini-batch] Average Loss: 51.152033\n",
      "Validation Loss: 51.882282 | Validation MAPE: 24.26%\n",
      "\n",
      "Epoch 171/500\n",
      "[Mini-batch] Average Loss: 50.666316\n",
      "Validation Loss: 51.602118 | Validation MAPE: 25.31%\n",
      "\n",
      "Epoch 172/500\n",
      "[Mini-batch] Average Loss: 50.744194\n",
      "Validation Loss: 52.376385 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 173/500\n",
      "[Mini-batch] Average Loss: 50.725697\n",
      "Validation Loss: 51.479824 | Validation MAPE: 23.89%\n",
      "\n",
      "Epoch 174/500\n",
      "[Mini-batch] Average Loss: 51.117363\n",
      "Validation Loss: 51.869703 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 175/500\n",
      "[Mini-batch] Average Loss: 51.403832\n",
      "Validation Loss: 52.227635 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 176/500\n",
      "[Mini-batch] Average Loss: 51.306135\n",
      "Validation Loss: 52.045832 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 177/500\n",
      "[Mini-batch] Average Loss: 51.414679\n",
      "Validation Loss: 51.827638 | Validation MAPE: 24.59%\n",
      "\n",
      "Epoch 178/500\n",
      "[Mini-batch] Average Loss: 50.752323\n",
      "Validation Loss: 52.229286 | Validation MAPE: 24.50%\n",
      "\n",
      "Epoch 179/500\n",
      "[Mini-batch] Average Loss: 51.222825\n",
      "Validation Loss: 52.594445 | Validation MAPE: 24.62%\n",
      "\n",
      "Epoch 180/500\n",
      "[Mini-batch] Average Loss: 50.832630\n",
      "Validation Loss: 51.317360 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 181/500\n",
      "[Mini-batch] Average Loss: 51.229789\n",
      "Validation Loss: 51.542815 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 182/500\n",
      "[Mini-batch] Average Loss: 50.444430\n",
      "Validation Loss: 52.065712 | Validation MAPE: 24.91%\n",
      "\n",
      "Epoch 183/500\n",
      "[Mini-batch] Average Loss: 51.097926\n",
      "Validation Loss: 51.553134 | Validation MAPE: 24.85%\n",
      "\n",
      "Epoch 184/500\n",
      "[Mini-batch] Average Loss: 50.745697\n",
      "Validation Loss: 51.544192 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 185/500\n",
      "[Mini-batch] Average Loss: 50.760941\n",
      "Validation Loss: 53.326007 | Validation MAPE: 25.09%\n",
      "\n",
      "Epoch 186/500\n",
      "[Mini-batch] Average Loss: 51.309347\n",
      "Validation Loss: 52.933120 | Validation MAPE: 24.73%\n",
      "\n",
      "Epoch 187/500\n",
      "[Mini-batch] Average Loss: 51.078172\n",
      "Validation Loss: 52.059664 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 188/500\n",
      "[Mini-batch] Average Loss: 52.051448\n",
      "Validation Loss: 51.668340 | Validation MAPE: 24.89%\n",
      "\n",
      "Epoch 189/500\n",
      "[Mini-batch] Average Loss: 51.544169\n",
      "Validation Loss: 52.756357 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 190/500\n",
      "[Mini-batch] Average Loss: 50.985769\n",
      "Validation Loss: 52.525298 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 191/500\n",
      "[Mini-batch] Average Loss: 50.850254\n",
      "Validation Loss: 51.709105 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 192/500\n",
      "[Mini-batch] Average Loss: 51.033850\n",
      "Validation Loss: 51.475577 | Validation MAPE: 24.61%\n",
      "\n",
      "Epoch 193/500\n",
      "[Mini-batch] Average Loss: 51.083174\n",
      "Validation Loss: 51.806257 | Validation MAPE: 24.10%\n",
      "\n",
      "Epoch 194/500\n",
      "[Mini-batch] Average Loss: 51.489662\n",
      "Validation Loss: 51.682118 | Validation MAPE: 24.65%\n",
      "\n",
      "Epoch 195/500\n",
      "[Mini-batch] Average Loss: 51.153375\n",
      "Validation Loss: 52.713396 | Validation MAPE: 25.32%\n",
      "\n",
      "Epoch 196/500\n",
      "[Mini-batch] Average Loss: 50.863987\n",
      "Validation Loss: 51.396222 | Validation MAPE: 24.63%\n",
      "\n",
      "Epoch 197/500\n",
      "[Mini-batch] Average Loss: 50.697399\n",
      "Validation Loss: 51.829282 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 198/500\n",
      "[Mini-batch] Average Loss: 50.930533\n",
      "Validation Loss: 52.722392 | Validation MAPE: 24.83%\n",
      "\n",
      "Epoch 199/500\n",
      "[Mini-batch] Average Loss: 50.991891\n",
      "Validation Loss: 52.975693 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 200/500\n",
      "[Mini-batch] Average Loss: 50.938090\n",
      "Validation Loss: 51.467916 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 201/500\n",
      "[Mini-batch] Average Loss: 51.252106\n",
      "Validation Loss: 52.265124 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 202/500\n",
      "[Mini-batch] Average Loss: 51.363843\n",
      "Validation Loss: 51.884109 | Validation MAPE: 23.65%\n",
      "\n",
      "Epoch 203/500\n",
      "[Mini-batch] Average Loss: 50.419453\n",
      "Validation Loss: 52.326160 | Validation MAPE: 24.51%\n",
      "\n",
      "Epoch 204/500\n",
      "[Mini-batch] Average Loss: 50.286157\n",
      "Validation Loss: 51.308144 | Validation MAPE: 23.98%\n",
      "\n",
      "Epoch 205/500\n",
      "[Mini-batch] Average Loss: 50.265542\n",
      "Validation Loss: 51.367201 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 206/500\n",
      "[Mini-batch] Average Loss: 50.586627\n",
      "Validation Loss: 51.348691 | Validation MAPE: 23.96%\n",
      "\n",
      "Epoch 207/500\n",
      "[Mini-batch] Average Loss: 50.532401\n",
      "Validation Loss: 51.338664 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 208/500\n",
      "[Mini-batch] Average Loss: 50.600529\n",
      "Validation Loss: 51.644659 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 209/500\n",
      "[Mini-batch] Average Loss: 50.345519\n",
      "Validation Loss: 52.905337 | Validation MAPE: 25.24%\n",
      "\n",
      "Epoch 210/500\n",
      "[Mini-batch] Average Loss: 50.678090\n",
      "Validation Loss: 51.089632 | Validation MAPE: 24.17%\n",
      "\n",
      "Epoch 211/500\n",
      "[Mini-batch] Average Loss: 50.464784\n",
      "Validation Loss: 50.604656 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 212/500\n",
      "[Mini-batch] Average Loss: 50.551065\n",
      "Validation Loss: 51.592887 | Validation MAPE: 24.08%\n",
      "\n",
      "Epoch 213/500\n",
      "[Mini-batch] Average Loss: 50.827170\n",
      "Validation Loss: 52.483201 | Validation MAPE: 24.15%\n",
      "\n",
      "Epoch 214/500\n",
      "[Mini-batch] Average Loss: 50.791712\n",
      "Validation Loss: 51.444449 | Validation MAPE: 24.41%\n",
      "\n",
      "Epoch 215/500\n",
      "[Mini-batch] Average Loss: 50.787586\n",
      "Validation Loss: 54.058735 | Validation MAPE: 25.27%\n",
      "\n",
      "Epoch 216/500\n",
      "[Mini-batch] Average Loss: 50.763160\n",
      "Validation Loss: 50.998882 | Validation MAPE: 24.66%\n",
      "\n",
      "Epoch 217/500\n",
      "[Mini-batch] Average Loss: 50.633014\n",
      "Validation Loss: 51.102485 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 218/500\n",
      "[Mini-batch] Average Loss: 50.349270\n",
      "Validation Loss: 50.767215 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 219/500\n",
      "[Mini-batch] Average Loss: 50.765740\n",
      "Validation Loss: 50.944478 | Validation MAPE: 23.95%\n",
      "\n",
      "Epoch 220/500\n",
      "[Mini-batch] Average Loss: 50.518822\n",
      "Validation Loss: 51.724231 | Validation MAPE: 25.09%\n",
      "\n",
      "Epoch 221/500\n",
      "[Mini-batch] Average Loss: 50.724133\n",
      "Validation Loss: 51.649984 | Validation MAPE: 24.16%\n",
      "\n",
      "Epoch 222/500\n",
      "[Mini-batch] Average Loss: 50.908777\n",
      "Validation Loss: 51.677465 | Validation MAPE: 24.40%\n",
      "\n",
      "Epoch 223/500\n",
      "[Mini-batch] Average Loss: 50.945193\n",
      "Validation Loss: 51.447364 | Validation MAPE: 24.65%\n",
      "\n",
      "Epoch 224/500\n",
      "[Mini-batch] Average Loss: 51.335187\n",
      "Validation Loss: 51.338205 | Validation MAPE: 23.81%\n",
      "\n",
      "Epoch 225/500\n",
      "[Mini-batch] Average Loss: 50.751624\n",
      "Validation Loss: 51.747264 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 226/500\n",
      "[Mini-batch] Average Loss: 50.598052\n",
      "Validation Loss: 50.961930 | Validation MAPE: 24.30%\n",
      "\n",
      "Epoch 227/500\n",
      "[Mini-batch] Average Loss: 50.675750\n",
      "Validation Loss: 51.950536 | Validation MAPE: 24.79%\n",
      "\n",
      "Epoch 228/500\n",
      "[Mini-batch] Average Loss: 51.126426\n",
      "Validation Loss: 50.696785 | Validation MAPE: 24.34%\n",
      "\n",
      "Epoch 229/500\n",
      "[Mini-batch] Average Loss: 51.054252\n",
      "Validation Loss: 51.703478 | Validation MAPE: 23.86%\n",
      "\n",
      "Epoch 230/500\n",
      "[Mini-batch] Average Loss: 50.660116\n",
      "Validation Loss: 51.816879 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 231/500\n",
      "[Mini-batch] Average Loss: 50.954074\n",
      "Validation Loss: 51.947395 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 232/500\n",
      "[Mini-batch] Average Loss: 50.725868\n",
      "Validation Loss: 51.879316 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 233/500\n",
      "[Mini-batch] Average Loss: 52.630838\n",
      "Validation Loss: 53.589921 | Validation MAPE: 24.51%\n",
      "\n",
      "Epoch 234/500\n",
      "[Mini-batch] Average Loss: 51.592564\n",
      "Validation Loss: 52.259539 | Validation MAPE: 24.84%\n",
      "\n",
      "Epoch 235/500\n",
      "[Mini-batch] Average Loss: 51.438335\n",
      "Validation Loss: 51.494835 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 236/500\n",
      "[Mini-batch] Average Loss: 51.030736\n",
      "Validation Loss: 50.717981 | Validation MAPE: 23.62%\n",
      "\n",
      "Epoch 237/500\n",
      "[Mini-batch] Average Loss: 51.120612\n",
      "Validation Loss: 51.396820 | Validation MAPE: 24.75%\n",
      "\n",
      "Epoch 238/500\n",
      "[Mini-batch] Average Loss: 51.623036\n",
      "Validation Loss: 53.179106 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 239/500\n",
      "[Mini-batch] Average Loss: 51.396103\n",
      "Validation Loss: 51.309957 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 240/500\n",
      "[Mini-batch] Average Loss: 51.240964\n",
      "Validation Loss: 51.309232 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 241/500\n",
      "[Mini-batch] Average Loss: 50.941003\n",
      "Validation Loss: 52.831541 | Validation MAPE: 23.66%\n",
      "\n",
      "Epoch 242/500\n",
      "[Mini-batch] Average Loss: 50.175944\n",
      "Validation Loss: 50.705993 | Validation MAPE: 24.06%\n",
      "\n",
      "Epoch 243/500\n",
      "[Mini-batch] Average Loss: 51.105835\n",
      "Validation Loss: 51.980981 | Validation MAPE: 25.10%\n",
      "\n",
      "Epoch 244/500\n",
      "[Mini-batch] Average Loss: 51.141519\n",
      "Validation Loss: 51.966045 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 245/500\n",
      "[Mini-batch] Average Loss: 50.549935\n",
      "Validation Loss: 51.995018 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 246/500\n",
      "[Mini-batch] Average Loss: 50.961852\n",
      "Validation Loss: 52.292850 | Validation MAPE: 23.78%\n",
      "\n",
      "Epoch 247/500\n",
      "[Mini-batch] Average Loss: 50.517601\n",
      "Validation Loss: 52.252996 | Validation MAPE: 24.69%\n",
      "\n",
      "Epoch 248/500\n",
      "[Mini-batch] Average Loss: 51.257220\n",
      "Validation Loss: 50.871842 | Validation MAPE: 23.57%\n",
      "\n",
      "Epoch 249/500\n",
      "[Mini-batch] Average Loss: 51.334430\n",
      "Validation Loss: 53.117354 | Validation MAPE: 24.09%\n",
      "\n",
      "Epoch 250/500\n",
      "[Mini-batch] Average Loss: 51.169966\n",
      "Validation Loss: 51.509192 | Validation MAPE: 24.65%\n",
      "\n",
      "Epoch 251/500\n",
      "[Mini-batch] Average Loss: 51.002934\n",
      "Validation Loss: 52.113261 | Validation MAPE: 23.99%\n",
      "\n",
      "Epoch 252/500\n",
      "[Mini-batch] Average Loss: 51.037477\n",
      "Validation Loss: 52.647384 | Validation MAPE: 25.17%\n",
      "\n",
      "Epoch 253/500\n",
      "[Mini-batch] Average Loss: 51.126419\n",
      "Validation Loss: 53.325385 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 254/500\n",
      "[Mini-batch] Average Loss: 51.208142\n",
      "Validation Loss: 51.168945 | Validation MAPE: 24.19%\n",
      "\n",
      "Epoch 255/500\n",
      "[Mini-batch] Average Loss: 51.081856\n",
      "Validation Loss: 52.024947 | Validation MAPE: 25.17%\n",
      "\n",
      "Epoch 256/500\n",
      "[Mini-batch] Average Loss: 51.194980\n",
      "Validation Loss: 50.987607 | Validation MAPE: 24.50%\n",
      "\n",
      "Epoch 257/500\n",
      "[Mini-batch] Average Loss: 50.851419\n",
      "Validation Loss: 51.435906 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 258/500\n",
      "[Mini-batch] Average Loss: 51.114298\n",
      "Validation Loss: 51.736625 | Validation MAPE: 23.90%\n",
      "\n",
      "Epoch 259/500\n",
      "[Mini-batch] Average Loss: 50.969873\n",
      "Validation Loss: 51.860072 | Validation MAPE: 24.79%\n",
      "\n",
      "Epoch 260/500\n",
      "[Mini-batch] Average Loss: 51.126160\n",
      "Validation Loss: 51.349172 | Validation MAPE: 24.88%\n",
      "\n",
      "Epoch 261/500\n",
      "[Mini-batch] Average Loss: 50.779376\n",
      "Validation Loss: 51.560694 | Validation MAPE: 24.72%\n",
      "\n",
      "Epoch 262/500\n",
      "[Mini-batch] Average Loss: 50.889212\n",
      "Validation Loss: 51.527435 | Validation MAPE: 24.23%\n",
      "\n",
      "Epoch 263/500\n",
      "[Mini-batch] Average Loss: 51.477113\n",
      "Validation Loss: 53.036050 | Validation MAPE: 24.88%\n",
      "\n",
      "Epoch 264/500\n",
      "[Mini-batch] Average Loss: 51.528385\n",
      "Validation Loss: 52.199126 | Validation MAPE: 25.06%\n",
      "\n",
      "Epoch 265/500\n",
      "[Mini-batch] Average Loss: 51.192499\n",
      "Validation Loss: 52.073644 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 266/500\n",
      "[Mini-batch] Average Loss: 50.936876\n",
      "Validation Loss: 50.953485 | Validation MAPE: 24.67%\n",
      "\n",
      "Epoch 267/500\n",
      "[Mini-batch] Average Loss: 50.689236\n",
      "Validation Loss: 51.504311 | Validation MAPE: 24.79%\n",
      "\n",
      "Epoch 268/500\n",
      "[Mini-batch] Average Loss: 50.540111\n",
      "Validation Loss: 51.996637 | Validation MAPE: 24.67%\n",
      "\n",
      "Epoch 269/500\n",
      "[Mini-batch] Average Loss: 51.074115\n",
      "Validation Loss: 53.279768 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 270/500\n",
      "[Mini-batch] Average Loss: 51.037324\n",
      "Validation Loss: 51.963929 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 271/500\n",
      "[Mini-batch] Average Loss: 51.470033\n",
      "Validation Loss: 51.836381 | Validation MAPE: 23.98%\n",
      "\n",
      "Epoch 272/500\n",
      "[Mini-batch] Average Loss: 51.046208\n",
      "Validation Loss: 52.173602 | Validation MAPE: 24.63%\n",
      "\n",
      "Epoch 273/500\n",
      "[Mini-batch] Average Loss: 50.846993\n",
      "Validation Loss: 51.122124 | Validation MAPE: 24.38%\n",
      "\n",
      "Epoch 274/500\n",
      "[Mini-batch] Average Loss: 51.653002\n",
      "Validation Loss: 51.284417 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 275/500\n",
      "[Mini-batch] Average Loss: 51.207811\n",
      "Validation Loss: 52.404245 | Validation MAPE: 25.16%\n",
      "\n",
      "Epoch 276/500\n",
      "[Mini-batch] Average Loss: 51.167040\n",
      "Validation Loss: 51.488743 | Validation MAPE: 24.42%\n",
      "\n",
      "Epoch 277/500\n",
      "[Mini-batch] Average Loss: 51.262471\n",
      "Validation Loss: 51.146561 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 278/500\n",
      "[Mini-batch] Average Loss: 50.684933\n",
      "Validation Loss: 51.983363 | Validation MAPE: 25.39%\n",
      "\n",
      "Epoch 279/500\n",
      "[Mini-batch] Average Loss: 50.647715\n",
      "Validation Loss: 53.166778 | Validation MAPE: 24.16%\n",
      "\n",
      "Epoch 280/500\n",
      "[Mini-batch] Average Loss: 51.154231\n",
      "Validation Loss: 51.488707 | Validation MAPE: 24.34%\n",
      "\n",
      "Epoch 281/500\n",
      "[Mini-batch] Average Loss: 50.776503\n",
      "Validation Loss: 51.751913 | Validation MAPE: 24.85%\n",
      "\n",
      "Epoch 282/500\n",
      "[Mini-batch] Average Loss: 50.384451\n",
      "Validation Loss: 52.267843 | Validation MAPE: 24.82%\n",
      "\n",
      "Epoch 283/500\n",
      "[Mini-batch] Average Loss: 50.581390\n",
      "Validation Loss: 51.198532 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 284/500\n",
      "[Mini-batch] Average Loss: 50.963255\n",
      "Validation Loss: 50.758960 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 285/500\n",
      "[Mini-batch] Average Loss: 50.845830\n",
      "Validation Loss: 51.927781 | Validation MAPE: 25.12%\n",
      "\n",
      "Epoch 286/500\n",
      "[Mini-batch] Average Loss: 50.941971\n",
      "Validation Loss: 51.573492 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 287/500\n",
      "[Mini-batch] Average Loss: 51.247936\n",
      "Validation Loss: 52.439460 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 288/500\n",
      "[Mini-batch] Average Loss: 51.028223\n",
      "Validation Loss: 51.575280 | Validation MAPE: 24.29%\n",
      "\n",
      "Epoch 289/500\n",
      "[Mini-batch] Average Loss: 50.897894\n",
      "Validation Loss: 51.344467 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 290/500\n",
      "[Mini-batch] Average Loss: 51.899610\n",
      "Validation Loss: 53.236062 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 291/500\n",
      "[Mini-batch] Average Loss: 51.679241\n",
      "Validation Loss: 52.819863 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 292/500\n",
      "[Mini-batch] Average Loss: 51.932345\n",
      "Validation Loss: 51.493528 | Validation MAPE: 24.39%\n",
      "\n",
      "Epoch 293/500\n",
      "[Mini-batch] Average Loss: 51.298527\n",
      "Validation Loss: 52.101015 | Validation MAPE: 25.51%\n",
      "\n",
      "Epoch 294/500\n",
      "[Mini-batch] Average Loss: 51.211218\n",
      "Validation Loss: 52.296856 | Validation MAPE: 24.08%\n",
      "\n",
      "Epoch 295/500\n",
      "[Mini-batch] Average Loss: 50.886415\n",
      "Validation Loss: 52.078299 | Validation MAPE: 24.62%\n",
      "\n",
      "Epoch 296/500\n",
      "[Mini-batch] Average Loss: 50.711701\n",
      "Validation Loss: 52.431846 | Validation MAPE: 25.04%\n",
      "\n",
      "Epoch 297/500\n",
      "[Mini-batch] Average Loss: 51.197690\n",
      "Validation Loss: 51.144830 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 298/500\n",
      "[Mini-batch] Average Loss: 51.393111\n",
      "Validation Loss: 53.760120 | Validation MAPE: 25.37%\n",
      "\n",
      "Epoch 299/500\n",
      "[Mini-batch] Average Loss: 51.782614\n",
      "Validation Loss: 52.392947 | Validation MAPE: 24.92%\n",
      "\n",
      "Epoch 300/500\n",
      "[Mini-batch] Average Loss: 51.513164\n",
      "Validation Loss: 51.949847 | Validation MAPE: 23.58%\n",
      "\n",
      "Epoch 301/500\n",
      "[Mini-batch] Average Loss: 51.133016\n",
      "Validation Loss: 52.932692 | Validation MAPE: 24.00%\n",
      "\n",
      "Epoch 302/500\n",
      "[Mini-batch] Average Loss: 51.276663\n",
      "Validation Loss: 52.129102 | Validation MAPE: 25.25%\n",
      "\n",
      "Epoch 303/500\n",
      "[Mini-batch] Average Loss: 51.766323\n",
      "Validation Loss: 51.642488 | Validation MAPE: 24.33%\n",
      "\n",
      "Epoch 304/500\n",
      "[Mini-batch] Average Loss: 51.122240\n",
      "Validation Loss: 52.215230 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 305/500\n",
      "[Mini-batch] Average Loss: 51.703686\n",
      "Validation Loss: 53.885403 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 306/500\n",
      "[Mini-batch] Average Loss: 51.404096\n",
      "Validation Loss: 53.016115 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 307/500\n",
      "[Mini-batch] Average Loss: 51.785483\n",
      "Validation Loss: 52.374716 | Validation MAPE: 24.86%\n",
      "\n",
      "Epoch 308/500\n",
      "[Mini-batch] Average Loss: 51.106128\n",
      "Validation Loss: 52.013454 | Validation MAPE: 24.67%\n",
      "\n",
      "Epoch 309/500\n",
      "[Mini-batch] Average Loss: 51.101672\n",
      "Validation Loss: 51.318699 | Validation MAPE: 24.67%\n",
      "\n",
      "Epoch 310/500\n",
      "[Mini-batch] Average Loss: 50.904713\n",
      "Validation Loss: 51.840499 | Validation MAPE: 24.78%\n",
      "\n",
      "Epoch 311/500\n",
      "[Mini-batch] Average Loss: 51.124001\n",
      "Validation Loss: 51.791927 | Validation MAPE: 23.93%\n",
      "\n",
      "Epoch 312/500\n",
      "[Mini-batch] Average Loss: 50.844847\n",
      "Validation Loss: 53.082247 | Validation MAPE: 25.01%\n",
      "\n",
      "Epoch 313/500\n",
      "[Mini-batch] Average Loss: 50.739583\n",
      "Validation Loss: 51.445395 | Validation MAPE: 23.61%\n",
      "\n",
      "Epoch 314/500\n",
      "[Mini-batch] Average Loss: 50.943361\n",
      "Validation Loss: 54.606592 | Validation MAPE: 23.85%\n",
      "\n",
      "Epoch 315/500\n",
      "[Mini-batch] Average Loss: 51.072525\n",
      "Validation Loss: 51.808782 | Validation MAPE: 24.87%\n",
      "\n",
      "Epoch 316/500\n",
      "[Mini-batch] Average Loss: 51.827531\n",
      "Validation Loss: 51.792576 | Validation MAPE: 24.27%\n",
      "\n",
      "Epoch 317/500\n",
      "[Mini-batch] Average Loss: 51.535643\n",
      "Validation Loss: 53.812471 | Validation MAPE: 24.82%\n",
      "\n",
      "Epoch 318/500\n",
      "[Mini-batch] Average Loss: 51.471745\n",
      "Validation Loss: 51.326165 | Validation MAPE: 23.56%\n",
      "\n",
      "Epoch 319/500\n",
      "[Mini-batch] Average Loss: 50.544825\n",
      "Validation Loss: 53.038448 | Validation MAPE: 24.56%\n",
      "\n",
      "Epoch 320/500\n",
      "[Mini-batch] Average Loss: 51.235450\n",
      "Validation Loss: 51.897931 | Validation MAPE: 24.03%\n",
      "\n",
      "Epoch 321/500\n",
      "[Mini-batch] Average Loss: 50.920883\n",
      "Validation Loss: 51.816464 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 322/500\n",
      "[Mini-batch] Average Loss: 50.928825\n",
      "Validation Loss: 50.769756 | Validation MAPE: 23.96%\n",
      "\n",
      "Epoch 323/500\n",
      "[Mini-batch] Average Loss: 51.082337\n",
      "Validation Loss: 52.033072 | Validation MAPE: 24.29%\n",
      "\n",
      "Epoch 324/500\n",
      "[Mini-batch] Average Loss: 51.036087\n",
      "Validation Loss: 53.604214 | Validation MAPE: 24.42%\n",
      "\n",
      "Epoch 325/500\n",
      "[Mini-batch] Average Loss: 50.937016\n",
      "Validation Loss: 52.563894 | Validation MAPE: 24.13%\n",
      "\n",
      "Epoch 326/500\n",
      "[Mini-batch] Average Loss: 51.016195\n",
      "Validation Loss: 51.346609 | Validation MAPE: 24.05%\n",
      "\n",
      "Epoch 327/500\n",
      "[Mini-batch] Average Loss: 50.899383\n",
      "Validation Loss: 51.594941 | Validation MAPE: 23.91%\n",
      "\n",
      "Epoch 328/500\n",
      "[Mini-batch] Average Loss: 51.208517\n",
      "Validation Loss: 53.211922 | Validation MAPE: 24.76%\n",
      "\n",
      "Epoch 329/500\n",
      "[Mini-batch] Average Loss: 51.901586\n",
      "Validation Loss: 51.806217 | Validation MAPE: 25.07%\n",
      "\n",
      "Epoch 330/500\n",
      "[Mini-batch] Average Loss: 51.838208\n",
      "Validation Loss: 53.030161 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 331/500\n",
      "[Mini-batch] Average Loss: 50.995041\n",
      "Validation Loss: 50.913652 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 332/500\n",
      "[Mini-batch] Average Loss: 50.807763\n",
      "Validation Loss: 53.975776 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 333/500\n",
      "[Mini-batch] Average Loss: 50.921627\n",
      "Validation Loss: 53.309534 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 334/500\n",
      "[Mini-batch] Average Loss: 51.007090\n",
      "Validation Loss: 51.819424 | Validation MAPE: 24.76%\n",
      "\n",
      "Epoch 335/500\n",
      "[Mini-batch] Average Loss: 51.288727\n",
      "Validation Loss: 51.886797 | Validation MAPE: 25.16%\n",
      "\n",
      "Epoch 336/500\n",
      "[Mini-batch] Average Loss: 50.974828\n",
      "Validation Loss: 51.869496 | Validation MAPE: 24.33%\n",
      "\n",
      "Epoch 337/500\n",
      "[Mini-batch] Average Loss: 50.732170\n",
      "Validation Loss: 52.330276 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 338/500\n",
      "[Mini-batch] Average Loss: 51.470697\n",
      "Validation Loss: 52.525605 | Validation MAPE: 23.90%\n",
      "\n",
      "Epoch 339/500\n",
      "[Mini-batch] Average Loss: 51.709544\n",
      "Validation Loss: 51.342969 | Validation MAPE: 23.73%\n",
      "\n",
      "Epoch 340/500\n",
      "[Mini-batch] Average Loss: 50.591591\n",
      "Validation Loss: 50.974893 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 341/500\n",
      "[Mini-batch] Average Loss: 50.466221\n",
      "Validation Loss: 52.247637 | Validation MAPE: 24.44%\n",
      "\n",
      "Epoch 342/500\n",
      "[Mini-batch] Average Loss: 51.056922\n",
      "Validation Loss: 53.149784 | Validation MAPE: 25.11%\n",
      "\n",
      "Epoch 343/500\n",
      "[Mini-batch] Average Loss: 50.931952\n",
      "Validation Loss: 52.658027 | Validation MAPE: 25.34%\n",
      "\n",
      "Epoch 344/500\n",
      "[Mini-batch] Average Loss: 51.200409\n",
      "Validation Loss: 52.151094 | Validation MAPE: 25.04%\n",
      "\n",
      "Epoch 345/500\n",
      "[Mini-batch] Average Loss: 51.309937\n",
      "Validation Loss: 52.343028 | Validation MAPE: 25.47%\n",
      "\n",
      "Epoch 346/500\n",
      "[Mini-batch] Average Loss: 51.169319\n",
      "Validation Loss: 52.811591 | Validation MAPE: 24.97%\n",
      "\n",
      "Epoch 347/500\n",
      "[Mini-batch] Average Loss: 51.333104\n",
      "Validation Loss: 51.778915 | Validation MAPE: 24.06%\n",
      "\n",
      "Epoch 348/500\n",
      "[Mini-batch] Average Loss: 51.425976\n",
      "Validation Loss: 52.241585 | Validation MAPE: 24.88%\n",
      "\n",
      "Epoch 349/500\n",
      "[Mini-batch] Average Loss: 51.521925\n",
      "Validation Loss: 52.457434 | Validation MAPE: 24.49%\n",
      "\n",
      "Epoch 350/500\n",
      "[Mini-batch] Average Loss: 51.406579\n",
      "Validation Loss: 54.460646 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 351/500\n",
      "[Mini-batch] Average Loss: 52.272808\n",
      "Validation Loss: 53.497197 | Validation MAPE: 24.85%\n",
      "\n",
      "Epoch 352/500\n",
      "[Mini-batch] Average Loss: 51.392754\n",
      "Validation Loss: 51.481401 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 353/500\n",
      "[Mini-batch] Average Loss: 51.344859\n",
      "Validation Loss: 51.807011 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 354/500\n",
      "[Mini-batch] Average Loss: 51.892562\n",
      "Validation Loss: 52.085444 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 355/500\n",
      "[Mini-batch] Average Loss: 51.534800\n",
      "Validation Loss: 53.085948 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 356/500\n",
      "[Mini-batch] Average Loss: 51.447692\n",
      "Validation Loss: 53.358016 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 357/500\n",
      "[Mini-batch] Average Loss: 51.261418\n",
      "Validation Loss: 52.597944 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 358/500\n",
      "[Mini-batch] Average Loss: 51.522106\n",
      "Validation Loss: 52.139430 | Validation MAPE: 24.19%\n",
      "\n",
      "Epoch 359/500\n",
      "[Mini-batch] Average Loss: 51.335476\n",
      "Validation Loss: 52.389703 | Validation MAPE: 24.91%\n",
      "\n",
      "Epoch 360/500\n",
      "[Mini-batch] Average Loss: 51.093497\n",
      "Validation Loss: 51.693892 | Validation MAPE: 24.41%\n",
      "\n",
      "Epoch 361/500\n",
      "[Mini-batch] Average Loss: 50.578776\n",
      "Validation Loss: 51.365700 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 362/500\n",
      "[Mini-batch] Average Loss: 50.625837\n",
      "Validation Loss: 51.664773 | Validation MAPE: 24.96%\n",
      "\n",
      "Epoch 363/500\n",
      "[Mini-batch] Average Loss: 50.977435\n",
      "Validation Loss: 52.501099 | Validation MAPE: 24.90%\n",
      "\n",
      "Epoch 364/500\n",
      "[Mini-batch] Average Loss: 50.730909\n",
      "Validation Loss: 51.806161 | Validation MAPE: 24.23%\n",
      "\n",
      "Epoch 365/500\n",
      "[Mini-batch] Average Loss: 50.897776\n",
      "Validation Loss: 51.594683 | Validation MAPE: 23.73%\n",
      "\n",
      "Epoch 366/500\n",
      "[Mini-batch] Average Loss: 51.004146\n",
      "Validation Loss: 52.315280 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 367/500\n",
      "[Mini-batch] Average Loss: 51.385381\n",
      "Validation Loss: 52.471573 | Validation MAPE: 24.60%\n",
      "\n",
      "Epoch 368/500\n",
      "[Mini-batch] Average Loss: 51.620268\n",
      "Validation Loss: 52.124876 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 369/500\n",
      "[Mini-batch] Average Loss: 50.914621\n",
      "Validation Loss: 51.522861 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 370/500\n",
      "[Mini-batch] Average Loss: 50.608886\n",
      "Validation Loss: 51.093038 | Validation MAPE: 24.41%\n",
      "\n",
      "Epoch 371/500\n",
      "[Mini-batch] Average Loss: 50.626134\n",
      "Validation Loss: 51.462649 | Validation MAPE: 24.29%\n",
      "\n",
      "Epoch 372/500\n",
      "[Mini-batch] Average Loss: 51.394364\n",
      "Validation Loss: 51.747957 | Validation MAPE: 24.68%\n",
      "\n",
      "Epoch 373/500\n",
      "[Mini-batch] Average Loss: 50.917599\n",
      "Validation Loss: 51.268407 | Validation MAPE: 24.25%\n",
      "\n",
      "Epoch 374/500\n",
      "[Mini-batch] Average Loss: 51.162854\n",
      "Validation Loss: 51.748463 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 375/500\n",
      "[Mini-batch] Average Loss: 51.032275\n",
      "Validation Loss: 53.215679 | Validation MAPE: 23.97%\n",
      "\n",
      "Epoch 376/500\n",
      "[Mini-batch] Average Loss: 50.952770\n",
      "Validation Loss: 52.373423 | Validation MAPE: 24.74%\n",
      "\n",
      "Epoch 377/500\n",
      "[Mini-batch] Average Loss: 50.868889\n",
      "Validation Loss: 51.671445 | Validation MAPE: 23.81%\n",
      "\n",
      "Epoch 378/500\n",
      "[Mini-batch] Average Loss: 50.489028\n",
      "Validation Loss: 51.090827 | Validation MAPE: 24.01%\n",
      "\n",
      "Epoch 379/500\n",
      "[Mini-batch] Average Loss: 50.532742\n",
      "Validation Loss: 51.851875 | Validation MAPE: 24.37%\n",
      "\n",
      "Epoch 380/500\n",
      "[Mini-batch] Average Loss: 50.346187\n",
      "Validation Loss: 51.670148 | Validation MAPE: 24.66%\n",
      "\n",
      "Epoch 381/500\n",
      "[Mini-batch] Average Loss: 50.639714\n",
      "Validation Loss: 53.282133 | Validation MAPE: 23.52%\n",
      "\n",
      "Epoch 382/500\n",
      "[Mini-batch] Average Loss: 51.312438\n",
      "Validation Loss: 52.361560 | Validation MAPE: 24.37%\n",
      "\n",
      "Epoch 383/500\n",
      "[Mini-batch] Average Loss: 51.615724\n",
      "Validation Loss: 51.688064 | Validation MAPE: 23.60%\n",
      "\n",
      "Epoch 384/500\n",
      "[Mini-batch] Average Loss: 50.770820\n",
      "Validation Loss: 52.125396 | Validation MAPE: 24.92%\n",
      "\n",
      "Epoch 385/500\n",
      "[Mini-batch] Average Loss: 50.680265\n",
      "Validation Loss: 51.543612 | Validation MAPE: 24.22%\n",
      "\n",
      "Epoch 386/500\n",
      "[Mini-batch] Average Loss: 50.918304\n",
      "Validation Loss: 51.496826 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 387/500\n",
      "[Mini-batch] Average Loss: 50.774667\n",
      "Validation Loss: 51.770832 | Validation MAPE: 24.34%\n",
      "\n",
      "Epoch 388/500\n",
      "[Mini-batch] Average Loss: 50.944963\n",
      "Validation Loss: 51.327665 | Validation MAPE: 24.81%\n",
      "\n",
      "Epoch 389/500\n",
      "[Mini-batch] Average Loss: 50.521822\n",
      "Validation Loss: 51.591000 | Validation MAPE: 24.50%\n",
      "\n",
      "Epoch 390/500\n",
      "[Mini-batch] Average Loss: 50.672027\n",
      "Validation Loss: 51.601210 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 391/500\n",
      "[Mini-batch] Average Loss: 51.242618\n",
      "Validation Loss: 51.895978 | Validation MAPE: 24.65%\n",
      "\n",
      "Epoch 392/500\n",
      "[Mini-batch] Average Loss: 51.265163\n",
      "Validation Loss: 50.962324 | Validation MAPE: 23.70%\n",
      "\n",
      "Epoch 393/500\n",
      "[Mini-batch] Average Loss: 51.084070\n",
      "Validation Loss: 51.262582 | Validation MAPE: 23.54%\n",
      "\n",
      "Epoch 394/500\n",
      "[Mini-batch] Average Loss: 50.794528\n",
      "Validation Loss: 51.658715 | Validation MAPE: 24.66%\n",
      "\n",
      "Epoch 395/500\n",
      "[Mini-batch] Average Loss: 50.954020\n",
      "Validation Loss: 52.710317 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 396/500\n",
      "[Mini-batch] Average Loss: 50.876796\n",
      "Validation Loss: 51.123408 | Validation MAPE: 24.70%\n",
      "\n",
      "Epoch 397/500\n",
      "[Mini-batch] Average Loss: 51.139379\n",
      "Validation Loss: 53.586777 | Validation MAPE: 25.35%\n",
      "\n",
      "Epoch 398/500\n",
      "[Mini-batch] Average Loss: 51.484575\n",
      "Validation Loss: 51.874047 | Validation MAPE: 24.14%\n",
      "\n",
      "Epoch 399/500\n",
      "[Mini-batch] Average Loss: 51.384984\n",
      "Validation Loss: 52.822214 | Validation MAPE: 24.95%\n",
      "\n",
      "Epoch 400/500\n",
      "[Mini-batch] Average Loss: 50.821702\n",
      "Validation Loss: 51.705128 | Validation MAPE: 24.73%\n",
      "\n",
      "Epoch 401/500\n",
      "[Mini-batch] Average Loss: 50.661616\n",
      "Validation Loss: 52.248242 | Validation MAPE: 24.68%\n",
      "\n",
      "Epoch 402/500\n",
      "[Mini-batch] Average Loss: 50.763528\n",
      "Validation Loss: 52.386507 | Validation MAPE: 25.21%\n",
      "\n",
      "Epoch 403/500\n",
      "[Mini-batch] Average Loss: 51.220516\n",
      "Validation Loss: 52.600086 | Validation MAPE: 24.12%\n",
      "\n",
      "Epoch 404/500\n",
      "[Mini-batch] Average Loss: 50.814786\n",
      "Validation Loss: 51.330507 | Validation MAPE: 23.14%\n",
      "\n",
      "Epoch 405/500\n",
      "[Mini-batch] Average Loss: 50.864183\n",
      "Validation Loss: 51.962437 | Validation MAPE: 25.20%\n",
      "\n",
      "Epoch 406/500\n",
      "[Mini-batch] Average Loss: 50.739342\n",
      "Validation Loss: 51.981520 | Validation MAPE: 24.86%\n",
      "\n",
      "Epoch 407/500\n",
      "[Mini-batch] Average Loss: 50.603694\n",
      "Validation Loss: 52.221689 | Validation MAPE: 24.73%\n",
      "\n",
      "Epoch 408/500\n",
      "[Mini-batch] Average Loss: 51.539217\n",
      "Validation Loss: 51.218074 | Validation MAPE: 24.02%\n",
      "\n",
      "Epoch 409/500\n",
      "[Mini-batch] Average Loss: 51.563049\n",
      "Validation Loss: 51.745858 | Validation MAPE: 24.77%\n",
      "\n",
      "Epoch 410/500\n",
      "[Mini-batch] Average Loss: 50.844463\n",
      "Validation Loss: 53.047343 | Validation MAPE: 24.52%\n",
      "\n",
      "Epoch 411/500\n",
      "[Mini-batch] Average Loss: 51.420962\n",
      "Validation Loss: 51.430136 | Validation MAPE: 24.12%\n",
      "\n",
      "Epoch 412/500\n",
      "[Mini-batch] Average Loss: 51.028807\n",
      "Validation Loss: 50.739319 | Validation MAPE: 23.72%\n",
      "\n",
      "Epoch 413/500\n",
      "[Mini-batch] Average Loss: 50.743911\n",
      "Validation Loss: 51.493207 | Validation MAPE: 25.20%\n",
      "\n",
      "Epoch 414/500\n",
      "[Mini-batch] Average Loss: 50.981816\n",
      "Validation Loss: 51.182065 | Validation MAPE: 24.60%\n",
      "\n",
      "Epoch 415/500\n",
      "[Mini-batch] Average Loss: 50.765337\n",
      "Validation Loss: 51.761650 | Validation MAPE: 24.57%\n",
      "\n",
      "Epoch 416/500\n",
      "[Mini-batch] Average Loss: 50.776396\n",
      "Validation Loss: 52.330394 | Validation MAPE: 24.35%\n",
      "\n",
      "Epoch 417/500\n",
      "[Mini-batch] Average Loss: 51.214010\n",
      "Validation Loss: 51.838919 | Validation MAPE: 24.09%\n",
      "\n",
      "Epoch 418/500\n",
      "[Mini-batch] Average Loss: 51.014224\n",
      "Validation Loss: 51.583133 | Validation MAPE: 24.17%\n",
      "\n",
      "Epoch 419/500\n",
      "[Mini-batch] Average Loss: 50.498899\n",
      "Validation Loss: 51.466200 | Validation MAPE: 24.88%\n",
      "\n",
      "Epoch 420/500\n",
      "[Mini-batch] Average Loss: 50.580696\n",
      "Validation Loss: 52.449019 | Validation MAPE: 23.89%\n",
      "\n",
      "Epoch 421/500\n",
      "[Mini-batch] Average Loss: 50.536702\n",
      "Validation Loss: 51.360045 | Validation MAPE: 24.97%\n",
      "\n",
      "Epoch 422/500\n",
      "[Mini-batch] Average Loss: 50.706886\n",
      "Validation Loss: 51.650414 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 423/500\n",
      "[Mini-batch] Average Loss: 50.886593\n",
      "Validation Loss: 51.109245 | Validation MAPE: 23.65%\n",
      "\n",
      "Epoch 424/500\n",
      "[Mini-batch] Average Loss: 50.722159\n",
      "Validation Loss: 51.647748 | Validation MAPE: 24.94%\n",
      "\n",
      "Epoch 425/500\n",
      "[Mini-batch] Average Loss: 51.279438\n",
      "Validation Loss: 51.872523 | Validation MAPE: 23.66%\n",
      "\n",
      "Epoch 426/500\n",
      "[Mini-batch] Average Loss: 50.898102\n",
      "Validation Loss: 54.768911 | Validation MAPE: 26.52%\n",
      "\n",
      "Epoch 427/500\n",
      "[Mini-batch] Average Loss: 52.006327\n",
      "Validation Loss: 51.995770 | Validation MAPE: 24.28%\n",
      "\n",
      "Epoch 428/500\n",
      "[Mini-batch] Average Loss: 50.693758\n",
      "Validation Loss: 50.710991 | Validation MAPE: 24.16%\n",
      "\n",
      "Epoch 429/500\n",
      "[Mini-batch] Average Loss: 51.670544\n",
      "Validation Loss: 53.094169 | Validation MAPE: 24.27%\n",
      "\n",
      "Epoch 430/500\n",
      "[Mini-batch] Average Loss: 51.581118\n",
      "Validation Loss: 52.132554 | Validation MAPE: 25.20%\n",
      "\n",
      "Epoch 431/500\n",
      "[Mini-batch] Average Loss: 51.131595\n",
      "Validation Loss: 51.371460 | Validation MAPE: 24.47%\n",
      "\n",
      "Epoch 432/500\n",
      "[Mini-batch] Average Loss: 50.867963\n",
      "Validation Loss: 53.122297 | Validation MAPE: 24.73%\n",
      "\n",
      "Epoch 433/500\n",
      "[Mini-batch] Average Loss: 52.007201\n",
      "Validation Loss: 51.884635 | Validation MAPE: 24.54%\n",
      "\n",
      "Epoch 434/500\n",
      "[Mini-batch] Average Loss: 50.913135\n",
      "Validation Loss: 51.471802 | Validation MAPE: 23.16%\n",
      "\n",
      "Epoch 435/500\n",
      "[Mini-batch] Average Loss: 50.910639\n",
      "Validation Loss: 51.218440 | Validation MAPE: 23.72%\n",
      "\n",
      "Epoch 436/500\n",
      "[Mini-batch] Average Loss: 50.905370\n",
      "Validation Loss: 51.184449 | Validation MAPE: 23.90%\n",
      "\n",
      "Epoch 437/500\n",
      "[Mini-batch] Average Loss: 51.063177\n",
      "Validation Loss: 52.000234 | Validation MAPE: 24.64%\n",
      "\n",
      "Epoch 438/500\n",
      "[Mini-batch] Average Loss: 50.977770\n",
      "Validation Loss: 52.087834 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 439/500\n",
      "[Mini-batch] Average Loss: 50.684434\n",
      "Validation Loss: 52.497722 | Validation MAPE: 24.63%\n",
      "\n",
      "Epoch 440/500\n",
      "[Mini-batch] Average Loss: 51.290084\n",
      "Validation Loss: 51.915054 | Validation MAPE: 25.14%\n",
      "\n",
      "Epoch 441/500\n",
      "[Mini-batch] Average Loss: 51.180137\n",
      "Validation Loss: 50.955634 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 442/500\n",
      "[Mini-batch] Average Loss: 51.136349\n",
      "Validation Loss: 52.540021 | Validation MAPE: 24.20%\n",
      "\n",
      "Epoch 443/500\n",
      "[Mini-batch] Average Loss: 51.775427\n",
      "Validation Loss: 51.640835 | Validation MAPE: 24.01%\n",
      "\n",
      "Epoch 444/500\n",
      "[Mini-batch] Average Loss: 50.600053\n",
      "Validation Loss: 51.377589 | Validation MAPE: 24.87%\n",
      "\n",
      "Epoch 445/500\n",
      "[Mini-batch] Average Loss: 50.910020\n",
      "Validation Loss: 51.655223 | Validation MAPE: 25.07%\n",
      "\n",
      "Epoch 446/500\n",
      "[Mini-batch] Average Loss: 50.938680\n",
      "Validation Loss: 51.548860 | Validation MAPE: 24.11%\n",
      "\n",
      "Epoch 447/500\n",
      "[Mini-batch] Average Loss: 50.627264\n",
      "Validation Loss: 53.102788 | Validation MAPE: 24.30%\n",
      "\n",
      "Epoch 448/500\n",
      "[Mini-batch] Average Loss: 50.776835\n",
      "Validation Loss: 52.929325 | Validation MAPE: 24.12%\n",
      "\n",
      "Epoch 449/500\n",
      "[Mini-batch] Average Loss: 51.167655\n",
      "Validation Loss: 53.339216 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 450/500\n",
      "[Mini-batch] Average Loss: 51.096110\n",
      "Validation Loss: 51.754490 | Validation MAPE: 25.34%\n",
      "\n",
      "Epoch 451/500\n",
      "[Mini-batch] Average Loss: 50.912018\n",
      "Validation Loss: 51.080026 | Validation MAPE: 24.08%\n",
      "\n",
      "Epoch 452/500\n",
      "[Mini-batch] Average Loss: 50.744055\n",
      "Validation Loss: 52.951655 | Validation MAPE: 25.42%\n",
      "\n",
      "Epoch 453/500\n",
      "[Mini-batch] Average Loss: 50.821710\n",
      "Validation Loss: 51.500964 | Validation MAPE: 25.08%\n",
      "\n",
      "Epoch 454/500\n",
      "[Mini-batch] Average Loss: 50.843271\n",
      "Validation Loss: 52.280222 | Validation MAPE: 25.10%\n",
      "\n",
      "Epoch 455/500\n",
      "[Mini-batch] Average Loss: 50.701689\n",
      "Validation Loss: 52.027097 | Validation MAPE: 23.89%\n",
      "\n",
      "Epoch 456/500\n",
      "[Mini-batch] Average Loss: 50.620284\n",
      "Validation Loss: 51.918739 | Validation MAPE: 23.99%\n",
      "\n",
      "Epoch 457/500\n",
      "[Mini-batch] Average Loss: 50.958253\n",
      "Validation Loss: 52.167409 | Validation MAPE: 25.11%\n",
      "\n",
      "Epoch 458/500\n",
      "[Mini-batch] Average Loss: 50.869676\n",
      "Validation Loss: 51.354046 | Validation MAPE: 24.16%\n",
      "\n",
      "Epoch 459/500\n",
      "[Mini-batch] Average Loss: 50.976759\n",
      "Validation Loss: 51.788488 | Validation MAPE: 24.37%\n",
      "\n",
      "Epoch 460/500\n",
      "[Mini-batch] Average Loss: 51.034571\n",
      "Validation Loss: 51.805129 | Validation MAPE: 24.59%\n",
      "\n",
      "Epoch 461/500\n",
      "[Mini-batch] Average Loss: 50.498435\n",
      "Validation Loss: 52.223930 | Validation MAPE: 24.84%\n",
      "\n",
      "Epoch 462/500\n",
      "[Mini-batch] Average Loss: 50.461573\n",
      "Validation Loss: 51.582395 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 463/500\n",
      "[Mini-batch] Average Loss: 50.899997\n",
      "Validation Loss: 52.346996 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 464/500\n",
      "[Mini-batch] Average Loss: 51.697324\n",
      "Validation Loss: 51.518718 | Validation MAPE: 24.13%\n",
      "\n",
      "Epoch 465/500\n",
      "[Mini-batch] Average Loss: 51.789540\n",
      "Validation Loss: 52.048766 | Validation MAPE: 24.55%\n",
      "\n",
      "Epoch 466/500\n",
      "[Mini-batch] Average Loss: 50.941289\n",
      "Validation Loss: 51.757329 | Validation MAPE: 25.01%\n",
      "\n",
      "Epoch 467/500\n",
      "[Mini-batch] Average Loss: 50.965881\n",
      "Validation Loss: 51.784320 | Validation MAPE: 25.43%\n",
      "\n",
      "Epoch 468/500\n",
      "[Mini-batch] Average Loss: 50.866641\n",
      "Validation Loss: 51.929265 | Validation MAPE: 24.27%\n",
      "\n",
      "Epoch 469/500\n",
      "[Mini-batch] Average Loss: 51.980352\n",
      "Validation Loss: 53.271572 | Validation MAPE: 25.32%\n",
      "\n",
      "Epoch 470/500\n",
      "[Mini-batch] Average Loss: 51.505213\n",
      "Validation Loss: 51.914933 | Validation MAPE: 23.70%\n",
      "\n",
      "Epoch 471/500\n",
      "[Mini-batch] Average Loss: 51.402436\n",
      "Validation Loss: 51.301683 | Validation MAPE: 24.28%\n",
      "\n",
      "Epoch 472/500\n",
      "[Mini-batch] Average Loss: 51.046942\n",
      "Validation Loss: 51.983595 | Validation MAPE: 23.95%\n",
      "\n",
      "Epoch 473/500\n",
      "[Mini-batch] Average Loss: 52.325542\n",
      "Validation Loss: 52.233814 | Validation MAPE: 24.30%\n",
      "\n",
      "Epoch 474/500\n",
      "[Mini-batch] Average Loss: 52.226018\n",
      "Validation Loss: 52.068610 | Validation MAPE: 25.39%\n",
      "\n",
      "Epoch 475/500\n",
      "[Mini-batch] Average Loss: 51.416345\n",
      "Validation Loss: 52.163357 | Validation MAPE: 24.43%\n",
      "\n",
      "Epoch 476/500\n",
      "[Mini-batch] Average Loss: 51.490934\n",
      "Validation Loss: 51.087665 | Validation MAPE: 24.04%\n",
      "\n",
      "Epoch 477/500\n",
      "[Mini-batch] Average Loss: 50.839675\n",
      "Validation Loss: 52.647375 | Validation MAPE: 24.59%\n",
      "\n",
      "Epoch 478/500\n",
      "[Mini-batch] Average Loss: 51.559413\n",
      "Validation Loss: 52.652876 | Validation MAPE: 24.24%\n",
      "\n",
      "Epoch 479/500\n",
      "[Mini-batch] Average Loss: 51.309554\n",
      "Validation Loss: 51.951143 | Validation MAPE: 25.11%\n",
      "\n",
      "Epoch 480/500\n",
      "[Mini-batch] Average Loss: 50.935415\n",
      "Validation Loss: 52.256376 | Validation MAPE: 23.84%\n",
      "\n",
      "Epoch 481/500\n",
      "[Mini-batch] Average Loss: 51.107015\n",
      "Validation Loss: 52.022149 | Validation MAPE: 24.31%\n",
      "\n",
      "Epoch 482/500\n",
      "[Mini-batch] Average Loss: 50.218395\n",
      "Validation Loss: 52.108757 | Validation MAPE: 23.87%\n",
      "\n",
      "Epoch 483/500\n",
      "[Mini-batch] Average Loss: 50.965227\n",
      "Validation Loss: 52.421780 | Validation MAPE: 24.96%\n",
      "\n",
      "Epoch 484/500\n",
      "[Mini-batch] Average Loss: 50.734945\n",
      "Validation Loss: 54.657558 | Validation MAPE: 26.86%\n",
      "\n",
      "Epoch 485/500\n",
      "[Mini-batch] Average Loss: 51.629281\n",
      "Validation Loss: 51.206815 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 486/500\n",
      "[Mini-batch] Average Loss: 50.950235\n",
      "Validation Loss: 51.522941 | Validation MAPE: 24.21%\n",
      "\n",
      "Epoch 487/500\n",
      "[Mini-batch] Average Loss: 51.020358\n",
      "Validation Loss: 52.262193 | Validation MAPE: 24.13%\n",
      "\n",
      "Epoch 488/500\n",
      "[Mini-batch] Average Loss: 51.184492\n",
      "Validation Loss: 51.423699 | Validation MAPE: 24.33%\n",
      "\n",
      "Epoch 489/500\n",
      "[Mini-batch] Average Loss: 51.079380\n",
      "Validation Loss: 51.625217 | Validation MAPE: 24.39%\n",
      "\n",
      "Epoch 490/500\n",
      "[Mini-batch] Average Loss: 51.326473\n",
      "Validation Loss: 51.942830 | Validation MAPE: 24.32%\n",
      "\n",
      "Epoch 491/500\n",
      "[Mini-batch] Average Loss: 50.669007\n",
      "Validation Loss: 50.571732 | Validation MAPE: 24.17%\n",
      "\n",
      "Epoch 492/500\n",
      "[Mini-batch] Average Loss: 50.682779\n",
      "Validation Loss: 52.659290 | Validation MAPE: 24.91%\n",
      "\n",
      "Epoch 493/500\n",
      "[Mini-batch] Average Loss: 50.588290\n",
      "Validation Loss: 52.115369 | Validation MAPE: 24.82%\n",
      "\n",
      "Epoch 494/500\n",
      "[Mini-batch] Average Loss: 50.567794\n",
      "Validation Loss: 51.369581 | Validation MAPE: 23.85%\n",
      "\n",
      "Epoch 495/500\n",
      "[Mini-batch] Average Loss: 50.459513\n",
      "Validation Loss: 51.549967 | Validation MAPE: 24.48%\n",
      "\n",
      "Epoch 496/500\n",
      "[Mini-batch] Average Loss: 50.714040\n",
      "Validation Loss: 51.628430 | Validation MAPE: 24.33%\n",
      "\n",
      "Epoch 497/500\n",
      "[Mini-batch] Average Loss: 50.879475\n",
      "Validation Loss: 51.842600 | Validation MAPE: 25.07%\n",
      "\n",
      "Epoch 498/500\n",
      "[Mini-batch] Average Loss: 50.533753\n",
      "Validation Loss: 51.961738 | Validation MAPE: 24.18%\n",
      "\n",
      "Epoch 499/500\n",
      "[Mini-batch] Average Loss: 50.502723\n",
      "Validation Loss: 51.475273 | Validation MAPE: 24.46%\n",
      "\n",
      "Epoch 500/500\n",
      "[Mini-batch] Average Loss: 51.190574\n",
      "Validation Loss: 52.617590 | Validation MAPE: 24.35%\n",
      "Training completed!\n",
      "\n",
      "--- TEST EVALUATION ---\n",
      "Test Loss: 847.692299 | Test MAPE: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(847.6922987508428, 99.99995481530202)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(X_train, y_train, \n",
    "      lr=0.01, \n",
    "      epochs= 500,\n",
    "      method=\"Mini Batch\", \n",
    "      batch_size=32, \n",
    "      alpha=0.001, \n",
    "      X_val=X_val, y_val=y_val)\n",
    "\n",
    "evaluate_model(nn_adam, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4bec1",
   "metadata": {},
   "source": [
    "## prova con minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc85619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obj function (loss) to minimize \n",
    "def objective(theta, nn, X, y, alpha):\n",
    "    nn.set_params_vector(theta)\n",
    "    y_pred, _, _ = nn.forward(X)\n",
    "    return nn.mse_loss(y, y_pred, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7368f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian of the objective\n",
    "def objective_jac(theta, nn, X, y, alpha):\n",
    "    nn.set_params_vector(theta)\n",
    "    grads_W, grads_b = nn.backward(X, y, alpha)\n",
    "    # Flatten grads\n",
    "    grads = []\n",
    "    for gw, gb in zip(grads_W, grads_b):\n",
    "        grads.append(gw.flatten())\n",
    "        grads.append(gb.flatten())\n",
    "    return np.concatenate(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "nn_minimize = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")\n",
    "\n",
    "# Inizializzazione\n",
    "theta0 = nn_minimize.get_params_vector()\n",
    "\n",
    "# Obiettivo da minimizzare\n",
    "obj_fun = lambda theta: objective(theta, nn_minimize, X_train, y_train, alpha=0.01)\n",
    "obj_jac = lambda theta: objective_jac(theta, nn_minimize, X_train, y_train, alpha=0.01)\n",
    "\n",
    "# Ottimizzazione\n",
    "res = minimize(fun=obj_fun,\n",
    "               x0=theta0,\n",
    "               jac=obj_jac,\n",
    "               method=\"L-BFGS-B\",\n",
    "               options={\"maxiter\": 500})\n",
    "\n",
    "# Aggiorna i parametri del modello\n",
    "nn_minimize.set_params_vector(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 46.731886 | Training MAPE: 23.08%\n",
      "Validation Loss: 48.126933 | Validation MAPE: 23.79%\n",
      "\n",
      "--- TEST EVALUATION ---\n",
      "Test Loss: 46.590493 | Test MAPE: 22.58%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46.59049299711187, 22.575495773220574)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "y_pred_train, _, _ = nn_minimize.forward(X_train)\n",
    "train_loss = nn_minimize.mse_loss(y_train, y_pred_train, alpha=0)  # senza reg!\n",
    "train_mape = mape(y_train, y_pred_train)\n",
    "\n",
    "# Validation\n",
    "y_pred_val, _, _ = nn_minimize.forward(X_val)\n",
    "val_loss = nn_minimize.mse_loss(y_val, y_pred_val, alpha=0)\n",
    "val_mape = mape(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Training Loss: {train_loss:.6f} | Training MAPE: {train_mape:.2f}%\")\n",
    "print(f\"Validation Loss: {val_loss:.6f} | Validation MAPE: {val_mape:.2f}%\")\n",
    "\n",
    "evaluate_model(nn_minimize, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3118a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
