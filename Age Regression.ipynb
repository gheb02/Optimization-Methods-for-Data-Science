{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e61ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ff8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/AGE_PREDICTION.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232953f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.686191</td>\n",
       "      <td>-0.989465</td>\n",
       "      <td>-0.920503</td>\n",
       "      <td>1.607427</td>\n",
       "      <td>-0.896248</td>\n",
       "      <td>1.118974</td>\n",
       "      <td>-0.969456</td>\n",
       "      <td>1.811707</td>\n",
       "      <td>2.560955</td>\n",
       "      <td>3.803463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862891</td>\n",
       "      <td>-0.909545</td>\n",
       "      <td>-0.915361</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>-0.989461</td>\n",
       "      <td>1.911855</td>\n",
       "      <td>1.409705</td>\n",
       "      <td>2.303997</td>\n",
       "      <td>-0.981840</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887917</td>\n",
       "      <td>4.915272</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>-0.964685</td>\n",
       "      <td>-0.478649</td>\n",
       "      <td>4.342395</td>\n",
       "      <td>-0.332870</td>\n",
       "      <td>-0.768041</td>\n",
       "      <td>-0.815375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.965917</td>\n",
       "      <td>-0.969461</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>5.304822</td>\n",
       "      <td>0.934790</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>4.919212</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.923215</td>\n",
       "      <td>2.746968</td>\n",
       "      <td>-0.918085</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.908587</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>2.984481</td>\n",
       "      <td>0.535007</td>\n",
       "      <td>-0.591029</td>\n",
       "      <td>-0.324043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809726</td>\n",
       "      <td>-0.929934</td>\n",
       "      <td>-0.891814</td>\n",
       "      <td>-0.881796</td>\n",
       "      <td>3.415373</td>\n",
       "      <td>1.044108</td>\n",
       "      <td>-0.442615</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>2.628199</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.268866</td>\n",
       "      <td>-0.408416</td>\n",
       "      <td>-0.935145</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>-0.922438</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>-0.046606</td>\n",
       "      <td>1.149634</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.357959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.834968</td>\n",
       "      <td>-0.937475</td>\n",
       "      <td>-0.917737</td>\n",
       "      <td>-0.929519</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>1.608048</td>\n",
       "      <td>0.276169</td>\n",
       "      <td>1.246468</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529231</td>\n",
       "      <td>-0.829957</td>\n",
       "      <td>-0.897425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>-0.865304</td>\n",
       "      <td>0.331018</td>\n",
       "      <td>-0.644940</td>\n",
       "      <td>1.296097</td>\n",
       "      <td>1.166863</td>\n",
       "      <td>2.036034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775411</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-0.864018</td>\n",
       "      <td>-0.908001</td>\n",
       "      <td>-0.784495</td>\n",
       "      <td>1.329586</td>\n",
       "      <td>0.547925</td>\n",
       "      <td>1.195395</td>\n",
       "      <td>-0.810089</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
       "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
       "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
       "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
       "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
       "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
       "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
       "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
       "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
       "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
       "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
       "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
       "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_loss(y_real, y_pred, alpha, weights):\n",
    "    reg = alpha*(weights**2)\n",
    "\n",
    "    loss = np.mean((y_real-y_pred)**2)\n",
    "    return loss+reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def _init_(self, neurons, activation='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros(out_dim)\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return 1-self.activation(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return self.activation(x)*(1-self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        for i in range(self.layers - 1):  # all hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return output\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size]\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha, weights):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        weights: list of weight matrices\n",
    "        \"\"\"\n",
    "        loss = np.mean((y_real - y_pred)**2)\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    def loss(flat_params, model, X, y, alpha):\n",
    "        \"\"\"\n",
    "        flat_params: flat vector of weights and biases\n",
    "        model: NeuralNetwork model\n",
    "        X: input features\n",
    "        y: target values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        model.set_params_vector(flat_params)\n",
    "        y_pred = model.forward(X)\n",
    "        return mse_loss(y, y_pred, alpha, model.weights)\n",
    "    \n",
    "    def mape(y_real, y_pred):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.sum(np.abs((y_real-y_pred)/y_real))*(100/len(y_real))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
