{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e61ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8ff8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.686191</td>\n",
       "      <td>-0.989465</td>\n",
       "      <td>-0.920503</td>\n",
       "      <td>1.607427</td>\n",
       "      <td>-0.896248</td>\n",
       "      <td>1.118974</td>\n",
       "      <td>-0.969456</td>\n",
       "      <td>1.811707</td>\n",
       "      <td>2.560955</td>\n",
       "      <td>3.803463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862891</td>\n",
       "      <td>-0.909545</td>\n",
       "      <td>-0.915361</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>-0.989461</td>\n",
       "      <td>1.911855</td>\n",
       "      <td>1.409705</td>\n",
       "      <td>2.303997</td>\n",
       "      <td>-0.981840</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887917</td>\n",
       "      <td>4.915272</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>-0.964685</td>\n",
       "      <td>-0.478649</td>\n",
       "      <td>4.342395</td>\n",
       "      <td>-0.332870</td>\n",
       "      <td>-0.768041</td>\n",
       "      <td>-0.815375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.965917</td>\n",
       "      <td>-0.969461</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>5.304822</td>\n",
       "      <td>0.934790</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>4.919212</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.923215</td>\n",
       "      <td>2.746968</td>\n",
       "      <td>-0.918085</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.908587</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>2.984481</td>\n",
       "      <td>0.535007</td>\n",
       "      <td>-0.591029</td>\n",
       "      <td>-0.324043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809726</td>\n",
       "      <td>-0.929934</td>\n",
       "      <td>-0.891814</td>\n",
       "      <td>-0.881796</td>\n",
       "      <td>3.415373</td>\n",
       "      <td>1.044108</td>\n",
       "      <td>-0.442615</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>2.628199</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.268866</td>\n",
       "      <td>-0.408416</td>\n",
       "      <td>-0.935145</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>-0.922438</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>-0.046606</td>\n",
       "      <td>1.149634</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.357959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.834968</td>\n",
       "      <td>-0.937475</td>\n",
       "      <td>-0.917737</td>\n",
       "      <td>-0.929519</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>1.608048</td>\n",
       "      <td>0.276169</td>\n",
       "      <td>1.246468</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529231</td>\n",
       "      <td>-0.829957</td>\n",
       "      <td>-0.897425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>-0.865304</td>\n",
       "      <td>0.331018</td>\n",
       "      <td>-0.644940</td>\n",
       "      <td>1.296097</td>\n",
       "      <td>1.166863</td>\n",
       "      <td>2.036034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775411</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-0.864018</td>\n",
       "      <td>-0.908001</td>\n",
       "      <td>-0.784495</td>\n",
       "      <td>1.329586</td>\n",
       "      <td>0.547925</td>\n",
       "      <td>1.195395</td>\n",
       "      <td>-0.810089</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
       "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
       "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
       "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
       "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
       "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
       "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
       "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
       "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
       "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
       "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
       "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
       "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/AGE_PREDICTION.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return 1-(self.activation(x)**2)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return self.activation(x)*(1-self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def mape(y_real, y_pred):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "    def dloss_dypred(y_real, y_pred):\n",
    "        return (y_pred-y_real)/y_real.shape[0]\n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = self.dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            da_dz = self.derivate_activation(zs[layer])\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "\n",
    "    def train(model, X, y, epochs, lr, method=\"Batch\", alpha = 0.5, batch_size=None, X_val = None, y_val = None):\n",
    "\n",
    "        if method != \"Batch\" and method != \"Mini Batch\" and method != \"SGD\":\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\" and \"SGD\".')\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "            print(f\"Mini-batch size: {batch_size}\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                # Use the entire dataset\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                loss = model.forward(X_batch, y_batch)\n",
    "                model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                model.update(lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    loss = model.forward(xi, yi)\n",
    "                    model.backward(xi, yi, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                    print(f\"[SGD] Sample {i+1}/{len(X_shuffled)} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                # Shuffle the data\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    loss = model.forward(X_batch, y_batch)\n",
    "                    model.backward(X_batch, y_batch, alpha=alpha)\n",
    "                    model.update(lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                    print(f\"[Mini-batch] Batch {num_batches} - Loss: {loss:.6f}\")\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "\n",
    "                y_pred_val = model.forward(X_val, y_val)[0]\n",
    "\n",
    "                val_loss = model.mse_loss(y_val, y_pred_val, alpha=alpha)\n",
    "\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Training completed!\\nFinal training loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb5bde",
   "metadata": {},
   "source": [
    "## Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62527eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, neurons, act='tanh'):\n",
    "        \"\"\"\n",
    "        neurons: list with the number of neurons in each layer.\n",
    "                      e.g. [512, 16, 8, 1]\n",
    "        activation: string, 'tanh' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.layers = len(neurons) - 1  # number of layers (excluding input)\n",
    "        self.act = act\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = neurons[i]\n",
    "            out_dim = neurons[i + 1]\n",
    "            W = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def derivate_activation(self, x):\n",
    "        if self.act == 'tanh':\n",
    "            return 1-(self.activation(x)**2)\n",
    "        elif self.act == 'sigmoid':\n",
    "            return self.activation(x)*(1-self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        X: input data of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Output prediction of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        a = X\n",
    "        zs = []\n",
    "        activations = [X]  # Store input as the first activation\n",
    "        \n",
    "        for i in range(self.layers - 1):  # Hidden layers\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(output)\n",
    "        \n",
    "        return output, activations, zs\n",
    "\n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Returns all weights and biases flattened into a single vector.\"\"\"\n",
    "        params = []\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            params.append(W.flatten())\n",
    "            params.append(b.flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "\n",
    "    def set_params_vector(self, flat_params):\n",
    "        \"\"\"Set the weights and biases from a flat parameter vector.\"\"\"\n",
    "        idx = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layers):\n",
    "            in_dim = self.neurons[i]\n",
    "            out_dim = self.neurons[i + 1]\n",
    "            w_size = in_dim * out_dim\n",
    "            b_size = out_dim\n",
    "\n",
    "            W = flat_params[idx:idx + w_size].reshape((in_dim, out_dim))\n",
    "            idx += w_size\n",
    "            b = flat_params[idx:idx + b_size].reshape(1, out_dim)\n",
    "            idx += b_size\n",
    "\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def mse_loss(self, y_real, y_pred, alpha):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        alpha: regularization parameter\n",
    "        \"\"\"\n",
    "        weights = self.weights\n",
    "\n",
    "        loss = np.mean((y_real - y_pred)**2)/2\n",
    "        reg = sum([np.sum(w**2) for w in weights])\n",
    "        return loss + alpha * reg\n",
    "    \n",
    "    \n",
    "    def mape(y_real, y_pred):\n",
    "        \"\"\"\n",
    "        y_real: true target values\n",
    "        y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs((y_real - y_pred) / (y_real + 1e-8))) * 100\n",
    "    \n",
    "    def dloss_dypred(y_real, y_pred):\n",
    "        return (y_pred-y_real)/y_real.shape[0]\n",
    "    \n",
    "    def backward(self, X, y, alpha = 0.5):\n",
    "        y_pred, activations, zs = self.forward(X)\n",
    "        deltas = [None] * self.layers\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        # delta output layer\n",
    "        dL_dy = self.dloss_dypred(y, y_pred)  # shape (batch, out_dim)\n",
    "        deltas[-1] = dL_dy  # ultimo layer: no activation\n",
    "\n",
    "        # hidden layers backwards\n",
    "        for layer in reversed(range(self.layers - 1)):\n",
    "            da_dz = self.derivate_activation(zs[layer])\n",
    "            deltas[layer] = (deltas[layer+1] @ self.weights[layer+1].T) * da_dz\n",
    "\n",
    "        # grad W, b\n",
    "        for layer in range(self.layers):\n",
    "            a_prev = activations[layer]\n",
    "            grad_W = (a_prev.T @ deltas[layer])+2*alpha*self.weights[layer]\n",
    "            grads_W.append(grad_W)\n",
    "            grads_b.append(np.sum(deltas[layer], axis=0))\n",
    "\n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def update(self, grads_W, grads_b, lr=1e-3):\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= lr * grads_W[i]\n",
    "            self.biases[i]  -= lr * grads_b[i]\n",
    "\n",
    "    def train(self, X, y, epochs, lr, method=\"Batch\", alpha=0.5, batch_size=None, X_val=None, y_val=None):\n",
    "        if method not in [\"Batch\", \"Mini Batch\", \"SGD\"]:\n",
    "            raise ValueError('Select a method between: \"Batch\", \"Mini Batch\", and \"SGD\".')\n",
    "\n",
    "        print(f\"\\nTraining started using '{method}' method for {epochs} epochs.\")\n",
    "\n",
    "        if method == \"Mini Batch\" and batch_size is None:\n",
    "            raise ValueError(\"You must specify a batch_size for mini-batch training.\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if method == \"Batch\":\n",
    "                X_batch, y_batch = X, y\n",
    "\n",
    "                y_pred, _, _ = self.forward(X_batch)\n",
    "                loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                self.update(grads_W, grads_b, lr)\n",
    "\n",
    "                print(f\"[Batch] Loss: {loss:.6f}\")\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                for i in range(len(X_shuffled)):\n",
    "                    xi = X_shuffled[i].reshape(1, -1)\n",
    "                    yi = y_shuffled[i].reshape(1, -1)\n",
    "\n",
    "                    y_pred, _, _ = self.forward(xi)\n",
    "                    loss = self.mse_loss(yi, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(xi, yi, alpha)\n",
    "                    self.update(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "                avg_loss = total_loss / len(X_shuffled)\n",
    "                print(f\"[SGD] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            elif method == \"Mini Batch\":\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "\n",
    "                for i in range(0, len(X_shuffled), batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                    y_pred, _, _ = self.forward(X_batch)\n",
    "                    loss = self.mse_loss(y_batch, y_pred, alpha)\n",
    "\n",
    "                    grads_W, grads_b = self.backward(X_batch, y_batch, alpha)\n",
    "                    self.update(grads_W, grads_b, lr)\n",
    "\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"[Mini-batch] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            # Validation\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val, _, _ = self.forward(X_val)\n",
    "                val_loss = self.mse_loss(y_val, y_pred_val, alpha)\n",
    "                print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bedf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started using 'Mini Batch' method for [[38]\n",
      " [32]\n",
      " [55]\n",
      " [26]\n",
      " [34]\n",
      " [26]\n",
      " [24]\n",
      " [32]\n",
      " [25]\n",
      " [28]\n",
      " [27]\n",
      " [27]\n",
      " [34]\n",
      " [56]\n",
      " [40]\n",
      " [24]\n",
      " [35]\n",
      " [10]\n",
      " [25]\n",
      " [36]\n",
      " [47]\n",
      " [43]\n",
      " [36]\n",
      " [21]\n",
      " [30]\n",
      " [26]\n",
      " [45]\n",
      " [15]\n",
      " [32]\n",
      " [55]\n",
      " [66]\n",
      " [57]\n",
      " [28]\n",
      " [23]\n",
      " [20]\n",
      " [14]\n",
      " [39]\n",
      " [16]\n",
      " [35]\n",
      " [55]\n",
      " [26]\n",
      " [59]\n",
      " [24]\n",
      " [26]\n",
      " [25]\n",
      " [26]\n",
      " [26]\n",
      " [34]\n",
      " [27]\n",
      " [32]\n",
      " [17]\n",
      " [26]\n",
      " [28]\n",
      " [31]\n",
      " [27]\n",
      " [86]\n",
      " [30]\n",
      " [30]\n",
      " [26]\n",
      " [35]\n",
      " [53]\n",
      " [26]\n",
      " [65]\n",
      " [27]\n",
      " [12]\n",
      " [17]\n",
      " [47]\n",
      " [68]\n",
      " [39]\n",
      " [34]\n",
      " [36]\n",
      " [68]\n",
      " [23]\n",
      " [50]\n",
      " [16]\n",
      " [28]\n",
      " [28]\n",
      " [16]\n",
      " [27]\n",
      " [29]\n",
      " [31]\n",
      " [29]\n",
      " [16]\n",
      " [35]\n",
      " [30]\n",
      " [47]\n",
      " [26]\n",
      " [37]\n",
      " [45]\n",
      " [58]\n",
      " [24]\n",
      " [37]\n",
      " [48]\n",
      " [43]\n",
      " [25]\n",
      " [50]\n",
      " [35]\n",
      " [20]\n",
      " [38]\n",
      " [16]\n",
      " [36]\n",
      " [63]\n",
      " [41]\n",
      " [63]\n",
      " [32]\n",
      " [38]\n",
      " [53]\n",
      " [25]\n",
      " [52]\n",
      " [26]\n",
      " [85]\n",
      " [36]\n",
      " [65]\n",
      " [49]\n",
      " [26]\n",
      " [28]\n",
      " [85]\n",
      " [35]\n",
      " [41]\n",
      " [65]\n",
      " [21]\n",
      " [62]\n",
      " [36]\n",
      " [22]\n",
      " [30]\n",
      " [25]\n",
      " [10]\n",
      " [36]\n",
      " [26]\n",
      " [35]\n",
      " [30]\n",
      " [23]\n",
      " [26]\n",
      " [26]\n",
      " [60]\n",
      " [26]\n",
      " [35]\n",
      " [70]\n",
      " [25]\n",
      " [31]\n",
      " [36]\n",
      " [45]\n",
      " [23]\n",
      " [30]\n",
      " [24]\n",
      " [57]\n",
      " [64]\n",
      " [55]\n",
      " [66]\n",
      " [32]\n",
      " [55]\n",
      " [37]\n",
      " [31]\n",
      " [60]\n",
      " [79]\n",
      " [50]\n",
      " [32]\n",
      " [25]\n",
      " [38]\n",
      " [26]\n",
      " [50]\n",
      " [54]\n",
      " [23]\n",
      " [34]\n",
      " [26]\n",
      " [27]\n",
      " [58]\n",
      " [54]\n",
      " [26]\n",
      " [60]\n",
      " [21]\n",
      " [26]\n",
      " [20]\n",
      " [25]\n",
      " [25]\n",
      " [34]\n",
      " [57]\n",
      " [13]\n",
      " [32]\n",
      " [30]\n",
      " [24]\n",
      " [35]\n",
      " [35]\n",
      " [35]\n",
      " [25]\n",
      " [50]\n",
      " [58]\n",
      " [45]\n",
      " [33]\n",
      " [19]\n",
      " [28]\n",
      " [18]\n",
      " [31]\n",
      " [47]\n",
      " [60]\n",
      " [35]\n",
      " [24]\n",
      " [33]\n",
      " [36]\n",
      " [26]\n",
      " [26]\n",
      " [20]\n",
      " [26]\n",
      " [48]\n",
      " [26]\n",
      " [58]\n",
      " [54]\n",
      " [28]\n",
      " [49]\n",
      " [35]\n",
      " [38]\n",
      " [11]\n",
      " [25]\n",
      " [30]\n",
      " [35]\n",
      " [27]\n",
      " [32]\n",
      " [60]\n",
      " [28]\n",
      " [24]\n",
      " [53]\n",
      " [32]\n",
      " [82]\n",
      " [55]\n",
      " [28]\n",
      " [36]\n",
      " [55]\n",
      " [63]\n",
      " [22]\n",
      " [10]\n",
      " [34]\n",
      " [48]\n",
      " [55]\n",
      " [26]\n",
      " [23]\n",
      " [25]\n",
      " [22]\n",
      " [49]\n",
      " [25]\n",
      " [27]\n",
      " [22]\n",
      " [12]\n",
      " [24]\n",
      " [65]\n",
      " [16]\n",
      " [32]\n",
      " [24]\n",
      " [38]\n",
      " [54]\n",
      " [18]\n",
      " [84]\n",
      " [22]\n",
      " [26]\n",
      " [26]\n",
      " [46]\n",
      " [33]\n",
      " [70]\n",
      " [30]\n",
      " [40]\n",
      " [24]\n",
      " [27]\n",
      " [54]\n",
      " [13]\n",
      " [26]\n",
      " [25]\n",
      " [37]\n",
      " [40]\n",
      " [34]\n",
      " [33]\n",
      " [58]\n",
      " [39]\n",
      " [60]\n",
      " [15]\n",
      " [25]\n",
      " [40]\n",
      " [30]\n",
      " [35]\n",
      " [16]\n",
      " [54]\n",
      " [28]\n",
      " [29]\n",
      " [85]\n",
      " [17]\n",
      " [45]\n",
      " [45]\n",
      " [22]\n",
      " [22]\n",
      " [25]\n",
      " [23]\n",
      " [50]\n",
      " [62]\n",
      " [70]\n",
      " [59]\n",
      " [89]\n",
      " [37]\n",
      " [22]\n",
      " [11]\n",
      " [54]\n",
      " [63]\n",
      " [26]\n",
      " [28]\n",
      " [40]\n",
      " [26]\n",
      " [34]\n",
      " [23]\n",
      " [65]\n",
      " [18]\n",
      " [44]\n",
      " [26]\n",
      " [26]\n",
      " [36]\n",
      " [46]\n",
      " [53]\n",
      " [54]\n",
      " [25]\n",
      " [60]\n",
      " [31]\n",
      " [65]\n",
      " [26]\n",
      " [40]\n",
      " [32]\n",
      " [24]\n",
      " [70]\n",
      " [48]\n",
      " [60]\n",
      " [35]\n",
      " [26]\n",
      " [36]\n",
      " [50]\n",
      " [48]\n",
      " [70]\n",
      " [54]\n",
      " [17]\n",
      " [60]\n",
      " [35]\n",
      " [56]\n",
      " [54]\n",
      " [18]\n",
      " [42]\n",
      " [27]\n",
      " [26]\n",
      " [25]\n",
      " [35]\n",
      " [30]\n",
      " [10]\n",
      " [35]\n",
      " [21]\n",
      " [45]\n",
      " [58]\n",
      " [16]\n",
      " [58]\n",
      " [60]\n",
      " [45]\n",
      " [79]\n",
      " [25]\n",
      " [60]\n",
      " [26]\n",
      " [30]\n",
      " [28]\n",
      " [37]\n",
      " [63]\n",
      " [26]\n",
      " [22]\n",
      " [25]\n",
      " [25]\n",
      " [65]\n",
      " [65]\n",
      " [27]\n",
      " [40]\n",
      " [25]\n",
      " [19]\n",
      " [64]\n",
      " [34]\n",
      " [24]\n",
      " [52]\n",
      " [54]\n",
      " [85]\n",
      " [35]\n",
      " [24]\n",
      " [55]\n",
      " [25]\n",
      " [36]\n",
      " [22]\n",
      " [32]\n",
      " [49]\n",
      " [17]\n",
      " [18]\n",
      " [32]\n",
      " [29]\n",
      " [49]\n",
      " [26]\n",
      " [27]\n",
      " [75]\n",
      " [35]\n",
      " [49]\n",
      " [32]\n",
      " [80]\n",
      " [26]\n",
      " [29]\n",
      " [21]\n",
      " [42]\n",
      " [27]\n",
      " [45]\n",
      " [28]\n",
      " [45]\n",
      " [36]\n",
      " [28]\n",
      " [85]\n",
      " [27]\n",
      " [61]\n",
      " [26]\n",
      " [25]\n",
      " [23]\n",
      " [31]\n",
      " [34]\n",
      " [15]\n",
      " [39]\n",
      " [50]\n",
      " [15]\n",
      " [25]\n",
      " [55]\n",
      " [35]\n",
      " [60]\n",
      " [20]\n",
      " [59]\n",
      " [26]\n",
      " [12]\n",
      " [32]\n",
      " [23]\n",
      " [34]\n",
      " [20]\n",
      " [30]\n",
      " [27]\n",
      " [59]\n",
      " [85]\n",
      " [26]\n",
      " [18]\n",
      " [30]\n",
      " [46]\n",
      " [30]\n",
      " [78]\n",
      " [27]\n",
      " [49]\n",
      " [85]\n",
      " [26]\n",
      " [34]\n",
      " [28]\n",
      " [32]\n",
      " [28]\n",
      " [50]\n",
      " [47]\n",
      " [45]\n",
      " [35]\n",
      " [32]\n",
      " [32]\n",
      " [24]\n",
      " [26]\n",
      " [34]\n",
      " [30]\n",
      " [26]\n",
      " [54]\n",
      " [25]\n",
      " [37]\n",
      " [45]\n",
      " [48]\n",
      " [36]\n",
      " [26]\n",
      " [28]\n",
      " [29]\n",
      " [28]\n",
      " [21]\n",
      " [85]\n",
      " [30]\n",
      " [19]\n",
      " [26]\n",
      " [26]\n",
      " [32]\n",
      " [27]\n",
      " [38]\n",
      " [28]\n",
      " [70]\n",
      " [24]\n",
      " [23]\n",
      " [35]\n",
      " [24]\n",
      " [23]\n",
      " [26]\n",
      " [27]\n",
      " [22]\n",
      " [23]\n",
      " [10]\n",
      " [12]\n",
      " [15]\n",
      " [24]\n",
      " [37]\n",
      " [29]\n",
      " [16]\n",
      " [39]\n",
      " [58]\n",
      " [64]\n",
      " [32]\n",
      " [35]\n",
      " [28]\n",
      " [35]\n",
      " [30]\n",
      " [26]\n",
      " [39]\n",
      " [57]\n",
      " [50]\n",
      " [66]\n",
      " [46]\n",
      " [21]\n",
      " [32]\n",
      " [53]\n",
      " [27]\n",
      " [32]\n",
      " [26]\n",
      " [65]\n",
      " [26]\n",
      " [53]\n",
      " [25]\n",
      " [26]\n",
      " [18]\n",
      " [24]\n",
      " [27]\n",
      " [26]\n",
      " [40]\n",
      " [26]\n",
      " [26]\n",
      " [50]\n",
      " [24]\n",
      " [71]\n",
      " [34]\n",
      " [35]\n",
      " [40]\n",
      " [50]\n",
      " [49]\n",
      " [49]\n",
      " [26]\n",
      " [28]\n",
      " [62]\n",
      " [26]\n",
      " [31]\n",
      " [37]\n",
      " [32]\n",
      " [50]\n",
      " [65]\n",
      " [30]\n",
      " [62]\n",
      " [80]\n",
      " [34]\n",
      " [58]\n",
      " [25]\n",
      " [45]\n",
      " [45]\n",
      " [52]\n",
      " [35]\n",
      " [26]\n",
      " [53]\n",
      " [26]\n",
      " [40]\n",
      " [62]\n",
      " [19]\n",
      " [54]\n",
      " [39]\n",
      " [26]\n",
      " [85]\n",
      " [54]\n",
      " [28]\n",
      " [60]\n",
      " [48]\n",
      " [45]\n",
      " [29]\n",
      " [12]\n",
      " [66]\n",
      " [24]\n",
      " [46]\n",
      " [56]\n",
      " [26]\n",
      " [40]\n",
      " [50]\n",
      " [22]\n",
      " [30]\n",
      " [36]\n",
      " [26]\n",
      " [25]\n",
      " [31]\n",
      " [35]\n",
      " [28]\n",
      " [55]\n",
      " [34]\n",
      " [68]\n",
      " [35]\n",
      " [59]\n",
      " [35]\n",
      " [47]\n",
      " [31]\n",
      " [18]\n",
      " [27]\n",
      " [46]\n",
      " [33]\n",
      " [35]\n",
      " [24]\n",
      " [52]\n",
      " [10]\n",
      " [66]\n",
      " [30]\n",
      " [51]\n",
      " [32]\n",
      " [28]\n",
      " [26]\n",
      " [38]\n",
      " [42]\n",
      " [16]\n",
      " [56]\n",
      " [15]\n",
      " [32]\n",
      " [55]\n",
      " [80]\n",
      " [65]\n",
      " [24]\n",
      " [12]\n",
      " [30]\n",
      " [18]\n",
      " [24]\n",
      " [44]\n",
      " [26]\n",
      " [31]\n",
      " [54]\n",
      " [12]\n",
      " [78]\n",
      " [33]\n",
      " [27]\n",
      " [45]\n",
      " [24]\n",
      " [54]\n",
      " [33]\n",
      " [58]\n",
      " [29]\n",
      " [26]\n",
      " [23]\n",
      " [25]\n",
      " [28]\n",
      " [35]\n",
      " [30]\n",
      " [29]\n",
      " [48]\n",
      " [53]\n",
      " [30]\n",
      " [58]\n",
      " [55]\n",
      " [26]\n",
      " [26]\n",
      " [30]\n",
      " [60]\n",
      " [49]\n",
      " [30]\n",
      " [26]\n",
      " [26]\n",
      " [40]\n",
      " [38]\n",
      " [38]\n",
      " [26]\n",
      " [25]\n",
      " [18]\n",
      " [26]\n",
      " [26]\n",
      " [60]\n",
      " [25]\n",
      " [30]\n",
      " [18]\n",
      " [32]\n",
      " [21]\n",
      " [22]\n",
      " [17]\n",
      " [35]\n",
      " [24]\n",
      " [22]\n",
      " [26]\n",
      " [41]\n",
      " [42]\n",
      " [30]\n",
      " [26]\n",
      " [29]\n",
      " [26]\n",
      " [28]\n",
      " [20]\n",
      " [32]\n",
      " [64]\n",
      " [32]\n",
      " [60]\n",
      " [60]\n",
      " [24]\n",
      " [45]\n",
      " [23]\n",
      " [52]\n",
      " [19]\n",
      " [13]\n",
      " [30]\n",
      " [29]\n",
      " [26]\n",
      " [38]\n",
      " [67]\n",
      " [78]\n",
      " [72]\n",
      " [27]\n",
      " [59]\n",
      " [42]\n",
      " [53]\n",
      " [53]\n",
      " [30]\n",
      " [28]\n",
      " [39]\n",
      " [70]\n",
      " [61]\n",
      " [10]\n",
      " [15]\n",
      " [24]\n",
      " [36]\n",
      " [62]\n",
      " [46]\n",
      " [34]\n",
      " [55]\n",
      " [25]\n",
      " [26]\n",
      " [14]\n",
      " [35]\n",
      " [26]\n",
      " [28]\n",
      " [39]\n",
      " [26]\n",
      " [24]\n",
      " [85]\n",
      " [26]\n",
      " [74]\n",
      " [58]\n",
      " [52]\n",
      " [30]\n",
      " [14]\n",
      " [45]\n",
      " [18]\n",
      " [43]\n",
      " [14]\n",
      " [24]\n",
      " [10]\n",
      " [26]\n",
      " [35]\n",
      " [43]\n",
      " [40]\n",
      " [32]\n",
      " [83]\n",
      " [22]\n",
      " [66]\n",
      " [26]\n",
      " [35]\n",
      " [50]\n",
      " [63]\n",
      " [26]\n",
      " [29]\n",
      " [54]\n",
      " [26]\n",
      " [34]\n",
      " [20]\n",
      " [32]\n",
      " [28]\n",
      " [22]\n",
      " [35]\n",
      " [35]\n",
      " [86]\n",
      " [35]\n",
      " [26]\n",
      " [26]\n",
      " [40]\n",
      " [26]\n",
      " [27]\n",
      " [32]\n",
      " [40]\n",
      " [45]\n",
      " [24]\n",
      " [32]\n",
      " [43]\n",
      " [50]\n",
      " [25]\n",
      " [35]\n",
      " [78]\n",
      " [26]\n",
      " [30]\n",
      " [28]\n",
      " [12]\n",
      " [18]\n",
      " [20]\n",
      " [25]\n",
      " [35]\n",
      " [16]\n",
      " [12]\n",
      " [12]\n",
      " [14]\n",
      " [50]\n",
      " [26]\n",
      " [18]] epochs.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Define and train model\u001b[39;00m\n\u001b[32m     17\u001b[39m nn = NeuralNetwork([X.shape[\u001b[32m1\u001b[39m], \u001b[32m16\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m1\u001b[39m], act=\u001b[33m\"\u001b[39m\u001b[33mtanh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m      \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMini Batch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m      \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m      \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mNeuralNetwork.train\u001b[39m\u001b[34m(self, X, y, epochs, lr, method, alpha, batch_size, X_val, y_val)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mMini Batch\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou must specify a batch_size for mini-batch training.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mBatch\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Take a random subset of 1000 rows\n",
    "df_subset = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Split features and target\n",
    "X = df_subset.drop(\"gt\", axis=1).values\n",
    "y = df_subset[\"gt\"].values.reshape(-1, 1)\n",
    "\n",
    "# Train/val split (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "# Define and train model\n",
    "nn = NeuralNetwork([X.shape[1], 16, 8, 1], act=\"tanh\")\n",
    "\n",
    "nn.train(nn, \n",
    "      X_train, y_train, \n",
    "      lr=0.01, \n",
    "      epochs= 20,\n",
    "      method=\"Mini Batch\", \n",
    "      batch_size=32, \n",
    "      alpha=0.001, \n",
    "      X_val=X_val, y_val=y_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
